[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Multi-Agent Systems",
    "section": "",
    "text": "About the course",
    "crumbs": [
      "Multi-Agent Systems",
      "About the course"
    ]
  },
  {
    "objectID": "index.html#general-information",
    "href": "index.html#general-information",
    "title": "Multi-Agent Systems",
    "section": "General Information (Winter 2025)",
    "text": "General Information (Winter 2025)\n\nInstructor\n\n\nAditya Mahajan\nOffice Hours: 4pm–5pm Monday (MC 533)\n\n\nTeaching Assistants\n\n\nZiqi Huang\n\n\nLectures\n\n\n2:35pm–3:55pm Monday, Wednesday (ENGTR 2120)\n\n\nPrerequisites\n\n\nECSE 205 (Probability and Random Signals I)\nYou are expected to know the following concepts from basic undergraduate probability: Probability of events, independence, random variables, probability distributions (PDFs and CDFs), expectation, conditional probability, and conditional expectation.\n\n\nCommunication\n\nUse the discussion board on myCourses for all questions related to the course. Only personal emails related to medical exceptions for missing a deliverable will be answered.",
    "crumbs": [
      "Multi-Agent Systems",
      "About the course"
    ]
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "Multi-Agent Systems",
    "section": "Course Content",
    "text": "Course Content\n\nRational decision making; Strategic games and Bayesian games; Solution concepts: iterated elimination of dominated strategies, Nash equilibrium, Correlated equilibrium, Bayesian equilibrium; Common knowledge.\nExtensive form games with perfect information, sub-game perfect equilibrium, Markov perfect equilibrium.\nGames with imperfect information, sequential equilibrium, common information based refinements of sequential equilibrium.\nMechanism design, market equilibrium, pricing, resource allocation; Application to communication networks and power systems\n\nIn the event of extraordinary circumstances beyond the University’s control, the content and/or evaluation scheme in this course is subject to change.",
    "crumbs": [
      "Multi-Agent Systems",
      "About the course"
    ]
  },
  {
    "objectID": "index.html#course-material",
    "href": "index.html#course-material",
    "title": "Multi-Agent Systems",
    "section": "Course Material",
    "text": "Course Material\n\nReference books\nI will loosely follow the following two books, though I present the material in a different order/style than the books.\n\nZamir, Maschler, Solan, Game Theory, Cambridge University Press, 2013.\nOsborne and Rubinstein, A Course in Game Theory, MIT Press, 1994\nShoham and Leyton-Brown, Multiagent Systems, Cambridge University Press, 2009.\n\n\n\nOnline Course Notes\nThis semesmter, I will be attempting to type my notes and make them available on this website. Most definitely, I will fall behind and the notes for the material covered during the end of the term will not be avaiable. Even for the earlier material, the notes are not meant to be exhaustive; rather my focus is to convey the key ideas in their simplest form. For a more exhaustive treatment of the subject, please refer to the reference books mentioned above.\nIf you find any typos/mistakes in the notes, please let me know. Pull requests are welcome.",
    "crumbs": [
      "Multi-Agent Systems",
      "About the course"
    ]
  },
  {
    "objectID": "index.html#evaluation",
    "href": "index.html#evaluation",
    "title": "Multi-Agent Systems",
    "section": "Evaluation",
    "text": "Evaluation\n\nAssignments (40%) Weekly homework assignments. Typically, each assignment will consist of four questions, out of which one or two randomly selected questions will be grader.\nMid Term (40%) Closed book in-class exam. March 12 (during class time)\nTerm Project (20%) A month long term project to be done in groups of two. Present one paper on any topic of your interest related to the material covered in the class.",
    "crumbs": [
      "Multi-Agent Systems",
      "About the course"
    ]
  },
  {
    "objectID": "index.html#marking-policy",
    "href": "index.html#marking-policy",
    "title": "Multi-Agent Systems",
    "section": "Marking policy",
    "text": "Marking policy\n\nAssignments must be submitted electronically on myCourses as a PDF. You may write the assignments on paper and then scan them as a PDF (there are several such apps available for all phone platforms), or write on a tablet and convert to PDF, or type using a word processor.\nThere will no make-up examination for students who miss a mid-term.\n\nStudent who miss the exam due to a valid reason (see Faculty of Engineering policy) should notify the instructor within a week of the exam and provide necessary documentation.\nIf, and only if, proper documentation for a missed exam is presented, the marks for the missed exam will be shifted to the final exam.\nStudents who miss the mid-term exam for any other reason (e.g., no medical note, going to the exam at the wrong time, or on the wrong day, etc.) will get zero marks on the exam.\n\nAny request for reevaluation of a mid-term or an assignment must be made in writing within a week of its return. Note that requesting a re-grade will mean that you WHOLE assignment or exam will be re-graded.\nDue to paucity of grading hours, only one or two randomly selected questions will be graded in each assignment.\nThe lowest two assignments and labs will be dropped. There will be no make-up for missed assignments and labs, even if it is for a valid reason. The whole point of dropping the lowest two assignments/labs is to reduce the administrative overhead of keeping track of such missed assignments/labs.\n\n\nRight to submit in English or French written work that is to be graded.\n\nIn accord with McGill University’s Charter of Students’ Rights, students in this course have the right to submit in English or in French any written work that is to be graded.\n\nAcademic Integrity\n\nMcGill University values academic integrity. Therefore all students must understand the meaning and consequences of cheating, plagiarism and other academic offences under the Code of Student Conduct and Disciplinary Procedures (see McGill’s guide to academic honesty for more information).\nL’université McGill attache une haute importance à l’honnêteté académique. Il incombe par conséquent à tous les étudiants de comprendre ce que l’on entend par tricherie, plagiat et autres infractions académiques, ainsi que les conséquences que peuvent avoir de telles actions, selon le Code de conduite de l’étudiant et des procédures disciplinaires (pour de plus amples renseignements, veuillez consulter le guide pour l’honnêteté académique de McGill.)",
    "crumbs": [
      "Multi-Agent Systems",
      "About the course"
    ]
  },
  {
    "objectID": "index.html#course-delivery",
    "href": "index.html#course-delivery",
    "title": "Multi-Agent Systems",
    "section": "Course delivery",
    "text": "Course delivery\nThe course is taught in a “chalk and board” style; there will be no power point presentations. All students are expected to attend lectures and take notes. Partial notes on some of the material will be provided, but are not a substitute for the material covered in class.\n© Instructor-generated course materials (e.g., handouts, notes, summaries, exam questions) are protected by law and may not be copied or distributed in any form or in any medium without explicit permission of the instructor. Note that infringements of copyright can be subject to follow up by the University under the Code of Student Conduct and Disciplinary Procedures.",
    "crumbs": [
      "Multi-Agent Systems",
      "About the course"
    ]
  },
  {
    "objectID": "index.html#additional-notes",
    "href": "index.html#additional-notes",
    "title": "Multi-Agent Systems",
    "section": "Additional Notes",
    "text": "Additional Notes\n\nAs the instructor of this course I endeavor to provide an inclusive learning environment. However, if you experience barriers to learning in this course, do not hesitate to discuss them with me or contact the office of Student Accessibility and Achievement.\nEnd-of-course evaluations are one of the ways that McGill works towards maintaining and improving the quality of courses and the student’s learning experience. You will be notified by e-mail when the evaluations are available. Please note that a minimum number of responses must be received for results to be available to students.",
    "crumbs": [
      "Multi-Agent Systems",
      "About the course"
    ]
  },
  {
    "objectID": "static-games/strategic-games.html",
    "href": "static-games/strategic-games.html",
    "title": "1  Rationalizable strategies",
    "section": "",
    "text": "1.1 Examples\nMulti-agent decision problems are conceptually different from single agent decision problems. In a single agent decision problem, a decision maker has to choose a strategy \\(s \\in \\ALPHABET S\\) and receives a payoff or utility \\(u(s)\\) (or, equivalently, incurs a cost \\(c(s)\\)). In multi-agent decision problems, the utility received by an agent may depend on the strategies of other agents. Such settings are called games. We start with the simplest setting of two player games.\nConsider a decision problem where there are two players. Player 1 (\\(P_1\\) from now on) chooses an strategy \\(s_1 \\in \\ALPHABET S_1\\) and player 2 (\\(P_2\\) from now on) chooses an strategy \\(s_2 \\in \\ALPHABET S_2\\). Once both players have chosen their strategies, \\(P_1\\) receives a payoff of \\(u_1(s_1, s_2)\\) and \\(P_2\\) receives a payoff of \\(u_2(s_1,s_2)\\). How should the players behave?\nWe start with a few examples.\nThis example can be modeled as a two player game as follow. The strategy sets of both players are \\(\\ALPHABET S_1 = \\ALPHABET S_2 = \\{ \\mathsf{A}, \\mathsf{R} \\}\\), where \\(\\mathsf{A}\\) means that the player accepts the bargain and \\(\\mathsf{R}\\) means that the player rejects the bargain. The utility functions can be represented compactly as follows, which is called the bimatrix representation of the game.\nIn the games described above, there is a conflict between the players. We cannot simply assert that the players should take an strategy which maximizes their utility because the utility of a player depends on the strategies of other players, who have different incentives. The objective of game theory is to understand how to make decisions in such scenarios.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Rationalizable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/strategic-games.html#examples",
    "href": "static-games/strategic-games.html#examples",
    "title": "1  Rationalizable strategies",
    "section": "",
    "text": "Prisoner’s dilemma.\n\n\n\nTwo criminals are arrested for a crime but the prosecutors have evidence to only charge them for a lesser crime but not enough to charge them for the main crime. The prisoners are kept in separate cells with no means to communicate. The prosecutors simultaneously offer the following bargain to both prisoners: serve as a witness that the other criminal committed the crime and walk free; unless the other also confesses in which case both get sentenced. If both criminals confess, both get a reduced sentenced for the main crime (2 years in prison). If only one confesses, the criminal who confessed walks free while the other gets a full sentence for the main crime (10 years in prison). If neither prisoner takes the bargain, then both get charged for the lesser crime (1 year in prison).\n\n\n\n\n\nBimatrix representation of Prisoner’s dilemma game\n\n\n\n\n\n\\(\\mathsf{A}\\)\n\n\n\\(\\mathsf{R}\\)\n\n\n\n\n\\(\\mathsf{A}\\)\n\n\n\\(-2\\)\n\n\n\\(-2\\)\n\n\n\\(0\\)\n\n\n\\(-3\\)\n\n\n\n\n\\(\\mathsf{R}\\)\n\n\n\\(-3\\)\n\n\n\\(0\\)\n\n\n\\(-1\\)\n\n\n\\(-1\\)\n\n\n\n\n\n\n\n\n\nBattle of sexes\n\n\n\nA couple wants to go out for the evening and there are two events taking place: a football game and an opera. One person (player 1) prefers to go to the football game while the other player (player 2) prefers to go the opera. But they want to go together and are miserable if they go to separate events.\n\n\nBattle of Sexes Game\n\n\n\n\n\n\\(\\mathsf{F}\\)\n\n\n\\(\\mathsf{O}\\)\n\n\n\n\n\\(\\mathsf{F}\\)\n\n\n\\(2\\)\n\n\n\\(1\\)\n\n\n\\(0\\)\n\n\n\\(0\\)\n\n\n\n\n\\(\\mathsf{O}\\)\n\n\n\\(0\\)\n\n\n\\(0\\)\n\n\n\\(1\\)\n\n\n\\(2\\)\n\n\n\n\n\n\n\n\n\n\n\nChicken (also called Hawk-Dove)\n\n\n\nTwo drivers are headed for a single lane bridge from opposite directions. The first to swerve away yields the bridge to the other and is called ‘chicken’ (i.e., a coward). If neither swerve, both are involved in a head-on collision.\n\n\nBattle of Hawk-Dove Game\n\n\n\n\n\n\\(\\mathsf{C}\\)\n\n\n\\(\\mathsf{H}\\)\n\n\n\n\n\\(\\mathsf{C}\\)\n\n\n\\(3\\)\n\n\n\\(3\\)\n\n\n\\(1\\)\n\n\n\\(10\\)\n\n\n\n\n\\(\\mathsf{H}\\)\n\n\n\\(10\\)\n\n\n\\(1\\)\n\n\n\\(0\\)\n\n\n\\(0\\)\n\n\n\n\n\n\n\n\n\n\n\nMatching pennies\n\n\n\nConsider a parlour game among two players. Each player has a penny and must secretly turn the penny to heads or tails. The players reveal their choices simultaneously. If the pennies match (both heads or both tails), player 1 wins. If they don’t player 2 wins.\n\n\nMatching pennies game\n\n\n\n\n\n\\(\\mathsf{H}\\)\n\n\n\\(\\mathsf{T}\\)\n\n\n\n\n\\(\\mathsf{H}\\)\n\n\n\\(1\\)\n\n\n\\(-1\\)\n\n\n\\(-1\\)\n\n\n\\(1\\)\n\n\n\n\n\\(\\mathsf{T}\\)\n\n\n\\(-1\\)\n\n\n\\(1\\)\n\n\n\\(1\\)\n\n\n\\(-1\\)",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Rationalizable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/strategic-games.html#modeling-strategic-games",
    "href": "static-games/strategic-games.html#modeling-strategic-games",
    "title": "1  Rationalizable strategies",
    "section": "1.2 Modeling Strategic Games",
    "text": "1.2 Modeling Strategic Games\nThe above examples described games between two players. General \\(n\\)-player games can be modeled as follows.\n\nDefinition 1.1 (Strategic game) A strategic game is described by a tuple \\(\\mathscr{G} = \\langle N, (\\ALPHABET S_i)_{i \\in N}, (u_i)_{i \\in N} \\rangle\\), where\n\n\\(N\\) is a (finite) set of players\n\\(\\ALPHABET S_i\\) is a non-empty set of strategies for player \\(i\\), \\(i \\in N\\). Define \\(\\ALPHABET S = \\prod_{i \\in N} \\ALPHABET S_i\\) as the strategy space of the game \\(\\mathscr{G}\\).\n\\(u_i \\colon \\ALPHABET S \\to \\reals\\) is the utility function of player \\(i\\), \\(i \\in N\\).\n\n\n\nRemarks\n\n\nThe utlity of player \\(i\\) depends on the strategies of all players and not just the strategy of player \\(i\\). This is the defining feature of a game.\nWhen all \\(\\ALPHABET S_i\\) are finite, the game is called a finite game. Such games are also sometimes called matrix games because they can be described by a matrix, as seen in the above examples.\nTo play the game, each player chooses a (pure) strategy \\(s_i \\in\n\\ALPHABET S_i\\).\nThe collection \\(s = (s_i)_{i \\in N}\\) is called the strategy profile.\nGiven a profile \\(x = (x_i)_{i \\in N}\\) (not necessarily strategy profile, but any list of elements, one for each player), \\(x_{-i}\\) denotes the list \\((x_j)_{j \\in N \\setminus \\{i\\}}\\). We will write \\(x = (x_i, x_{-i})\\).",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Rationalizable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/strategic-games.html#dominated-strategies",
    "href": "static-games/strategic-games.html#dominated-strategies",
    "title": "1  Rationalizable strategies",
    "section": "1.3 Dominated strategies",
    "text": "1.3 Dominated strategies\nWe now define the notion of dominance, which can be used to provide a solution concept for some games.\n\nDefinition 1.2 (Dominance) Consider a game \\(\\mathscr{G}\\) with standard notation. Let \\(s_i, t_i \\in \\ALPHABET S_i\\) be two strategies of player \\(i\\). We say, strategy \\(s_i\\) strictly dominates \\(t_i\\) if \\[\n  u_i(s_i, s_{-i}) \\mathbin{\\color{red}&gt;} u_i(t_i, s_{-i}),\n  \\quad \\forall s_{-i} \\in \\ALPHABET S_{-i}.\n\\] We say that \\(s_i\\) weakly dominates \\(t_i\\) if \\[\n  u_i(s_i, s_{-i}) \\mathbin{\\color{red}\\ge} u_i(t_i, s_{-i}),\n  \\quad \\forall s_{-i} \\in \\ALPHABET S_{-i}\n\\] and the inequality is strict for at least one \\(s_{-i}\\).\nWe will also use the phrase “\\(t_i\\) is (strongly or weakly) dominated by \\(s_i\\)” to denote the same fact.\n\nA strategy \\(s_i \\in \\ALPHABET S_i\\) is called (strongly or weakly) dominant strategy if it (strongly or weakly) dominates all other strategies \\(t_i \\in \\ALPHABET S_i \\setminus \\{s_i\\}\\).\nWe now impose some assumptions on the players.\n\nAssumption. A rational player will not choose a strictly dominated strategy.\n\nNote that we have not formally defined rationality. That is more of a philosophical discussion, and for the purpose of this course, we will not present a formal definition but rather go with the colloquial meaning of the word.\n\nAssumption. All players in a game are rational.\n\nIrrespective of the definition of rationality, this assumption is often violated in practice. Human decision makers are almost never rational. Even when decisions are made by algorithms, the decisions may not be rational. There is a whole branch of decision theory which considers players with bounded rationality. However, in this course, we will make the simplifying assumption that the players are rational.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Rationalizable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/strategic-games.html#dominant-strategy-equilibrium",
    "href": "static-games/strategic-games.html#dominant-strategy-equilibrium",
    "title": "1  Rationalizable strategies",
    "section": "1.4 Dominant strategy equilibrium",
    "text": "1.4 Dominant strategy equilibrium\nWe now present the simplest solution concept for a game.\n\n\n\n\n\n\nDominant strategy equilibrium\n\n\n\nDominant Strategy equilibrium is a strategy profile in which each player is playing a dominant strategy.\n\n\nFor example, consider the game of prisoner’s dilemma. Note that, for \\(P_1\\), \\[\\begin{align*}\n  u_1(\\mathsf{A}, \\cdot) &= \\begin{bmatrix} -2 & 0 \\end{bmatrix}, \\\\\n  u_1(\\mathsf{R}, \\cdot) &= \\begin{bmatrix} -3 & -1 \\end{bmatrix}.\n\\end{align*}\\] Note that \\[ u_1(\\mathsf{A}, ⋅) &gt; u_1(\\mathsf{R}, ⋅). \\] Thus \\(\\mathsf{A}\\) is a dominant strategy for \\(P_1\\).\nSimilarly, for \\(P_2\\), \\[\\begin{align*}\n  u_2(\\cdot, \\mathsf{A}) &= \\begin{bmatrix} -2 \\\\ 0 \\end{bmatrix},\n  &\n  u_2(\\cdot, \\mathsf{R}) &= \\begin{bmatrix} -3 \\\\ -1 \\end{bmatrix}.\n\\end{align*}\\] Note that \\[ u_2(⋅,\\mathsf{A}) &gt; u_2(⋅,\\mathsf{R}). \\] Thus, \\(\\mathsf{A}\\) is a dominant strategy for \\(P_2\\).\nTherefore, \\((\\mathsf{A}, \\mathsf{A})\\) is a dominant strategy equilibrium. Note that both players play \\(\\mathsf{A}\\) even though \\((\\mathsf{R}, \\mathsf{R})\\) gives a better outcome for both of them!\n\nA game may not have a dominant strategy equilibrium. For example, there are no dominant strategies in the battle of sexes and matching pennies. So, dominant strategy equilibrium is not a useful solution concept because it does not always exist. One option is to generalize the notion of dominance as explained next.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Rationalizable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/strategic-games.html#rationalizable-equilibrium",
    "href": "static-games/strategic-games.html#rationalizable-equilibrium",
    "title": "1  Rationalizable strategies",
    "section": "1.5 Rationalizable equilibrium",
    "text": "1.5 Rationalizable equilibrium\nFirst, we return to the assumptions imposed on the players. Consider the following game.\n\n\n\n\n\n\\(\\mathsf{L}\\)\n\n\n\\(\\mathsf{C}\\)\n\n\n\\(\\mathsf{R}\\)\n\n\n\n\n\\(\\mathsf{T}\\)\n\n\n\\(1\\)\n\n\n\\(0\\)\n\n\n\\(1\\)\n\n\n\\(2\\)\n\n\n\\(0\\)\n\n\n\\(1\\)\n\n\n\n\n\\(\\mathsf{B}\\)\n\n\n\\(0\\)\n\n\n\\(3\\)\n\n\n\\(0\\)\n\n\n\\(1\\)\n\n\n\\(2\\)\n\n\n\\(0\\)\n\n\n\nStrategy \\(\\mathsf{R}\\) is strictly dominated for player 2 (by strategy \\(\\mathsf{C}\\)). So, if \\(P_2\\) is rational, he will never choose \\(\\mathsf{R}\\). Can we eliminate strategy \\(\\mathsf{R}\\) from consideration?\nThe argument is not so simple. If \\(P_1\\) does not know that \\(P_2\\) is rational, he is liable to believe that \\(P_2\\) might choose strategy \\(\\mathsf{R}\\), in which case it would be in \\(P_1\\)’s interest to player strategy \\(\\mathsf{B}\\).\nHowever, if we postulate that\n\n\\(P_2\\) is rational, and\n\\(P_1\\) knows that \\(P_2\\) is rational.\n\nThen, \\(P_1\\) knows that \\(P_2\\) will not player strategy \\(\\mathsf{R}\\). However, \\(P_2\\) doesn’t know that \\(P_1\\) knows that \\(P_2\\) is rational. Therefore, \\(P_2\\) doesn’t know that \\(P_1\\) knows that \\(P_2\\) will not play strategy \\(\\mathsf{R}\\). Thus, we need to assume that\n\n\\(P_2\\) knows that \\(P_1\\) knows that \\(P_2\\) is rational.\n\nContinuing this way, we can argue that we we need to assume that\n\n\\(P_1\\) knows that \\(P_2\\) knows that \\(P_1\\) knows that \\(P_2\\) is rational.\nand so on.\n\nIf statements of this type hold for infinite depth, we say that the fact that \\(P_2\\) is rational is common knowledge. We will revisit common knowledge later in the course. For now, we assume the following.\n\nAssumption. It is common knowledge that all players are rational.\n\nUnder the assumption of common knowledge of rationality, we can assume that the players will eliminate strictly dominated strategies. For example, in the previous example, both players can eliminate strategy \\(\\mathsf{R}\\) from consideration and simplify the original game \\(\\mathscr{G}\\) to game \\(\\mathscr{G}_1\\) shown below.\n\n\nOriginal game \\(\\mathscr{G}\\)\n\n\n\n\n\n\\(\\mathsf{L}\\)\n\n\n\\(\\mathsf{C}\\)\n\n\n\\(\\mathsf{R}\\)\n\n\n\n\n\\(\\mathsf{T}\\)\n\n\n\\(1\\)\n\n\n\\(0\\)\n\n\n\\(1\\)\n\n\n\\(2\\)\n\n\n\\(0\\)\n\n\n\\(1\\)\n\n\n\n\n\\(\\mathsf{B}\\)\n\n\n\\(0\\)\n\n\n\\(3\\)\n\n\n\\(0\\)\n\n\n\\(1\\)\n\n\n\\(2\\)\n\n\n\\(0\\)\n\n\n\n\n\nReduced game \\(\\mathscr{G}_1\\)\n\n\n\n\n\n\\(\\mathsf{L}\\)\n\n\n\\(\\mathsf{C}\\)\n\n\n\n\n\\(\\mathsf{T}\\)\n\n\n\\(1\\)\n\n\n\\(0\\)\n\n\n\\(1\\)\n\n\n\\(2\\)\n\n\n\n\n\\(\\mathsf{B}\\)\n\n\n\\(0\\)\n\n\n\\(3\\)\n\n\n\\(0\\)\n\n\n\\(1\\)\n\n\n\nWe can continue this reasoning. In the reduced game \\(\\mathscr{G}_1\\), strategy \\(\\mathsf{B}\\) for \\(P_1\\) is dominated by strategy \\(\\mathsf{T}\\). So, we obtain the reduced game \\(\\mathscr{G}_2\\) shown below.\n\n\nReduced game \\(\\mathscr{G}_2\\)\n\n\n\n\n\n\\(\\mathsf{L}\\)\n\n\n\\(\\mathsf{C}\\)\n\n\n\n\n\\(\\mathsf{T}\\)\n\n\n\\(1\\)\n\n\n\\(0\\)\n\n\n\\(1\\)\n\n\n\\(2\\)\n\n\n\nFinally, we can eliminate strategy \\(\\mathsf{L}\\) for \\(P_2\\), which is strictly dominated by \\(\\mathsf{C}\\) to obtain game \\(\\mathscr{G}_3\\) shown below.\n\n\nReduced game \\(\\mathscr{G}_3\\)\n\n\n\n\n\n\\(\\mathsf{C}\\)\n\n\n\n\n\\(\\mathsf{T}\\)\n\n\n\\(1\\)\n\n\n\\(2\\)\n\n\n\nThis procedure is called IEDS (Iterative elimination of dominated strategies). If the procedure gives a unique strategy, that strategy is called a rationalizable strategy. (The formal definition of rationalizable strategies is different. But the case of two player games, rationalizable strategies are the seame as strategies obtained by IEDS, but they may different for games with more than two players. See Bernheim 1984)\n\nRemarks\n\n\nIn general, IEDS may not lead to a solution (i.e., we may be left with a game which is not \\(1 × 1\\) where no strategy is dominated).\nAt each step, there may be more than one strictly dominated strategy. If so, we can simultaneously eliminate all of them.\nIrrespective of the order in which strategies are eliminated, IEDS gives the same simplified game.\n\n\n\nRationalizable equilibrium are derived under the assumption of common knowledge of rationality, which appears to be a benign assumption but is not. To see why common knowledge of rationality is a strong assumption, consider the following example, which is known as the beauty contest game.\n\n\n\n\n\n\n\\(p\\)-Beauty contest game\n\n\n\nEach player picks a real number between 0 and 100. The person who was closest to half of the average of the group wins a prize.\n\n\nHow will you play this game?\n:Beauty contest games were introduced by Keynes (1936), where all participants have to guess the average. In \\(p\\)-beauty contest games, introduced by Moulin (1986), participants have to guess closest to \\(p\\) times the average, with \\(p \\in (0, 1)\\). For a discussion of an experimental study of \\(p\\)-beauty contest games, see Nagel (1995). More recently, NPR’s Planet Money ran the above game. More than 15,000 people participated. The distribution of the answers is shown in Figure 1.1. Check their website for more details.\n\n\n\n\n\n\nFigure 1.1: Histogram of the number chosen by different participants in a beauty contest game run by NRP’s Planet Money",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Rationalizable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/strategic-games.html#iterative-elimination-of-weakly-dominated-strategies",
    "href": "static-games/strategic-games.html#iterative-elimination-of-weakly-dominated-strategies",
    "title": "1  Rationalizable strategies",
    "section": "1.6 Iterative elimination of weakly dominated strategies",
    "text": "1.6 Iterative elimination of weakly dominated strategies\nWe can extend the notion of IEDS to IEWDS (Iterative elimination of weakly dominated strategies) if we make the following assumption.\n\nAssumption. A rational player never plays a weakly dominated strategy.\n\nThe assumption that a player never plays a weakly domainted strategy is less compelling than the assumption of never playing a strictly dominated strategy.\nOne way to justify the assumption of never playing a weakly dominated strategy is the concept of trembling hand principle, which is due to Selten (1975). The key idea of this principle is that the player may make mistakes when executing its strategy (hence the metaphor of trembling hand): thus all possible strategies may be used with positive probability. In such a scenario, it is never optimal to play weakly dominated strategies.\nSimilar to IEDS, we may consider an iterative procedure to eliminate weakly dominated strategies (abbreviated as IEWDS). IEWDS allows us to find solutions of games where IEDS doesn’t work (see the second price auction example below). If IEWDS gives us a unique solution, we call the resulting strategy as a rationalizable strategy\nHowever, unlike IEDS where the order of elimination of strategies does not matter, in IEWDS we can end up with different games depending on the order of elimination of strategies. For example, consider the following game:\n\n\n\n\n\n\\(\\mathsf{L}\\)\n\n\n\\(\\mathsf{C}\\)\n\n\n\\(\\mathsf{R}\\)\n\n\n\n\n\\(\\mathsf{T}\\)\n\n\n\\(1\\)\n\n\n\\(1\\)\n\n\n\\(1\\)\n\n\n\\(1\\)\n\n\n\\(0\\)\n\n\n\\(0\\)\n\n\n\n\n\\(\\mathsf{B}\\)\n\n\n\\(0\\)\n\n\n\\(0\\)\n\n\n\\(1\\)\n\n\n\\(2\\)\n\n\n\\(1\\)\n\n\n\\(2\\)\n\n\n\nNote that both strategies \\(\\mathsf{L}\\) and \\(\\mathsf{R}\\) are weakly dominated by \\(\\mathsf{C}\\).\n\nCase 1: Eliminate \\(\\mathsf{L}\\)\nIf we eliminate \\(\\mathsf{L}\\), we obtain the following reduction.\n\n\n\n\n\n\\(\\mathsf{C}\\)\n\n\n\\(\\mathsf{R}\\)\n\n\n\n\n\\(\\mathsf{T}\\)\n\n\n\\(1\\)\n\n\n\\(1\\)\n\n\n\\(0\\)\n\n\n\\(0\\)\n\n\n\n\n\\(\\mathsf{B}\\)\n\n\n\\(1\\)\n\n\n\\(2\\)\n\n\n\\(1\\)\n\n\n\\(2\\)\n\n\n\nNow, for \\(P_1\\), strategy \\(\\mathsf{T}\\) is dominated by strategy \\(\\mathsf{B}\\). Eliminating \\(\\mathsf{T}\\) we get\n\n\nGame \\(\\mathscr{G}_L\\)\n\n\n\n\n\n\\(\\mathsf{C}\\)\n\n\n\\(\\mathsf{R}\\)\n\n\n\n\n\\(\\mathsf{B}\\)\n\n\n\\(1\\)\n\n\n\\(2\\)\n\n\n\\(1\\)\n\n\n\\(2\\)\n\n\n\n\n\nCase 2: Eliminate \\(\\mathsf{R}\\)\nIf we eliminate \\(\\mathsf{R}\\) in the original game, we get the following reduction.\n\n\n\n\n\n\\(\\mathsf{L}\\)\n\n\n\\(\\mathsf{C}\\)\n\n\n\n\n\\(\\mathsf{T}\\)\n\n\n\\(1\\)\n\n\n\\(1\\)\n\n\n\\(1\\)\n\n\n\\(1\\)\n\n\n\n\n\\(\\mathsf{B}\\)\n\n\n\\(0\\)\n\n\n\\(0\\)\n\n\n\\(1\\)\n\n\n\\(2\\)\n\n\n\nHere, for \\(P_1\\), strategy \\(\\mathsf{B}\\) is weakly dominated by strategy \\(\\mathsf{T}\\). Eliminating \\(\\mathsf{B}\\), we get\n\n\nGame \\(\\mathscr{G}_R\\)\n\n\n\n\n\n\\(\\mathsf{L}\\)\n\n\n\\(\\mathsf{C}\\)\n\n\n\n\n\\(\\mathsf{T}\\)\n\n\n\\(1\\)\n\n\n\\(1\\)\n\n\n\\(1\\)\n\n\n\\(1\\)\n\n\n\nNote that games \\(\\mathscr{G}_L\\) and \\(\\mathscr{G}_R\\) cannot be reduced further. The resulting payoffs are also different.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Rationalizable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/strategic-games.html#second-price-auction",
    "href": "static-games/strategic-games.html#second-price-auction",
    "title": "1  Rationalizable strategies",
    "section": "1.7 Second price auction",
    "text": "1.7 Second price auction\nWe will study auctions in detail later in the course, but here we use the concept of dominance to identify a rationalizable strategy in what are knows as sealed bid, second price auctions. This is an example of a game with continuous strategy spaces.\n\n\n\n\n\n\nSealed bid auctions\n\n\n\nSealed bid second price auctions work as follows:\n\nAn indivisible object is for sale.\nThe set of buyers is known as \\(N\\). Each buyer \\(i\\) attaches a value \\(v_i\\) to the object, i.e., he is willing to pay at most \\(v_i\\) for the object. The value \\(v_i\\) is the buyer’s internal assessment.\nEach buyer \\(i\\) bids \\(b_i\\), presented to the auctioneer in a sealed envelop.\nThe buyer with the highest bid wins the object. If more than one buyer have the same highest bid, then the object goes to one of them at random.\nThe key feature of second price auction is that, unlike the standard auctions, the winner does not pay what he bid. Instead, he pays the second highest bid offered (and hence the name, second price auctions).\n\n\n\nHow should a rational player act in such an auction.\n\nProposition 1.1 : In sealed bid second price auctions, the bidding strategy \\(b_i = v_i\\) weakly dominates all other strategies.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider a buyer \\(i\\) with value \\(v_i\\). Given a bid profile \\(b = (b_j)_{j \\in\nN}\\), let\n\n\\(B_{-i}\\) be the highest bid by buyers other than \\(i\\).\n\\(N_{-i}\\) be the number of buyers (excluding \\(i\\)) who bid \\(B_{-i}\\)\n\nThen, the payoff to player \\(i\\) is \\[\nu_i(b) = \\begin{cases}\n  0, & \\text{if $b_i &lt; B_{-i}$} \\\\\n  \\dfrac{v_i - B_{-i}}{N_{-i} + 1}, & \\text{if $b_i = B_{-i}$} \\\\\n  v_i - B_{-i}, & \\text{if $b_i &gt; B_{-i}$}\n\\end{cases}\\]\n\n\n  \n  \n  \n    \n    \n      \n    \n  \n  \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n    \n        \n          $v_i$\n        \n    \n\n    \n\n    \n        \n          $b_i$\n        \n    \n\n    \n    \n\n    \n\n    \n\n    \n\n    \n    \n    \n    \n        \n          $v_i$\n        \n  \n  \n  \n  \n    bid = \n  \n  \n  \n  Figure: The plot of $u_i(b_i, B_{-i})$ for a fixed value of bid $b_i$ and a function of $B_{-i}$. The blue tick represents $v_i$ and the red veritical line represents the current bid.\n  Move the bid point around to see how utility function changes with \nthe bid.\n\n\n\n\nDivide the set of strategies \\(\\ALPHABET S_i = [0, ∞)\\) into three sets:\n\nUnder bid, i.e., bid less than \\(v_i\\): i.e., the set \\([0, v_i)\\).\nBid truthfully, bid equal to \\(v_i\\): i.e., the set \\(\\{v_i\\}\\).\nOver bid, i.e., bid greater than \\(v_i\\), i.e., the set \\((v_i,∞)\\).\n\nSee the plot in the figure above to see utility (as a function of \\(B_{-i}\\) for different values of \\(b_i\\).\nNote that the curve for truthful bidding (i.e., \\(b_i = v_i\\)) weakly dominates the curves for under bidding as well as over bidding.\n\n\n\nThe rationalizability of truthful bidding holds in many situations. More on that later.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Rationalizable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/strategic-games.html#dominance-by-mixed-strategies",
    "href": "static-games/strategic-games.html#dominance-by-mixed-strategies",
    "title": "1  Rationalizable strategies",
    "section": "1.8 Dominance by mixed strategies",
    "text": "1.8 Dominance by mixed strategies\nConsider a strategic game \\(\\mathscr{G} = \\langle N, (\\ALPHABET S_i)_{i \\in N},\n(u_i)_{i \\in N} \\rangle\\) where \\(\\ALPHABET S_i = \\{s_{i1}, \\dots, s_{ik}\\}\\). Then a mixed strategy \\(σ_i\\) for player \\(i\\) is a probability distribution over pure strategies, i.e., \\(σ_i \\colon \\ALPHABET S_i \\to [0,1]\\) such that \\[\n  \\sum_{s_i \\in \\ALPHABET S_i} σ_{i}(s_i) = 1\n\\]\nFor example, suppose \\(\\ALPHABET A = \\{1,2,3\\}\\) and \\(σ_i = (0.2, 0.3, 0.5)\\). This means that player \\(i\\) chooses strategy \\(1\\) with probability \\(0.2\\), strategy \\(2\\) with probability \\(0.3\\), and strategy \\(3\\) with probability \\(0.5\\).\nWhen other players are playing pure strategies \\(s_{-i}\\) and player \\(i\\) is playing a mixed strategy \\(σ_i\\), then the expected payoff to player \\(i\\) is \\[\n  U_i(σ_i, s_{-i}) = \\sum_{s_i \\in \\ALPHABET S_i} σ_{i}(s_i) u_i(s_{i}, s_{-i}).\n\\]\nFor example, consider the matching pennies game where \\(σ_1 = (p, 1-p)\\). Then,\n\\[\\begin{align*}\nU_1(σ_1, H) &= p - (1-p) = 2p - 1, \\\\\nU_1(σ_1, T) &= -p + (1-p) = 1 - 2p.\n\\end{align*}\\]\nWe can think of a mixed strategy as a virtual row or virtual column in the bimatrix representation of the game as follows:\n\n\n\n\n\n\\(\\mathsf{H}\\)\n\n\n\\(\\mathsf{T}\\)\n\n\n\n\n\\(\\mathsf{H}\\)\n\n\n\\(1\\)\n\n\n\\(-1\\)\n\n\n\\(-1\\)\n\n\n\\(1\\)\n\n\n\n\n\\(\\mathsf{T}\\)\n\n\n\\(-1\\)\n\n\n\\(1\\)\n\n\n\\(1\\)\n\n\n\\(-1\\)\n\n\n\n\n\\(σ_1\\)\n\n\n\\(2p-1\\)\n\n\n\\(\\bullet\\)\n\n\n\\(1-2p\\)\n\n\n\\(\\bullet\\)\n\n\n\nWe will revisit mixed strategies later in the course. For now, we will simply use the idea of mixed strategies to extend the notion of IEDS and IEWDS. For example consider the game shown below (where \\(\\bullet\\) means that the value is not specified because it is not important for the discussion)\n\n\n\n\n\n\\(\\mathsf{L}\\)\n\n\n\\(\\mathsf{C}\\)\n\n\n\\(\\mathsf{R}\\)\n\n\n\n\n\\(\\mathsf{T}\\)\n\n\n\\(\\bullet\\)\n\n\n\\(3\\)\n\n\n\\(\\bullet\\)\n\n\n\\(1\\)\n\n\n\\(\\bullet\\)\n\n\n\\(0\\)\n\n\n\n\n\\(\\mathsf{B}\\)\n\n\n\\(\\bullet\\)\n\n\n\\(0\\)\n\n\n\\(\\bullet\\)\n\n\n\\(1\\)\n\n\n\\(\\bullet\\)\n\n\n\\(3\\)\n\n\n\nNote that none of the strategies of \\(P_2\\) are dominated. However, if we consider the mixed strategy \\(σ_2 = (0.5, 0, 0.5)\\), then\n\\[ U_2(\\cdot, σ_2) = \\begin{bmatrix} 1.5 \\\\ 1.5 \\end{bmatrix} \\] which dominates \\(\\mathsf{C}\\). So we can eliminate strategy \\(\\mathsf{C}\\).",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Rationalizable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/strategic-games.html#cournot-competition",
    "href": "static-games/strategic-games.html#cournot-competition",
    "title": "1  Rationalizable strategies",
    "section": "1.9 Cournot competition",
    "text": "1.9 Cournot competition\nThis is a model from economics and we will study it because of its simplicity. It is based on a model proposed by Cournot in 1839. The model describes the strategic interstrategy between firms in an oligopoly market (i.e., a market where a few sellers serve the entire market; both monopoly and duopoly are special cases of oligopoly). Cournot competition is used to model competition between energy generators in electricity markets, e.g., see Willems (2002), Acemoglu et al. (2017), Lundin and Tangerås (2020).\nFormally, the Cournot competition model considers a homogeneous-goods market with \\(n\\) firms. A homogeneous-goods market is a market where all firms produce a homogeneous product which are perfect substitutes (e.g., consider a commodities market such as metal or oil or electricity).\nEach firm decides how much to produce. Let \\(s_i \\in \\reals_{\\ge 0}\\) denote the production of firm \\(i\\) and \\(a = \\sum_{i = 1}^n s_i\\) denote aggregate production. The price of the product depends on the aggregate production. We capture this relationship using a generic function \\(p \\colon \\reals_{\\ge 0} \\to \\reals_{\\ge 0}\\), i.e., price of product is \\(p(a)\\) when the aggregate production is \\(a\\).\nFurthermore, there is a cost \\(c\\) to produce one unit of goods. For simplicity, we assume that the cost is the same for all firms, though it is possible to consider the model this cost depends on the firm. Thus, the utility of firm \\(i\\) is \\[\n  u_i(s_i, s_{-i}) = s_i \\cdot p(a) - s_i \\cdot c.\n\\] We will analyze this game in the special case when the price function is \\[\n  p(a) = \\max \\{ M - a, 0 \\}\n\\] where \\(M\\) is the market capacity. Thus, we are assuming that prices are proportional to marginal demand.\n\nCase \\(n = 1\\)\nTo fix ideas, we consider the special case of \\(n=1\\) (i.e., a monopoly). In this case, for \\(s_1 \\in [0, M]\\), \\[\n  u_1(s_1) = s_1(M - s_1) - c s_1 = (M - c -s_1) s_1.\n\\] To find the optimal value of \\(s_1\\), note that \\(u_1(s_1)\\) is concave in \\(s_1\\). Thus, we pick \\(s_1\\) such that \\(∂u_1(s_1)/∂s_1 = 0\\), i.e., \\[\n  \\frac{∂u_1(s_1)}{∂s_1} = (M - c -s_1) - s_1 = 0.\n\\] Thus, \\[\n  s_1 = \\frac{M - c}{2}\n  \\quad\\text{and}\\quad\n  u_1(s_1) = \\frac{(M-c)^2}{4}.\n\\]\n\n\nCase \\(n = 2\\)\nNow, let’s consider the case when \\(n = 2\\) (a duopoly). In this case, \\[\n   u_i(s_1, s_2) = s_i( p(s_1 + s_2) - c).\n\\] Again, we can verify that for a fixed \\(s_2\\), \\(u_1(s_1, s_2)\\) is concave in \\(s_1\\) and for a fixed \\(s_1\\), \\(u_2(s_1, s_2)\\) is concave in \\(s_2\\). So, the BR can be obtained by the first order optimality condition \\(∂ u_i(s_1, s_2)/∂ s_i\n= 0\\).\nNote that \\[ p(s_1 + s_2) = \\begin{cases}\n    M - (s_1 + s_2), & \\text{if $s_1 + s_2 \\le M$} \\\\\n    0, & \\text{otherwise}\n\\end{cases}\\] Therefore, (ignoring the non-differentiable point \\(s_1 + s_2 = M\\)), we have \\[ \\frac{∂p(s_1 + s_2)}{∂s_i} = \\begin{cases}\n    -1, & \\text{if $s_1 + s_2 &lt; M$} \\\\\n    0, & \\text{otherwise}\n\\end{cases}\\] So, \\[\\begin{align*}\n\\frac{∂ u_i(s_1, s_2)}{∂ s_i}\n&= p(s_1 + s_2) - c + s_i \\frac{∂ p(s_1 + s_2)}{∂ s_i} \\\\\n&= \\begin{cases}\n    M - (s_1 + s_2) - c - s_i,  & \\text{if $s_1 + s_2 \\le M$} \\\\\n    -c, & \\text{otherwise}.\n  \\end{cases}\n\\end{align*}\\] So, if \\(s_1 + s_2 &lt; M\\), the BR of firm 1 and firm 2 are: \\[\n  B_1(s_2) = \\frac{M - c -s_2}{2}\n  \\quad\\text{and}\\quad\n  B_2(s_1) = \\frac{M - c - s_1}{2}.\n\\] We plot the BR relationships below.\n\n\n  \n  \n  \n    \n    \n      \n    \n  \n  \n    \n    \n    \n    \n    \n    \n    \n\n    \n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n        \n          $M-c$\n        \n    \n\n    \n        \n          $\\frac{M-c}{2}$\n        \n    \n\n    \n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n        \n          $\\frac{M-c}{2}$\n        \n    \n\n    \n        \n          $M-c$\n        \n    \n\n    \n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n  \n  \n  \n   Iteration = {{ step }}\n   Prev\n   Next\n  \n  \n  \n  Figure: Best response curves for Cournot competition.\n  \n\n\n\n\nNow we show that the game can be simplified using the elimination of never-best response strategies.\n\nAfter first round of elimination, we have \\(\\ALPHABET S_1 = \\ALPHABET S_2 =\n\\left[ 0, \\frac{M-c}{2} \\right]\\).\nAfter second round of elimination, we have \\(\\ALPHABET S_1 = \\ALPHABET S_2 =\n\\left[ \\frac{M-c}{4}, \\frac{M-c}{2} \\right]\\).\n…\n\nContinuing this way, we can show that the process converges to the intersection point \\(\\left( \\frac{M-c}{3}, \\frac{M-c}{3} \\right)\\) with payoff \\(\\left( \\frac{(M-c)^2}{9}, \\frac{(M-c)^2}{9}\\right)\\).\nThis example shows that we can find a rationalizable strategy for continuous strategy games as well.\n\n\n\n\n\n\nImplication of the model\n\n\n\nNote that if the firms collude and fix production to \\((M-c)/4\\) (which ensures that the total production is the same as the monopoly level), it will result in a higher price and increased profits. However, this feature depends on the assumptions of the model, some of which are unrealistic. For a more realistic market model, consider the later example of Bertrand competition, where players set prices rather than production levels.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Rationalizable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/strategic-games.html#beyond-rationalizable-strategies-maxmin-strategies",
    "href": "static-games/strategic-games.html#beyond-rationalizable-strategies-maxmin-strategies",
    "title": "1  Rationalizable strategies",
    "section": "1.10 Beyond rationalizable strategies: Maxmin strategies",
    "text": "1.10 Beyond rationalizable strategies: Maxmin strategies\nThe concept of rationalizable strategies is attractive ut it makes strong assumption about the common knowledge of rationality of all players and even then does not always precribe a solution.\nWe now describe a notation of rationality of a player that does not rely on the rationality of other others; in fact, it makes the most pessimistic assessment of their potential behavior.\nAs an example, consider the following game.\n\n\nAn illustrative game\n\n\n\n\n\n\\(\\mathsf{L}\\)\n\n\n\\(\\mathsf{R}\\)\n\n\n\n\n\\(\\mathsf{T}\\)\n\n\n\\(2\\)\n\n\n\\(1\\)\n\n\n\\(2\\)\n\n\n\\(-20\\)\n\n\n\n\n\\(\\mathsf{C}\\)\n\n\n\\(3\\)\n\n\n\\(0\\)\n\n\n\\(-10\\)\n\n\n\\(1\\)\n\n\n\n\n\\(\\mathsf{B}\\)\n\n\n\\(-100\\)\n\n\n\\(2\\)\n\n\n\\(3\\)\n\n\n\\(3\\)\n\n\n\nIn this game, the row player may prefer the strategy \\(\\mathsf{T}\\) because it gaurantees a payoff of \\(2\\) and also ensures that the “risky” payoffs of \\(-10\\) and \\(-100\\) are avoided.\nSimilarly, the column player may prefer the strategy \\(\\mathsf{L}\\) because it gaurantees a payoff of \\(1\\) and avoids the “risky” outcome of \\(-20\\).\nThis motivates the following question: What is the minimum payoff that player \\(i\\) can guarantee for himself?\nIf player \\(i\\) chooses strategy \\(s_i\\), the worst payoff that he can get is \\[\n\\min_{s_{-i} \\in \\ALPHABET S_{-i}} u_i(s_i, s_{-i}).\n\\] Player \\(i\\) can then choose the strategy \\(s_i\\) to maximize the above value. In other words, risregarding the possible rationality (or irrationality) of other players, \\(P_i\\) can gurantee for himself a payoff of \\[\n\\underline v_i = \\max_{s_i \\in \\ALPHABET S_i}\n\\min_{s_{-i} \\in \\ALPHABET S_{-i}} u_i(s_i, s_{-i}).\n\\] This quantity is called the maxmin value of player \\(i\\) (sometimes also called the security level). The strategy \\(s_i^*\\) that gurantees this value is called a maxmin strategy.\nThe maxmin strategy satisfies \\[\n\\min_{s_{-i} \\in \\ALPHABET S_{-i}} u_i(s^*_i, s_{-i})\n\\ge\n\\min_{s_{-i} \\in \\ALPHABET S_{-i}} u_i(s_i, s_{-i})\n, \\quad \\forall s_i \\in \\ALPHABET S_i\n\\] which is equivalent to \\[\n  u_i(s_i^*, s_{-i}) \\ge \\underline v_i,\n\\quad \\forall s_{-i} \\in \\ALPHABET S_{-i}.\n\\] Thus, if a player plays his maxmin strategy, his minimum guaranteed payoff equals to his maxmin value, irrespective of what the other players do.\nReturning to our example, we have\n\n\nMaxmin payoffs for the illustrative example\n\n\n\n\n\n\\(\\mathsf{L}\\)\n\n\n\\(\\mathsf{R}\\)\n\n\n\\(\\min_{s_2 \\in \\ALPHABET S_2} u_1(s_1, s_2)\\)\n\n\n\n\n\\(\\mathsf{T}\\)\n\n\n\\(2\\)\n\n\n\\(1\\)\n\n\n\\(2\\)\n\n\n\\(-20\\)\n\n\n\\(2\\)\n\n\n\n\n\\(\\mathsf{C}\\)\n\n\n\\(3\\)\n\n\n\\(0\\)\n\n\n\\(-10\\)\n\n\n\\(1\\)\n\n\n\\(-10\\)\n\n\n\n\n\\(\\mathsf{B}\\)\n\n\n\\(-100\\)\n\n\n\\(2\\)\n\n\n\\(3\\)\n\n\n\\(3\\)\n\n\n\\(-100\\)\n\n\n\n\n\\(\\min_{s_1 \\in \\ALPHABET S_1} u_2(s_1,s_2)\\)\n\n\n\n\n\\(0\\)\n\n\n\n\n\\(-20\\)\n\n\n\\(2, 0\\)\n\n\n\n\nSome remarks\n\n\nThe maxmin value of \\(P_1\\) is \\(2\\) (guaranteed by strategy \\(\\mathsf{T}\\))\n\n\n\n\nThe maxmin value of \\(P_2\\) is \\(0\\) (guaranteed by strategy \\(\\mathsf{L}\\)).\nIf both players play their maxmin strategies, the outcome is \\((\\mathsf{T}, \\mathsf{L})\\) which gives a payoff of \\((2,1)\\), in which player 2’s payoff is greater than her maxmin value.\n\nIn general, a player may have several maxmin strategies. In such a case, when the players play thier maxmin strategies, the payoff depends on which strategy they choose.\nAs an example consider the following game.\n\n\nAn example illustrating that the maxmin payoff is not unique\n\n\n\n\n\n\\(\\mathsf{L}\\)\n\n\n\\(\\mathsf{R}\\)\n\n\n\\(\\min_{s_2 \\in \\ALPHABET S_2} u_1(s_1, s_2)\\)\n\n\n\n\n\\(\\mathsf{T}\\)\n\n\n\\(3\\)\n\n\n\\(1\\)\n\n\n\\(0\\)\n\n\n\\(4\\)\n\n\n\\(0\\)\n\n\n\n\n\\(\\mathsf{B}\\)\n\n\n\\(2\\)\n\n\n\\(3\\)\n\n\n\\(1\\)\n\n\n\\(1\\)\n\n\n\\(1\\)\n\n\n\n\n\\(\\min_{s_1 \\in \\ALPHABET S_1} u_2(s_1,s_2)\\)\n\n\n\n\n\\(1\\)\n\n\n\n\n\\(1\\)\n\n\n\\(1, 1\\)\n\n\n\n\nSome remarks\n\n\nThe maxmin value of \\(P_1\\) is \\(1\\) (guaranteed by strategy \\(\\mathsf{B}\\))\n\n\n\n\nThe maxmin value of \\(P_2\\) is \\(1\\) (guaranteed by either strategy \\(\\mathsf{L}\\) or \\(\\mathsf{R}\\)).\nWhen both players play their maxmin strategies, the outcome is either \\((\\mathsf{B}, \\mathsf{L})\\) with a payoff of \\((2,3)\\) or is \\((\\mathsf{B}, \\mathsf{R})\\) with a payoff of \\((1,1)\\). Thus, the payoff depends on the strategy chosen by player 2.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Rationalizable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/strategic-games.html#relation-between-maxmin-strategies-and-dominant-strategies",
    "href": "static-games/strategic-games.html#relation-between-maxmin-strategies-and-dominant-strategies",
    "title": "1  Rationalizable strategies",
    "section": "1.11 Relation between maxmin strategies and dominant strategies",
    "text": "1.11 Relation between maxmin strategies and dominant strategies\n\nTheorem 1.1 A (strict or weak) dominant strategy of a player is also a maxmin strategy.\n\n\n\n\n\n\n\nProof\n\n\n\nA dominant strategy of a player is his best respose to any strategy of the other player.\n\n\nAs an example, consider sealed bid second price autions. We had shown that truthful bidding is a weakly dominant strategy of each player. It can also be obsereved that truthful bidding guarantees a payoff of \\(0\\), which is equal to the maxmin value of each player.\n\n\n\n\n\n\nStrategies obtained by iterative elimination\n\n\n\nStrategies obtained by iterative elimination are not maxmin strategies. Construct an example to show that.\n\n\n\nTheorem 1.2 A strictly dominant strategy equilibrium is the unique vector of maxmin strategies.\n\n\n\n\n\n\n\nProof\n\n\n\nA dominant strategy of a player is his best respose to any strategy of the other player.\n\n\nAs an example, consider prisoner’s dilemma.\n\n\nMaxmin strategies in prisoner’s dilemma\n\n\n\n\n\n\\(\\mathsf{A}\\)\n\n\n\\(\\mathsf{R}\\)\n\n\n\\(\\min_{s_2 \\in \\ALPHABET S_2} u_1(s_1, s_2)\\)\n\n\n\n\n\\(\\mathsf{A}\\)\n\n\n\\(-2\\)\n\n\n\\(-2\\)\n\n\n\\(0\\)\n\n\n\\(-3\\)\n\n\n\\(-2\\)\n\n\n\n\n\\(\\mathsf{R}\\)\n\n\n\\(-3\\)\n\n\n\\(0\\)\n\n\n\\(-1\\)\n\n\n\\(-1\\)\n\n\n\\(-3\\)\n\n\n\n\n\\(\\min_{s_1 \\in \\ALPHABET S_1} u_2(s_1,s_2)\\)\n\n\n\n\n\\(-2\\)\n\n\n\n\n\\(-3\\)\n\n\n\\(-2, -2\\)\n\n\n\nAs we can see, the maxmin strategy is \\((\\mathsf{A}, \\mathsf{A})\\), which is the same as the dominant strategy equilibrium.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Rationalizable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/strategic-games.html#exercises",
    "href": "static-games/strategic-games.html#exercises",
    "title": "1  Rationalizable strategies",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.1 Find the solution of the following games using iterative elimitation of strongly dominated strategies. Explicitly state the strategy that you are eliminating at each step and write the corresponding reduced game.\n\nGame \\(\\mathscr{G}_a\\)\n\n\n\n\n\n\n\\(\\mathsf{L}\\)\n\n\n\\(\\mathsf{C}\\)\n\n\n\\(\\mathsf{R}\\)\n\n\n\n\n\\(\\mathsf{T}\\)\n\n\n\\(73\\)\n\n\n\\(30\\)\n\n\n\\(57\\)\n\n\n\\(36\\)\n\n\n\\(66\\)\n\n\n\\(32\\)\n\n\n\n\n\\(\\mathsf{M}\\)\n\n\n\\(80\\)\n\n\n\\(26\\)\n\n\n\\(35\\)\n\n\n\\(12\\)\n\n\n\\(32\\)\n\n\n\\(54\\)\n\n\n\n\n\\(\\mathsf{B}\\)\n\n\n\\(28\\)\n\n\n\\(27\\)\n\n\n\\(63\\)\n\n\n\\(31\\)\n\n\n\\(54\\)\n\n\n\\(29\\)\n\n\n\n\nGame \\(\\mathscr{G}_b\\)\n\n\n\n\n\n\n\\(\\mathsf{L}\\)\n\n\n\\(\\mathsf{C}\\)\n\n\n\\(\\mathsf{R}\\)\n\n\n\n\n\\(\\mathsf{T}\\)\n\n\n\\(-5\\)\n\n\n\\(-1\\)\n\n\n\\(2\\)\n\n\n\\(2\\)\n\n\n\\(3\\)\n\n\n\\(3\\)\n\n\n\n\n\\(\\mathsf{M}\\)\n\n\n\\(1\\)\n\n\n\\(-3\\)\n\n\n\\(1\\)\n\n\n\\(2\\)\n\n\n\\(1\\)\n\n\n\\(1\\)\n\n\n\n\n\\(\\mathsf{B}\\)\n\n\n\\(0\\)\n\n\n\\(10\\)\n\n\n\\(0\\)\n\n\n\\(0\\)\n\n\n\\(0\\)\n\n\n\\(3\\)\n\n\n\n\n\nExercise 1.2 Find the solution of the following games using iterative elimitation of weakly dominated strategies. Explicitly state the strategy that you are eliminating at each step and write the corresponding reduced game. At each step, eliminate all weakly dominated strategies for a player.\n\nGame \\(\\mathscr{G}_a\\)\n\n\n\n\n\n\n\\(\\mathsf{A}\\)\n\n\n\\(\\mathsf{B}\\)\n\n\n\\(\\mathsf{C}\\)\n\n\n\\(\\mathsf{D}\\)\n\n\n\n\n\\(\\mathsf{T}\\)\n\n\n\\(6\\)\n\n\n\\(2\\)\n\n\n\\(6\\)\n\n\n\\(3\\)\n\n\n\\(7\\)\n\n\n\\(6\\)\n\n\n\\(2\\)\n\n\n\\(8\\)\n\n\n\n\n\\(\\mathsf{B}\\)\n\n\n\\(8\\)\n\n\n\\(5\\)\n\n\n\\(6\\)\n\n\n\\(9\\)\n\n\n\\(4\\)\n\n\n\\(6\\)\n\n\n\\(4\\)\n\n\n\\(7\\)\n\n\n\n\nGame \\(\\mathscr{G}_b\\)\n\n\n\n\n\n\n\\(\\mathsf{W}\\)\n\n\n\\(\\mathsf{X}\\)\n\n\n\\(\\mathsf{Y}\\)\n\n\n\\(\\mathsf{Z}\\)\n\n\n\n\n\\(\\mathsf{A}\\)\n\n\n\\(3\\)\n\n\n\\(7\\)\n\n\n\\(0\\)\n\n\n\\(13\\)\n\n\n\\(4\\)\n\n\n\\(5\\)\n\n\n\\(5\\)\n\n\n\\(3\\)\n\n\n\n\n\\(\\mathsf{B}\\)\n\n\n\\(5\\)\n\n\n\\(3\\)\n\n\n\\(4\\)\n\n\n\\(5\\)\n\n\n\\(4\\)\n\n\n\\(5\\)\n\n\n\\(3\\)\n\n\n\\(7\\)\n\n\n\n\n\\(\\mathsf{C}\\)\n\n\n\\(4\\)\n\n\n\\(5\\)\n\n\n\\(3\\)\n\n\n\\(7\\)\n\n\n\\(4\\)\n\n\n\\(5\\)\n\n\n\\(5\\)\n\n\n\\(3\\)\n\n\n\n\n\\(\\mathsf{D}\\)\n\n\n\\(4\\)\n\n\n\\(5\\)\n\n\n\\(4\\)\n\n\n\\(5\\)\n\n\n\\(4\\)\n\n\n\\(5\\)\n\n\n\\(4\\)\n\n\n\\(5\\)\n\n\n\n\n\nExercise 1.3 Find the solution of the following games using iterative elimitation of strongly dominated mixed strategies. Explicitly state the strategy that you are eliminating at each step and write the corresponding reduced game.\n\nGame \\(\\mathscr{G}_a\\)\n\n\n\n\n\n\n\\(\\mathsf{L}\\)\n\n\n\\(\\mathsf{C}\\)\n\n\n\\(\\mathsf{R}\\)\n\n\n\n\n\\(\\mathsf{T}\\)\n\n\n\\(2\\)\n\n\n\\(5\\)\n\n\n\\(4\\)\n\n\n\\(3\\)\n\n\n\\(6\\)\n\n\n\\(0\\)\n\n\n\n\n\\(\\mathsf{B}\\)\n\n\n\\(1\\)\n\n\n\\(1\\)\n\n\n\\(5\\)\n\n\n\\(2\\)\n\n\n\\(1\\)\n\n\n\\(4\\)\n\n\n\n\n\nExercise 1.4 For each of the games in Exercise 1.1, Exercise 1.2, and Exercise 1.3, find the maxmin strategies and the maxmin value of both players.\n\n\nExercise 1.5 Consider the following two player game: \\(\\ALPHABET S_1 = \\ALPHABET S_2 = \\{1, \\dots, 100 \\}\\) and \\[\n  u(s_1, s_2) = \\begin{cases}\n  (s_1, s_2), & \\text{if $s_1 + s_2 &lt; 100$} \\\\\n  (s_1, 100-s_1), & \\text{if $s_1 + s_2 &gt; 100$ and $s_1 &lt; s_2$}\\\\\n  (100-s_2, s_2), & \\text{if $s_1 + s_2 &gt; 100$ and $s_2 &lt; s_1$}\\\\\n  (50,50), & \\text{if $s_1 + s_2 &gt; 100$ and $s_1 = s_2$}\n\\end{cases}\\] Find the rationalizable strategies using IEWDS.\n\n\n\n\n\n\nAcemoglu, D., Kakhbod, A., and Ozdaglar, A. 2017. Competition in electricity markets with renewable energy sources. The Energy Journal 38, 1_suppl, 137–156.\n\n\nBernheim, B.D. 1984. Rationalizable strategic behavior. Econometrica: Journal of the Econometric Society, 1007–1028.\n\n\nKeynes, J.M. 1936. The general theory of employment, interest and money. Harcourt Brace; Company, New York.\n\n\nLundin, E. and Tangerås, T.P. 2020. Cournot competition in wholesale electricity markets: The nordic power exchange, nord pool. International Journal of Industrial Organization 68, 102536.\n\n\nMoulin, H. 1986. Game theory for social sciences. NYU Press, New York.\n\n\nNagel, R. 1995. Unraveling in guessing games: An experimental study. The American Economic Review 85, 5, 1313–1326.\n\n\nSelten, R. 1975. Reexamination of the perfectness concept for equilibrium points in extensive games. International Journal of Game Theory 4, 1, 25–55. DOI: 10.1007/bf01766400.\n\n\nWillems, B. 2002. Modeling cournot competition in an electricity market with transmission constraints. The Energy Journal 23, 3, 95–125.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Rationalizable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/zero-sum-games.html",
    "href": "static-games/zero-sum-games.html",
    "title": "2  Zero-sum games",
    "section": "",
    "text": "2.1 Simplified notation\nRecall the game of matching pennies.\nThis game has the property that for any strategy profile \\((s_1, s_2) \\in \\ALPHABET S\\), \\[\nu_1(s_1, s_2) + u_2(s_1, s_2) = 0.\n\\] Games with such property are called zero-sum games. In this course, we will focus on two player zero sum games (ZSG).\nFor two player ZSGs, we can simplify the notation. Since \\(u_2(s_1, s_2) = -u_1(s_1, s_2)\\), we can just specify the payoff of player 1 instead of specifying the payoff of both players. For instance, the matching pennies game can be represented as\nSimplified notation for ZSGs$\\mathsf{H}$$\\mathsf{T}$$\\mathsf{H}$$1$$-1$$\\mathsf{T}$$-1$$1$\nHere we can think of \\(P_1\\) as the maximizing player and \\(P_2\\) as the minimizing player. Thus, instead of a bimatrix representation, we will specify the payoffs by a matrix and assume that the row player is the maximizer and the column player is the minimizer.\nMoreover, we will use \\(u(s_1, s_2)\\) to denote \\(u_1(s_1, s_2)\\) and \\(-u_2(s_1, s_2)\\).\nNow recall that the maxmin levels of the two players: \\[\\begin{align*}\n  \\underline v_1\n  &= \\max_{s_1 \\in \\ALPHABET S_1} \\min_{s_2 \\in \\ALPHABET S_2}\n     u_1(s_1, s_2)\n  = \\textcolor{red}{\n    \\max_{s_1 \\in \\ALPHABET S_1} \\min_{s_2 \\in \\ALPHABET S_2}\n     u(s_1, s_2)}\n  \\\\\n  \\underline v_2\n  &= \\max_{s_2 \\in \\ALPHABET S_2} \\min_{s_1 \\in \\ALPHABET S_1}\n     u_2(s_1, s_2)\n  = \\textcolor{red}{\n    - \\min_{s_2 \\in \\ALPHABET S_2} \\max_{s_1 \\in \\ALPHABET S_1}\n     u(s_1, s_2)}\n\\end{align*}\\]\nFor ZSGs we use a simpler notation and define \\[\\begin{align*}\n  \\text{Maxmin value:} && \\underline v\n  &= \\max_{s_1 \\in \\ALPHABET S_1} \\min_{s_2 \\in \\ALPHABET S_2}\n     u(s_1, s_2)\n  \\\\\n  \\text{Minmax value:} && \\bar v\n  &= \\min_{s_2 \\in \\ALPHABET S_2} \\max_{s_1 \\in \\ALPHABET S_1}\n     u(s_1, s_2)\n\\end{align*}\\]\nAs shown in Example 2.1, in general the inequality is strict. However, for some games, the maxmin value is equal to the minmax value as seen from the following example.\nAs we have seen earlier, a two player ZSG may not have a value in pure strategies. In the sequel, we will show that if we allow mixed strategies, then every finite two player ZSG has a value!",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Zero-sum games</span>"
    ]
  },
  {
    "objectID": "static-games/zero-sum-games.html#simplified-notation",
    "href": "static-games/zero-sum-games.html#simplified-notation",
    "title": "2  Zero-sum games",
    "section": "",
    "text": "Example 2.1 Find the maxmin and minmax value of matching pennies.\n\n\nTheorem 2.1 In a two player ZSG, \\[\\underline v \\le \\bar v.\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\(s_1^*\\) be a maxmin strategy for \\(P_1\\) and \\(s_2^*\\) be a minmax strategy for \\(P_2\\). Then, by definition of maxmin strategy, we have \\[\n\\underline v\n  = \\max_{s_1 \\in \\ALPHABET S_1} \\min_{s_2 \\in \\ALPHABET S_2}\n     u(s_1, s_2)\n  = \\min_{s_2 \\in \\ALPHABET S_2} u(\\textcolor{red}{s^*_1}, s_2).\n\\] Thus, for any \\(s_2 \\in \\ALPHABET S_2\\), we have \\[ \\underline v \\le u(s_1^*, s_2). \\] Hence, by taking \\(s_2 = s_2^*\\), we get \\[\\begin{equation}\\label{eq:maxmin-bound}\n\\underline v \\le u(s_1^*, s_2^*).\n\\end{equation}\\]\nSimilarly, by the definition of minmax strategy, we have \\[\n\\bar v\n  = \\min_{s_2 \\in \\ALPHABET S_2} \\max_{s_1 \\in \\ALPHABET S_1}\n     u(s_1, s_2)\n  = \\max_{s_1 \\in \\ALPHABET S_1} u(s_1, \\textcolor{red}{s^*_2})\n\\] Thus, for any \\(s_1 \\in \\ALPHABET S_1\\), we have \\[ \\bar v \\ge u(s_1, s_2^*). \\] Hence, by taking \\(s_1 = s_1^*\\), we get \\[\\begin{equation}\\label{eq:minmax-bound}\n\\bar v \\ge u(s_1^*, s_2^*).\n\\end{equation}\\]\nCombining \\(\\eqref{eq:maxmin-bound}\\) and \\(\\eqref{eq:minmax-bound}\\), we get \\[\n\\underline v \\le u(s_1^*, s_2^*) \\le \\bar v.\n\\]\n\n\n\n\n\nExample 2.2 Find the maxmin and minmax value of the following game.\n\n\nA game where $\\underline v = \\bar v$.$\\mathsf{L}$$\\mathsf{C}$$\\mathsf{R}$$\\mathsf{T}$$2$$-1$$-2$$\\mathsf{M}$$1$$0$$1$$\\mathsf{B}$$-2$$-1$$2$\n\n\n\n\nDefinition 2.1 In a two player ZSG if \\(\\underline v = \\bar v\\), then the quantity \\[\nv \\coloneqq \\underline v = \\bar v\n\\] is called the value of the game. Any \\((\\text{maxmin}, \\text{minmax})\\) strategy of the players is called the optimal strategy profile.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Zero-sum games</span>"
    ]
  },
  {
    "objectID": "static-games/zero-sum-games.html#zero-sum-games-on-the-unit-square",
    "href": "static-games/zero-sum-games.html#zero-sum-games-on-the-unit-square",
    "title": "2  Zero-sum games",
    "section": "2.2 Zero sum games on the unit square",
    "text": "2.2 Zero sum games on the unit square\nAs an intermediate step to mixed strategies, we consider two player ZSGs on the unit square, i.e., games on which \\(\\ALPHABET S_1 = \\ALPHABET S_2 = [0,1]\\).\nFor the ease of notation, we will use \\(\\ALPHABET X = [0, 1]\\) and \\(\\ALPHABET Y = [0, 1]\\) to denote the strategy spaces of the player.\n\nExample 2.3 Consider a two player ZSG on the unit square with \\[\nu(x,y) = 4xy - 2x - y + 3, \\quad \\forall x \\in \\ALPHABET X, y \\in \\ALPHABET Y.\n\\] Does this game have a value? If so, find all optimal strategy profiles\n\n\n\n\n\n\n\nSolution\n\n\n\nTo check if the game has a value, we will compute the maxmin value \\[\n\\underline v = \\max_{x \\in \\ALPHABET X} \\min_{y \\in \\ALPHABET Y} u(x,y)\n\\] and the minmax value \\[\n\\bar v = \\min_{y \\in \\ALPHABET Y} \\max_{x \\in \\ALPHABET X} u(x,y)\n\\] and check if they are equal.\nConsider, \\[\\begin{align*}\n  \\min_{y \\in \\ALPHABET Y} u(x,y)\n  &= \\min_{y \\in [0,1]} \\bigl[ 4xy - 2x - y - 4 \\bigr] \\\\\n  &= \\min_{y \\in [0,1]} \\bigl[ (4x -1) y - 2x + 3 \\bigr].\n\\end{align*}\\] For a fixed \\(x\\), this is a linear function in \\(y\\) and the minimizer depends on the slope.\n\nIf the slope is positive, then the function is inreasing in \\(y\\) and the miniizer is \\(0\\).\nIf the slope is negative, then the function is decreasing in \\(y\\) and the minimizer is \\(1\\).\n\n\n\n\n\n\n\n\n\n\nMinimizing a linear function with positive slope\n\n\n\n\n\n\n\nMinimizing a linear function with negative slope\n\n\n\n\n\nTherefore, we have the following: \\[\n  \\min_{y \\in [0,1]} u(x,y) =\n  \\begin{cases}\n    2x + 2, & \\text{if } x &lt; \\tfrac 14 \\\\\n    2.5, & \\text{if } x = \\tfrac 14 \\\\\n   -2x + 3, & \\text{if } x &gt; \\tfrac 14\n  \\end{cases}\n\\]\nThe plot of \\(\\min_{y \\in [0,1]} u(x,y)\\) is shown below in Figure 2.1.\n\n\n\n\n\n\nFigure 2.1: Plot of \\(\\min_{y \\in [0,1]} u(x,y)\\)\n\n\n\nAs we can see from the plot, \\[\n\\bbox[5pt,border: 1px solid]{\n  \\underline v =\n  \\max_{x \\in [0,1]} \\min_{y \\in [0,1]} u(x,y) = 2.5.\n}\n\\]\nNow consider \\[\\begin{align*}\n  \\max_{x \\in \\ALPHABET X} u(x,y)\n  &= \\max_{x \\in [0,1]} \\bigl[ 4xy - 2x - y + 3 \\bigr] \\\\\n  &= \\max_{x \\in [0,1]} \\bigl[ (4y - 2) x - y + 3 \\bigr]\n\\end{align*}\\]\nBy the same argument as before, we have \\[\n  \\max_{x \\in [0,1]} u(x,y) =\n  \\begin{cases}\n    -y + 3, & \\text{if } y &lt; \\tfrac 12 \\\\\n    2.5, & \\text{if } y = \\tfrac 12 \\\\\n    3y + 1, & \\text{if } y &gt; \\tfrac 12\n  \\end{cases}\n\\]\nThe plot of \\(\\max_{x \\in [0,1]} u(x,y)\\) is shown below Figure 2.2.\n\n\n\n\n\n\nFigure 2.2: Plot of \\(\\max_{x \\in [0,1]} u(x,y)\\)\n\n\n\nAs we can see from the plot, \\[\n\\bbox[5pt,border: 1px solid]{\n  \\bar v =\n  \\min_{y \\in [0,1]} \\max_{x \\in [0,1]} u(x,y) = 2.5.\n}\n\\]\nSince \\(\\underline v = \\bar v = 2.5\\), the game has a value of \\(2.5\\). The unique optimal strategy profile is \\((\\tfrac 14, \\tfrac 12)\\).\n\n\nThe previous example is an illustration of what is known as the :Minimax Theorem. One of the foundational results of Game Theory is the von Neumann minimax theorem (von Neumann 1928).\nRecall that a function \\(f \\colon \\ALPHABET X × \\ALPHABET Y \\to \\reals\\) is called bilinear if it is linear in each argument separately, i.e.,\n\n\\(f(x_1 + x_2, y) = f(x_1,y) + f(x_2, y)\\) and \\(f(α x, y) = α f(x,y)\\);\n\\(f(x, y_1 + y_2) = f(x, y_1) + f(x, y_2)\\) and \\(f(x, αy) = α f(x,y)\\).\n\nNote that the utility function in Example 2.3 is bilinear.\n\nTheorem 2.2 (von Neumann’s Minimax Theorem) Let \\(\\ALPHABET X\\) and \\(\\ALPHABET Y\\) be compact (i.e., closed and bounded) subsets of Eucledian spaces and \\(f \\colon \\ALPHABET X × \\ALPHABET Y \\to \\reals\\) be a bilinear function. Then, \\[\n  \\max_{x \\in \\ALPHABET X} \\min_{y \\in \\ALPHABET Y} f(x,y) =\n  \\min_{y \\in \\ALPHABET Y} \\max_{x \\in \\ALPHABET X} f(x,y).\n\\]\n\nSome remarks:\n\nFor a facinating historial discussion of this result, see Kjeldsen (2001).\nFor a self contained proof of the result using elementary ideas from convex analysis, see Neumann and Morgenstern (1944), Chapter 3 and Hespanha (2017), Chapter 5.\nFor a short proof based on :Brouwer’s fixed point theorem, see Raghavan (1994).\nThe minmax theorem is equivalent to the duality theorem of linear programming. See Dantzig (1951) and Stengel (2024).\n\nIn Example 2.3, since the utility function is bilinear, we could have simply used Theorem 2.2 to conclude that the game has a value, without doing any calculations.\nA useful generalization of von Neumann’s minimax theorem is the following.\n\nTheorem 2.3 Let \\(\\ALPHABET X\\) and \\(\\ALPHABET Y\\) be compact subsets of Eucledian space. If \\(f \\colon \\ALPHABET X × \\ALPHABET Y \\to \\reals\\) is concave-convex, i.e.,\n\nfor any fixed \\(y\\), \\(f(⋅, y) \\colon \\ALPHABET X \\to \\reals\\) is concave;\nfor any fixed \\(x\\), \\(f(x, ⋅) \\colon \\ALPHABET Y \\to \\reals\\) is convex.\n\nThen, \\[\n  \\max_{x \\in \\ALPHABET X} \\min_{y \\in \\ALPHABET Y} f(x,y) =\n  \\min_{y \\in \\ALPHABET Y} \\max_{x \\in \\ALPHABET X} f(x,y).\n\\]\n\nFor a generalization that removes the compactness assumption, see Sion (1958). For a general discussion of various fixed point theorems and their relevance to zero sum games, see Raghavan (1994).",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Zero-sum games</span>"
    ]
  },
  {
    "objectID": "static-games/zero-sum-games.html#mixed-strategies-in-finite-games",
    "href": "static-games/zero-sum-games.html#mixed-strategies-in-finite-games",
    "title": "2  Zero-sum games",
    "section": "2.3 Mixed strategies in finite games",
    "text": "2.3 Mixed strategies in finite games\nWe now revist the notion of mixed strategies in finite games. Recall that given a finite game \\(\\mathscr{G} = \\langle N, (\\ALPHABET S_i)_{i \\in N}, (u_i)_{i \\in N} \\rangle\\) and mixed strategies \\(σ = (σ_i)_{i \\in N}\\) for the players, the expected utility is defined as \\[\\begin{equation} \\label{eq:expected-utility}\n  U_i(σ) = \\sum_{s_1 \\in \\ALPHABET S_1} \\cdots \\sum_{s_N \\in \\ALPHABET S_N}\n  σ_1(s_1) \\cdots σ_N(s_N) u_i(s_1, \\dots, s_N)\n\\end{equation}\\] or more compactly for two player games \\[\n  U_i(σ_1, σ_2) = \\sum_{s_1 \\in \\ALPHABET S_1} \\sum_{s_2 \\in \\ALPHABET S_2}\n  σ_1(s_1) σ_2(s_2) u_i(s_1, s_2).\n\\]\nA game in which players are playing mixed strategies may be viewed as another game, called the mixed extension, in which all players have continuous action spaces and are playing pure strategies.\n\nDefinition 2.2 (Mixed extension) Given a game \\(\\mathscr{G} = \\langle N, (\\ALPHABET S_i)_{i \\in N}, (u_i)_{i \\in N} \\rangle\\), its mixed extension is a game \\(\\mathscr{G}^* = \\langle N, ( Δ(\\ALPHABET S_i) )_{i \\in N}, (U_i)_{i \\in N} \\rangle\\) where\n\n\\(Δ(\\ALPHABET S_i)\\) denotes the set of probability distributions over \\(\\ALPHABET S_i\\)\n\\(U_i \\colon \\prod_{j \\in N} Δ(\\ALPHABET S_j) \\to \\reals\\) is the expected utility function defined in \\(\\eqref{eq:expected-utility}\\).\n\n\nFor a discussion of different interpretations of mixed strategies, see Osborne and Rubinstein (1994, Sec 3.2).\nObserve that if the original game \\(\\mathscr{G}\\) is a two player ZSG, then its mixed extension \\(\\mathscr{G}^*\\) is also a two player ZSG because for any mixed strategy \\((σ_1, σ_2)\\) \\[\\begin{align*}\nU_1(σ_1, σ_2) + U_2(σ_1, σ_2)\n&= \\sum_{s_1 \\in \\ALPHABET S_1} \\sum_{s_2 \\in \\ALPHABET S_2}\nσ_1(s_1) σ_2(s_2) \\underbrace{\\bigl[ u_1(s_1,s_2) + u_2(s_1, s_2) \\bigr]}_{=0} \\\\\n&= 0.\n\\end{align*}\\] Thus, we may simply use \\(U\\) to denote the expected utility of \\(P_1\\) with the understanding that the expected utility of \\(P_2\\) is \\(-U\\).\nFurthermore, observe that \\(U\\) is bilinear. Thus, we can conclude the following from Theorem 2.2.\n\nTheorem 2.4 For any finite game \\(\\mathscr{G}\\), its mixed extesion \\(\\mathscr{G}^*\\) has a value, which is called value of \\(\\mathscr{G}\\) in mixed strategies.\n\nThus, we finally have a solution concept that always exists, albeit for a special subclass of games.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Zero-sum games</span>"
    ]
  },
  {
    "objectID": "static-games/zero-sum-games.html#properties-of-optimal-strategy-profiles",
    "href": "static-games/zero-sum-games.html#properties-of-optimal-strategy-profiles",
    "title": "2  Zero-sum games",
    "section": "2.4 Properties of optimal strategy profiles",
    "text": "2.4 Properties of optimal strategy profiles\nThe value of a game has a nice geometric interpretation.\n\nDefinition 2.3 Let \\(\\ALPHABET X\\) and \\(\\ALPHABET Y\\) be sets and \\(f \\colon \\ALPHABET X × \\ALPHABET  Y \\to \\reals\\). A point \\((x^*, y^*) \\in \\ALPHABET X × \\ALPHABET  Y\\) is said to be a saddle point of \\(f\\) if \\[\\begin{align*}\n    f(x^*, y^*) &\\ge f(x, y^*), & \\forall x &\\in \\ALPHABET X \\\\\n    f(x^*, y^*) &\\le f(x^*, y), & \\forall y &\\in \\ALPHABET Y\n\\end{align*}\\]\n\n\n\n\n\n\nA function with a saddle point, image taken from Wikimedia\n\n\nThe term saddle point comes from the fact that the typical two dimensional example of a function with a saddle point looks like a saddle of a horse (curves up in one direction and curves down in the other).\nA key property of an optimal strategy profile is the following.\n\nTheorem 2.5 In a two player zero sum game, \\((σ_1^*, σ_2^*)\\) is a saddle point of the expected utility function \\(U\\) if and only if \\(σ_1^*\\) is an optimal strategy for player 1 and \\(σ_2^*\\) is an optimal strategy for player 2.\nIn this case \\(U(σ_1^*, σ_2^*)\\) is the value of the game.\n\n\n\n\n\n\n\nProof\n\n\n\nThis follows immediately from the definition of saddle point and that of the value of a game.\n\n\nIn general, a ZSG can have more than one optimal strategy profile. Suppose \\((σ_1^*, σ_2^*)\\) and \\((τ_1^*, τ_2^*)\\) where \\(σ_1^* \\neq τ_1^*\\) and \\(σ_2^* \\neq τ_2^*\\) both satisfy \\[\n  \\max_{σ_1 \\in Δ(\\ALPHABET S_1)} \\min_{σ_2 \\in Δ(\\ALPHABET S_2)}\n  U(σ_1, σ_2) =\n  \\min_{σ_2 \\in Δ(\\ALPHABET S_2)} \\max_{σ_1 \\in Δ(\\ALPHABET S_1)}\n  U(σ_1, σ_2) = v.\n\\] Then,\n\n\\(U(σ_1^*, σ_2^*) = U(τ_1^*, τ_2^*)\\)\nHence, if a game has a value and multiple optimal strategy profiles, then each optimal strategy profiles gives the value of the game.\n\\(U(σ_1^*, τ_2^*) = U(τ_1^*, σ_2^*) = v\\)\nHence, it does not matter which optimal strategy is chosen by players 1 and 2. Every combination of optimal strategies is an optimal strategy profile.\n\nThe proof of this statement is left as an exercise (see Exercise 2.4).",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Zero-sum games</span>"
    ]
  },
  {
    "objectID": "static-games/zero-sum-games.html#computing-the-value-of-zsg",
    "href": "static-games/zero-sum-games.html#computing-the-value-of-zsg",
    "title": "2  Zero-sum games",
    "section": "2.5 Computing the value of ZSG",
    "text": "2.5 Computing the value of ZSG\nWe now consider how to compute the value of ZSG in mixed strategies. The key simplification is as follows. When computing the maxmin value \\[\n  \\underline v = \\max_{σ_1 \\in Δ(\\ALPHABET S_1)} \\min_{σ_2 \\in Δ(\\ALPHABET S_2)} U(σ_1, σ_2),\n\\] consider the inner minimization problem for a fixed \\(σ_1 \\in Δ(\\ALPHABET S_1)\\). In this case, the set of best resposes of \\(P_2\\) will always include pure strategies. This is because, we can write \\[\n  U(σ_1, σ_2) = \\sum_{s_2 \\in \\ALPHABET S_2} σ_2(s_2) U(σ_1, s_2).\n\\] Thus, if the minimizer \\(σ_2\\) gives positive weights to pure strategies \\(s_{2,i}, s_{2,j}, s_{2,k}\\), etc., each of them must have the same \\(U(σ_1, s_2)\\); otherwise, we can omit putting positive weight on the pure strategy which has strictly large payoff and reduce the expected utility, which is not possible because \\(σ_2\\) is a minimizer.\nTherefore, when computing the maxmin value, we may consider \\[\n\\underline v = \\max_{σ_1 \\in Δ(\\ALPHABET S_1)}\n\\textcolor{red}{ \\min_{s_2 \\in \\ALPHABET S_2} } U(σ_1, \\textcolor{red}{s_2}).\n\\]\nBy a similar argument, for minmax value, we may consider \\[\n\\bar v = \\min_{σ_2 \\in Δ(\\ALPHABET S_2)}\n\\textcolor{red}{ \\max_{s_1 \\in \\ALPHABET S_1} } U(\\textcolor{red}{s_1}, σ_2).\n\\]\nTo illustrate how this simplification helps, we start with an example of a \\(2 × 2\\) game. In this case, the mixed extension is similar to the game on the unit square, which we have considered earlier.\n\nExample 2.4 Find the value in mixed strategies of the game below:\n\n\n$\\mathsf{L}$$\\mathsf{R}$$\\mathsf{T}$$3$$0$$\\mathsf{B}$$1$$2$\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nStrategy for row player\nWe first consider the row player. Suppose the row player is playing \\(σ_1 = (p, 1-p)\\), i.e., it chooses action \\(\\mathsf{T}\\) with probability \\(p\\) and action \\(\\mathsf{B}\\) with probability \\(1-p\\). Then,\n\\[\\begin{alignat*}{2}\n  U_1(σ_1, \\mathsf{L}) &= 3p + (1-p) &&= 2p + 1, \\\\\n  U_1(σ_1, \\mathsf{R}) &= 2(1-p) &&= -2p + 2.\n\\end{alignat*}\\]\nThe two payoffs are shown in Figure 2.3.\n\n\n\n\n\n\nFigure 2.3: The payoffs for Example 2.4\n\n\n\nWe now consider \\[\n  \\underline v = \\max_{p \\in [0, 1]} \\min \\bigl{ 2p + 1, -2p + 2 \\bigr\\}.\n\\] Note that the inner minimization can be easily carried out graphically. The minimization of the two curves is what is called a lower envelop which is shown in red in Figure 2.4.\n\n\n\n\n\n\nFigure 2.4: The lower envelop of the two curves of Figure 2.3\n\n\n\nSince each curve is linear, the lower envelop is convex and its maximum value is the peak point, which is also the point of intersection of the two curves and is given by \\[\n  2p + 1 - -2p + 2 \\implies\n  \\bbox[5pt,border: 1px solid]{ p = \\tfrac{1}{4} }.\n\\]\nThus, \\(σ_1^* = (\\frac 14, \\frac 34)\\) and \\[\n  \\underline v = U(σ_1^*, \\mathsf{L}) = U(σ_1^*, \\mathsf{R}) = \\tfrac 32.\n\\]\n\n\nStrategy for column player\nNow, let’s repeat the calculations for the column player. Suppose the column player is playing \\(σ_2 = (q, 1-q)\\), i.e., it chooses action \\(\\mathsf{L}\\) with probability \\(q\\) and action \\(\\mathsf{R}\\) with probability \\(1-q\\). Then,\nThe two payoffs are shown in Figure 2.5.\n\n\n\n\n\n\nFigure 2.5: The payoffs for Example 2.4\n\n\n\nWe now consider \\[\n  \\bar v = \\min_{q \\in [0, 1]} \\max \\bigl\\{3q, -q + 2 \\bigr\\}.\n\\] As before, the inner maximization can be easily carried out graphically. The maximization of the two curves is called an upper envelop which is shown in red in Figure 2.6.\n\n\n\n\n\n\nFigure 2.6: The upper envelop of the two curves of Figure 2.5\n\n\n\nSince each curve is linear, the upper envelop is concave and its minimum value is the lowest point, which is also the point of intersection of the two curves and is given by \\[\n  3q = -q + 2 \\implies\n  \\bbox[5pt,border: 1px solid]{ q = \\tfrac{1}{2} }.\n\\]\nThus, \\(σ_2^* = (\\frac 12, \\frac 12)\\) and \\[\n  \\bar v = U(\\mathsf{T}, σ_2^*) = U(\\mathsf{B}, σ_2^*) = \\tfrac 32.\n\\]\nNote that, as expected, \\(\\bar v = \\underline v\\).\n\n\n\nThe above idea works for general \\(2 × 2\\) games. See Exercise 2.1 for some examples. We now show that the idea also be extended to general \\(2 × n\\) games as well.\n\nExample 2.5 Find the value in mixed strategies of the game below:\n\n\n$\\mathsf{L}$$\\mathsf{C}$$\\mathsf{R}$$\\mathsf{T}$$2$$5$$-1$$\\mathsf{B}$$0$$-2$$5$\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nStrategy for row player\nThe row player has two pure strategies, so we use the same idea as before. Suppose the row player is playing \\(σ_1 = (p, 1-p)\\), i.e., it chooses action \\(\\mathsf{T}\\) with probability \\(p\\) and action \\(\\mathsf{B}\\) with probability \\(1-p\\). Then,\n\\[\\begin{alignat*}{2}\n  U_1(σ_1, \\mathsf{L}) &= 2p + (1-p) &&= 2p, \\\\\n  U_1(σ_1, \\mathsf{C}) &= 5p - 2(1-p) &&= 7p - 2, \\\\\n  U_1(σ_1, \\mathsf{R}) &= -p + 5(1-p) &&= -6p + 5.\n\\end{alignat*}\\]\nAs before, we can plot these functions as shown in Figure 2.7, where the lower concave envelop is shown in red.\n\n\n\n\n\n\nFigure 2.7: The lower envelop of the three payoff curves\n\n\n\nNote that the maximum of the lower convex envelop is the intersection of \\(U(σ_1, \\mathsf{L})\\) and \\(U(σ_1, \\mathsf{R})\\) which happens when \\[\n  2p = -6p + 5 \\implies\n  \\bbox[5pt,border: 1px solid]{ p = \\tfrac{5}{8} }.\n\\] Thus, \\(σ_1^* = (\\tfrac 58, \\tfrac 38)\\) and \\[\n  v = \\underline v = U(σ_1^*, \\mathsf{L}) = U(σ_1^*, \\mathsf{R}) = \\tfrac 54.\n\\]\nObserve that \\(U(σ_1^*, \\mathsf{C}) = \\tfrac{19}{8} &gt; v\\). Thus, if the row player plays \\(σ_1^*\\), then the column player will either play \\(\\mathsf{L}\\) or \\(\\mathsf{R}\\), but not \\(\\mathsf{C}\\).\n\n\nStrategy for column player\nWe now consider the column player. We cannot follow the previous procedure because we will need to construct \\(U(⋅, σ_2)\\), where \\(σ_2\\) lies in a subset of \\(\\reals^2\\). So, we take an alternative approach.\nSuppose \\(σ_1 = (p, 1-p)\\) and \\(σ_2\\) are mixed strategies. For the ease of notation, we will write \\(U(σ_1, σ_2)\\) as \\(U(p, σ_2)\\). By construction, \\(U(p, σ_2)\\) is linear in \\(p\\). Thus, if \\(σ_2^*\\) is optimal strategy for the column player, then \\[\n  U(p, σ_2^*) \\le v = \\tfrac 54, \\quad \\forall p \\in [0, 1].\n\\] Now, consider the graph in Figure 2.7. As we have previously seen, at \\(p^* = \\tfrac 58\\), \\[\nU(p^*, \\mathsf{L}) = U(p^*, \\mathsf{R}) = \\tfrac 54 = v\n\\quad\\text{but}\\quad\nU(p^*, \\mathsf{C}) &gt; \\tfrac 54 = v.\n\\] So, \\(σ_2^*\\) must give zero weight to \\(\\mathsf{C}\\); otherwise \\(U(p^*, σ_2^*)\\) cannot be equal to \\(\\tfrac 54\\). Thus, we know that \\[\n  σ_2^* = (q, 0, 1-q).\n\\] Hence, we effectively have a \\(2 × 2\\) game. For find \\(q\\), we can solve \\[\n  U(\\mathsf{T}, σ_2^*) = U(\\mathsf{B}, σ_2^*) = v = \\tfrac 54.\n\\] We know that these equations will have a consistent solution, so we only need to solve one of these. Let’s take \\[\n  U(\\mathsf{T}, σ_2^*) = 2q - (1-q) = 3q - 1 = v = \\tfrac{5}{4}.\n\\] Thus, \\[\n  \\bbox[5pt,border: 1px solid]{ q = \\tfrac{3}{4} }.\n\\]\n\n\nFinal solution\nThus, the optimal strategy is \\[\n  σ_1^* = (\\tfrac 58, \\tfrac 38)\n  \\quad\\text{and}\\quad\n  σ_2^* = (\\tfrac 34, 0, \\tfrac 14)\n\\] and the value is \\(v = \\tfrac 54\\).\n\n\n\nIn Example 2.5, the column player had three strategies but the optimal strategy randomizes between only two of them. This is an instance of the following general result.\n\nTheorem 2.6 In a two player zero-sum game where player 1 has \\(m_1\\) pure strategies and player 2 has \\(m_2\\) pure strategies, with \\(m_1 &lt; m_2\\), then player 2 has an optimal strategy that puts positive weight on at most \\(m_1\\) pure strategies.\n\nNote that Theorem 2.6 means that there exists an optimal strategy with the above feature; not that all optimal strategies have this feature.\nAn implication of Theorem 2.6 is that in a two player ZSG where one player, say the row player, has \\(2\\) pure strategies and the other player, the column player, has \\(m\\) pure strategies with \\(m &gt; 2\\), there is an optimal strategy for the column player that puts positive weight on at most two pure strategies. We can find the solution of such \\(2 × m\\) games using the procedure described above. In particular, we have the following:\n\nSTEP 1: Consider \\(σ_1 = (p, 1-p)\\) and for each pure strategy \\(s_2\\) of player 2, compute \\(U(σ_1, s_2)\\). In general, we can have six possibilities as shown in Figure 2.8.\n\n\n\n\n\n\nFigure 2.8: Different possibilities for \\(2 × m\\) games\n\n\n\nObserve the following:\n\nIn cases (a) and (f), the optimal strategy is attained on an internal point \\(p^*\\)\nIn cases (b) and (c), the optimal strategy is a boundary point and corresponds to a pure strategy.\nIn case (d) and (e), the maximum is attained in an interval; hence every point in this interval is an optimal strategy.\n\nSTEP 2: It can be shown that for player 2:\n\ncase (a) implies that the optimal strategy is an internal point\ncase (b)–(e) implies thta the optimal strategy is a pure strategy\ncase (f) implies that there is an interval of randomization probabilities that are optimal.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Zero-sum games</span>"
    ]
  },
  {
    "objectID": "static-games/zero-sum-games.html#computing-optimal-strategy-profile-using-linear-programming",
    "href": "static-games/zero-sum-games.html#computing-optimal-strategy-profile-using-linear-programming",
    "title": "2  Zero-sum games",
    "section": "2.6 Computing optimal strategy profile using linear programming",
    "text": "2.6 Computing optimal strategy profile using linear programming\nThe graphical method described in the previous section does not work if both players have more than two actions. In general, the optimal solution can be obtained using linear programming.\nLet \\((σ_1, σ_2)\\) be a candidate optimal strategy profile. Then, it must satisfy \\[\\begin{align}\n  U(σ_1, s_2) &\\ge v, & \\forall s_2 &\\in \\ALPHABET S_2\n  \\label{eq:constraint-1}\n  \\\\\n  U(s_1, σ_2) &\\le v, & \\forall s_1 &\\in \\ALPHABET S_1.\n  \\label{eq:constraint-2}\n\\end{align}\\]\nWe can write \\(\\eqref{eq:constraint-1}\\)–\\(\\eqref{eq:constraint-2}\\) as two linear programs. Suppose \\(\\ABS{\\ALPHABET S_1} = n\\) and \\(\\ABS{\\ALPHABET S_2} = m\\). For the ease of notation, we assume that \\(\\ALPHABET S_1 = \\{1, \\dots, n\\}\\) and \\(\\ALPHABET S_2 = \\{1, \\dots, m\\}\\) and use \\(u(i,j)\\) to denote the utility of player 1 (the maximizer) when player 1 plays \\(i \\in \\ALPHABET S_1\\) and player 2 plays \\(j \\in \\ALPHABET S_2\\).\nLet \\[\n  σ_1 = (p_1, \\dots, p_n)\n  \\quad\\text{and}\\quad\n  σ_2 = (q_1, \\dots, q_m)\n\\] be an optimal strategy profile of the game and \\(v\\) be the value. Then, \\(\\eqref{eq:constraint-1}\\) and \\(\\eqref{eq:constraint-2}\\) are equalent to the following linear programs.\n\\[\n\\bbox[5pt,border: 1px solid]{\n  \\begin{gathered}\n    \\max v \\\\\n    \\text{s.t. }\n    \\begin{aligned}[t]\n      & \\sum_{i=1}^n p_i u(i,j) \\ge v, \\quad \\forall j \\in \\ALPHABET S_2 \\\\\n      & p_i \\ge 0, \\quad \\forall i \\in \\{1, \\dots, n\\} \\\\\n      & \\sum_{i=1}^n p_i = 1\n   \\end{aligned}\n  \\end{gathered}}\n  \\qquad\n\\bbox[5pt,border: 1px solid]{\n  \\begin{gathered}\n    \\min v \\\\\n    \\text{s.t. }\n    \\begin{aligned}[t]\n      & \\sum_{j=1}^m q_j u(i,j) \\le v, \\quad \\forall i \\in \\ALPHABET S_1 \\\\\n      & q_j \\ge 0, \\quad \\forall j \\in \\{1, \\dots, m\\} \\\\\n      & \\sum_{j=1}^m q_j = 1\n   \\end{aligned}\n  \\end{gathered}}\n\\]\nA naive implementation of the above LP is given below:\n\nusing JuMP, GLPK\n\nfunction solve_ZSG_naive(u)\n    n, m = size(u)\n\n    ## Primal LP to find strategies of row player\n    primal = Model(GLPK.Optimizer)\n\n    @variable(primal, v_lower)\n    @variable(primal, p[1:n] &gt;= 0)\n\n    @objective(primal, Max, v_lower)\n\n    @constraint(primal, sum(p[i] for i ∈ 1:n) == 1)\n    for j ∈ 1:m\n        @constraint(primal, sum(p[i]*u[i,j] for i ∈ 1:n) &gt;= v_lower)\n    end\n\n    ## Dual LP to find strategies for column player\n    dual = Model(GLPK.Optimizer)\n\n    @variable(dual, v_upper)\n    @variable(dual, q[1:m] &gt;= 0)\n\n    @objective(dual, Min, v_upper)\n    \n    @constraint(dual, sum(q[j] for j ∈ 1:m) == 1)\n    for i ∈ 1:n\n        @constraint(dual, sum(q[j]*u[i,j] for j ∈ 1:m) &lt;= v_upper)\n    end\n    \n    JuMP.optimize!(primal)\n    JuMP.optimize!(dual)\n\n    row_strategy = JuMP.value.(p)\n    col_strategy = JuMP.value.(q)\n    game_value   = JuMP.value(v_lower) # Same as v_upper\n\n    return row_strategy, col_strategy, game_value\nend\n\nHowever, from LP duality, we know that the primal and the dual linear programs have the same solution. So, a more efficient implementation is the following, where we compute the strategies of the column player from the dual variables of the primal LP.\n\nfunction solve_ZSG(u)\n    n, m = size(u)\n\n    lp = Model(GLPK.Optimizer)\n\n    @variable(lp, v)\n    @variable(lp, p[1:n] &gt;= 0)\n\n    @objective(lp, Max, v)\n\n    q = Vector{JuMP.ConstraintRef}(undef, m)\n\n    @constraint(lp, sum(p[i] for i ∈ 1:n) == 1)\n    for j ∈ 1:m\n        q[j] = @constraint(lp, sum(p[i]*u[i,j] for i ∈ 1:n) &gt;= v)\n    end\n    JuMP.optimize!(lp)\n\n    row_strategy = JuMP.value.(p)\n    col_strategy = JuMP.dual.(q)\n    game_value   = JuMP.value(v) \n\n    return row_strategy, col_strategy, game_value\nend\n\n\nExample 2.6 Solve Example 2.5 using linear programming\n\n\n\n\n\n\n\nSolution\n\n\n\nWe first use the naive solution presented above:\n\nu = [2 5 -1; 0 -2 5]\np, q, v = solve_ZSG_naive(u)\n\n([0.6249999999999998, 0.375], [0.75, 0.0, 0.25], 1.2500000000000002)\n\n\nNext, we use the efficient solution that uses the dual variables\n\nu = [2 5 -1; 0 -2 5]\np, q, v = solve_ZSG(u)\n\n([0.6249999999999998, 0.375], [0.75, -0.0, 0.25], 1.2500000000000002)\n\n\nNote that both solutions give the same answer (up to numerical precision) that we obtained “by hand”.\n\n\nSee Exercise 2.6 for larger examples.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Zero-sum games</span>"
    ]
  },
  {
    "objectID": "static-games/zero-sum-games.html#games-with-non-compact-action-spaces",
    "href": "static-games/zero-sum-games.html#games-with-non-compact-action-spaces",
    "title": "2  Zero-sum games",
    "section": "2.7 Games with non-compact action spaces",
    "text": "2.7 Games with non-compact action spaces\nSo far, we have restricted the discussion to finite games and shown that the game has a value in mixed strategies and there exist optimal strategies that achieve the value. We now present an example to show that this result is not always true if the game is not finite.\nConsider a \\(∞ × 2\\) game with the following payoff matrix:\n\n\n$\\mathsf{L}$$\\mathsf{R}$$\\mathsf{1}$$2$$0$$\\mathsf{2}$$\\frac{1}{2}$$\\frac{1}{2}$$\\mathsf{3}$$\\frac{1}{3}$$\\frac{2}{3}$$\\mathsf{4}$$\\frac{1}{4}$$\\frac{3}{4}$$\\mathsf{5}$$\\frac{1}{5}$$\\frac{4}{5}$$\\mathsf{⋅}$$\\cdot$$\\cdot$$\\mathsf{⋅}$$\\cdot$$\\cdot$$\\mathsf{⋅}$$\\cdot$$\\cdot$\n\n\nwhere the payoffs for \\(k &gt; 1\\) are \\[\n  U(k, \\mathsf{L}) = \\frac{1}{k}\n  \\quad \\text{and} \\quad\n  U(k, \\mathsf{R}) = \\frac{k-1}{k}.\n\\]\nNote that the row player has countable infinite number of strategies. A natural solution approach in this case is to truncate the strategy space of the row player to \\(K\\) strategies and then solve the resulting finite game. Since the column player has two strategies, we use the graphical method presented above.\nIn particular, suppose the column player is playering a mixed srtategy \\(σ_2 = (q_K, 1-q_K)\\), where we use subscript \\(K\\) to empahsize the fact that this is the strategy for the \\(K × 2\\) truncated game.\nThen, we have \\[\n  U(1, σ_2) = 2q_K,\n  \\quad\\text{and}\\quad\n  U(k, σ_2) = \\frac{q_K}{k} + \\left(1 - \\frac 1k\\right)(1-q_K),\n  \\quad \\forall k &gt; 1.\n\\]\nThe payoffs for \\(K = 4\\) are shown in Figure 2.9, where the upper convex envelop is shown in red.\n\n\n\n\n\n\nFigure 2.9: Payoff profile for \\(∞ × 2\\) game\n\n\n\nObserve that the upper convex envelop consists of \\(U(1, σ_2)\\) and and \\(U(K, σ_2)\\). We find the maxmin strategy (for the truncated game) by solving \\[\n  U(1,σ_2) = U(K, σ_2)\n  \\implies\n  2q_K = \\frac{q_K}{K} + \\left(1 - \\frac 1K\\right)(1-q_K).\n\\] Solving the above, we have \\[\n  q_K = \\frac{K-1}{3K -2}\n  \\quad\\text{and}\\quad\n  v_K = \\frac{2(K-1)}{3K-2}\n\\]\nNow consider the row player. When the column player is randomizing according to \\(q_K\\), the row player gets the same payoff when playing strategies \\(1\\) and \\(K\\) and gets a lower payoff when playing any other strategy. So, the row player will choose a strategy \\[\n  σ_1 = (p_K, 0, 0, \\dots, 0, 1-p_K)\n\\] where \\(p_K\\) can be computed by solving \\[\n  U(σ_1, \\mathsf{L}) = U(σ_1, \\mathsf{R}) = v_K\n  \\implies\n  2 p_K + \\frac{1-p}{K} = \\left(1 - \\frac 1K\\right) (1 - p_K)\n  = \\frac{2(K-1)}{3K - 2}\n\\] Solving this, we get \\[\n  p_K = \\frac{K-2}{3K-2}.\n\\]\nObserve that \\[\n  \\lim_{K \\to ∞} v_K = \\tfrac{2}{3},\n\\] So, the original game has a value. However, to achieve this value, the row player has to randomize between strategies \\(1\\) and \\(K \\to ∞\\); so there is no strategy for the row player which actually achieves this value.\nThus, when action sets are not compact, the existence of a value does not imply that there is a strategy that will achieve that value.\nIn some sense, finding a strategy that achieves a value is equivalent to maximizing \\[\n  f(k) = 1 - \\frac 1k, \\quad k \\in \\naturalnumbers.\n\\] Clearly, \\(\\sup_{k \\in \\naturalnumbers} f(k) = 1\\), but there is no value of \\(k \\in \\naturalnumbers\\) which achieves \\(f(k) = 1\\).\nNonetheless, we can talk about an \\(ε\\)-optimal strategy. In the above example, pick \\[\n  σ_1 = (\\tfrac 13, 0, \\dots, 0, \\tfrac 23, 0, 0, \\dots)\n\\] where player is randomizing between strategy \\(1\\) and strategy \\(k\\). Then, \\[\n  U(σ_1, \\mathsf{L}) = \\frac 23 + \\frac{2}{3k}\n  \\quad\\text{and}\\quad\n  U(σ_1, \\mathsf{R}) = \\frac 23 \\left(1 - \\frac 1k\\right) =\n  \\frac{2}{3} - \\frac{2}{3k}.\n\\] Thus, for any \\(ε &gt; 0\\), we can guarantee a value of \\(\\frac 23 - ε\\) by choosing \\(k &gt; 2/(3 ε)\\).",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Zero-sum games</span>"
    ]
  },
  {
    "objectID": "static-games/zero-sum-games.html#sufficient-conditions-for-existence-of-optimal-strategies-for-general-strategy-spaces",
    "href": "static-games/zero-sum-games.html#sufficient-conditions-for-existence-of-optimal-strategies-for-general-strategy-spaces",
    "title": "2  Zero-sum games",
    "section": "2.8 Sufficient conditions for existence of optimal strategies for general strategy spaces",
    "text": "2.8 Sufficient conditions for existence of optimal strategies for general strategy spaces\nWe now present a general theorem that guarantees the existence of optimal strategies for a two player zero sum game with continuous action spaces.\n\nTheorem 2.7 Consider a two player ZSG where \\(\\ALPHABET S_1\\) and \\(\\ALPHABET S_2\\) are convex sets and \\(u(s_1, s_2)\\) is a continuous function and:\n\n\\(u(⋅, s_2)\\) is strictly concave for each \\(s_2\\)\n\\(u(s_1, ⋅)\\) is strictly convex for each \\(s_1\\).\n\nFurthermore, either\n\n\\(\\ALPHABET S_1\\) and \\(\\ALPHABET S_2\\) are closed and bounded\n\\(\\ALPHABET S_i = \\reals^{m_i}\\), \\(i \\in \\{1,2\\}\\) and\n\n\\(\\displaystyle \\lim_{\\NORM{s_1} \\to ∞} u(s_1, s_2) = -∞\\), for all \\(s_2 \\in \\ALPHABET S_2\\)\n\\(\\displaystyle \\lim_{\\NORM{s_2} \\to ∞} u(s_1, s_2) = ∞\\), for all \\(s_1 \\in \\ALPHABET S_1\\)\n\n\nThen the game has a unique optimal pure strategy.\n\nWhen strategy spaces are compact, a solution in mixed strategies exist under very weak conditions. This result is know as :Glicksberg Theorem. We present a simplified version of it below (in the more general setting we can relax continuity of the utility with semi-continuity).\n\nTheorem 2.8 A two player zero sum game with compact strategy spaces and continuous utility function has a value in mixed strategies.\n\nWe conclude this section by presenting an example to show that a game with continuous strategy spaces may not have a solution in pure strategies.\n\nExample 2.7 Consider a ZSG game with both players guess a number in the interval \\([0, 1]\\). Player 1 wants to be close to player while player 2 wants to be far from player 1. We model this game as follows:\nWe take \\(\\ALPHABET S_1 = \\ALPHABET S_2 = [0, 1]\\) and \\[\n    u(s_1, s_2) = - (s_1 - s_2)^2.\n  \\]\n\nObserve that the utility function is concave in both \\(s_1\\) and \\(s_2\\). So, the above example does not satisfy the sufficient conditions of Theorem 2.7. We check if a value exists from first principles.\n\n\n\n\n\n\nSolution\n\n\n\n\nStrategy for row player\nConsider the optimization problem \\[\n  \\underline v = \\max_{s_1 \\in [0,1]} \\min_{s_2 \\in [0,1]} u(s_1, s_2).\n\\] In the inner optimization problem, player 2 is minimizing a concave function. So the minima is attached at the boundary. Thus, the above optimization problem is same as one where player 2 is choosing from the discrete set \\(\\{0, 1\\}\\), that is \\[\\begin{align*}\n  \\max_{s_1 \\in [0,1]} \\min_{s_2 \\in [0,1]} u(s_1, s_2)\n  &= \\max_{s_1 \\in [0,1]} \\min_{s_2 \\in \\{0,1\\}} u(s_1, s_2)\n  \\\\ \\max_{s_1 \\in [0,1]} \\min \\{ -s_1^2, -(1-s_1)^2 \\}.\n\\end{align*}\\] Although the utility functions are not linear, we can follow the same argument as before: we plot both functions, find the lower envelop, and the find the maxmin value \\(s_1\\). See Figure 2.10.\n\n\n\n\n\n\nFigure 2.10: Payoff profile for Example 2.7\n\n\n\nThus, we have that \\[\n  s_1^* = \\tfrac 12\n  \\quad\\text{and}\\quad\n  \\underline v = - \\tfrac{1}{4}.\n\\]\n\n\nStrategy for column player\nWe know that player 2 only chooses among the actions in \\(\\{0, 1\\}\\). Suppose player 2 is playing a randomized strategy \\(σ_2\\) where it picks strategy \\(0\\) with probability \\(q\\) and strategy \\(1\\) with probability \\(1\\).\nThen, player 2 wants to solve: \\[\n  \\bar v = \\min_{q \\in [0, 1]} \\max_{s_1 \\in [0, 1]} u(s_1, s_2).\n\\]\nConsider the inner optimization problem for player 1: \\[\n  \\max_{s_1 \\in [0, 1]} u(s_1, σ_2)\n  = \\max_{s_1 \\in [0, 1]} \\bigl[\n      - q s_1^2 - (1-q) (1 - s_1)^2\n    \\bigr]\n\\] which is concave in \\(s_1\\). So, we look at the first order optimality conditions to find the optima: \\[\n  \\frac{∂u}{∂ s_1} = 0\n  \\implies\n  -2q s_1 + 2(1-q)(1-s_1) = 0\n  \\implies s_1 = 1 - q.\n\\] Thus, we have \\[\n  \\max_{s_1 \\in [0, 1]} \\min_{s_2 \\in \\ALPHABET S_2}\n  = - q (1-q)^2 - (1-q)q^2 = -q(1-q).\n\\]\nSubstituting this back in the equation for the minmax, we have \\[\n  \\bar v = \\min_{q \\in [0, 1]} - q(1-q).\n\\] By the :AM-GM inequality, we know that the minima is achieved at \\(q = \\tfrac 12\\). Therefore, \\[\n  \\bar v = - \\tfrac 14.\n\\]\nTherefore, we have that \\(\\underline v = \\bar v = -\\tfrac 14\\); thus, the game has a value, where player 1 plays a pure strategy of \\(s_1 = \\tfrac 12\\) and player 2 randomizes between \\(0\\) and \\(1\\) with equal probability (this makes sense from the description of the problem).\n\n\n\nIn the above example, the utility function was not convex in \\(s_2\\). The game does not have a value in pure strategies, but it still has a value in mixed strategies. There are general sufficient conditions to verify the existence of value in mixed strategies for games with continuous strategy spaces, but we will not discuss them in the course.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Zero-sum games</span>"
    ]
  },
  {
    "objectID": "static-games/zero-sum-games.html#exercises",
    "href": "static-games/zero-sum-games.html#exercises",
    "title": "2  Zero-sum games",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 2.1 For each of the following games, find the value of the game and the optimal strategy profile.\n\n\nGame 1$\\mathsf{L}$$\\mathsf{R}$$\\mathsf{T}$$6$$0$$\\mathsf{B}$$-3$$3$\n\n\nGame 2$\\mathsf{L}$$\\mathsf{R}$$\\mathsf{T}$$-3$$8$$\\mathsf{B}$$4$$4$\n\n\n\n\nExercise 2.2 Find the optimal strategy profile and the value of the following game.\n\n\n$\\mathsf{L}$$\\mathsf{C}$$\\mathsf{R}$$\\mathsf{T}$$0$$4$$6$$\\mathsf{M}$$5$$7$$4$$\\mathsf{B}$$9$$6$$3$\n\n\nHint: Use iterative elimination of strongly dominated strategies to first reduce the above to a \\(2 × 2\\) game and then simplify the resulting game.\n\n\nExercise 2.3 Find all optimal strategies of the following games. Clearly explain all your steps.\n\n\nGame 1$\\mathsf{L}$$\\mathsf{C}$$\\mathsf{R}$$\\mathsf{T}$$3.0$$1.2$$-1.0$$\\mathsf{B}$$1.0$$1.2$$2.0$\n\n\nGame 2$\\mathsf{L}$$\\mathsf{C}$$\\mathsf{R}$$\\mathsf{T}$$1$$\\frac{4}{3}$$3$$\\mathsf{B}$$3$$0$$1$\n\n\nGame 3$\\mathsf{L}$$\\mathsf{C}$$\\mathsf{R}$$\\mathsf{T}$$1$$-1$$0$$\\mathsf{B}$$2$$4$$3$\n\n\n\n\nExercise 2.4 Suppose \\((σ_1^*, σ_2^*)\\) and \\((τ_1^*, τ_2^*)\\) both satisfy \\[\n  \\max_{σ_1 \\in Δ(\\ALPHABET S_1)} \\min_{σ_2 \\in Δ(\\ALPHABET S_2)}\n  U(σ_1, σ_2) =\n  \\min_{σ_2 \\in Δ(\\ALPHABET S_2)} \\max_{σ_1 \\in Δ(\\ALPHABET S_1)}\n  U(σ_1, σ_2) = v.\n\\] Prove that \\[\n  U(σ_1^*, τ_2^*) = U(τ_1^*, σ_2^*) = v.\n\\]\nHint: Use Theorem 2.5 to argue that \\[\n  U(τ_1^*, τ_2^*) \\le U(τ_1^*, σ_2^*) \\le U(σ_1^*, σ_2^*)\n  \\quad\\text{and}\\quad\n  U(σ_1^*, σ_2^*) \\le U(σ_1^*, τ_2^*) \\le U(τ_1^*, τ_2^*).\n\\]\n\n\nExercise 2.5 A zero-sum game with \\(\\ALPHABET S_1 = \\ALPHABET S_2\\) is called symmetric if the utlity function is skew-symmetric, i.e., \\[\n  u(s_1, s_2) = - u(s_2, s_1), \\quad \\forall s_1, s_2 \\in \\ALPHABET S_1.\n\\] An example of a symmetric game is rock-paper-scissors.\nLet \\(\\mathscr{G}\\) be a finite symmetric game. Show that:\n\nThe value of \\(\\mathscr{G}\\) in mixed strategies is zero.\nIf \\((σ_1^*, σ_2^*)\\) is an optimal (mixed) strategy for \\(\\mathscr{G}\\), then \\((σ_2^*, σ_1^*)\\) is also an optimal (mixed) strategy.\nUse part b and Exercise 2.4 to argue that a symmetric zero-sum game always has a symmetric optimal (mixed) strategy of the form \\((σ^*, σ^*)\\), where both players are playing the same mixed strategy.\n\n\n\nExercise 2.6 Use the LP formulation to find optimal strategy profile of the following games.\n\n\nGame 1$\\mathsf{1}$$\\mathsf{2}$$\\mathsf{3}$$\\mathsf{1}$$3$$-1$$2$$\\mathsf{2}$$1$$2$$-2$\n\n\nGame 2$\\mathsf{1}$$\\mathsf{2}$$\\mathsf{3}$$\\mathsf{4}$$\\mathsf{1}$$6$$0$$5$$6$$\\mathsf{2}$$-3$$3$$-4$$3$$\\mathsf{3}$$8$$1$$2$$2$\n\n\nNote that part of the objective of this exercise is for you to learn how to use a linear programming solver. So, instead of blindly copy pasting the code provided above, implement the solution from the equations using the programming language of your choice.\n\n\nExercise 2.7 Consider a two player zero-sum game with \\(\\ALPHABET S_1 = \\ALPHABET S_2 = \\reals\\).\n\\[\n  u(s_1, s_2) = - s_1^2 + s_2^2 - 2 s_1 s_2 - s_1 + 2 s_2.\n\\]\n\nShow that the game satisfies the sufficient conditions of Theorem 2.7. Therefore, the game has a value and a unique optimal strategy.\nFind the maxmin strategy of player 1 and the maxmin value of the game by solving the following: \\[\n  \\underline v = \\sup_{s_1 \\in \\reals} \\inf_{s_2 \\in \\reals} u(s_1, s_2).\n\\]\nFind the minmax strategy of player 2 and the minmax value of the game by solving the following: \\[\n  \\bar v = \\inf_{s_2 \\in \\reals} \\sup_{s_1 \\in \\reals} u(s_1, s_2).\n\\]\nIs the maxmin value same as the minmax value?\n\n\n\n\n\n\nDantzig, G.B. 1951. Activity analysis ofproduetion and allocation. In: T.C. Koopmans, ed., John Wiley, New York, 333–335.\n\n\nHespanha, J.P. 2017. Noncooperative game theory: An introduction for engineers and computer scientists. Princeton University Press. DOI: 10.1515/9781400885442.\n\n\nKjeldsen, T.H. 2001. John von neumann’s conception of the minimax theorem: A journey through different mathematical contexts. Archive for History of Exact Sciences 56, 1, 39–68. Available at: http://www.jstor.org/stable/41134130 (Accessed: January 5, 2025).\n\n\nNeumann, J. von and Morgenstern, O. 1944. Theory of games and economic behaviour. Princeton University Press.\n\n\nOsborne, M.J. and Rubinstein, A. 1994. A course in game theory. MIT Press.\n\n\nRaghavan, T.E.S. 1994. Chapter 20 zero-sum two-person games. In: Handbook of game theory with economic applications. Elsevier, 735–768. DOI: 10.1016/s1574-0005(05)80052-9.\n\n\nSion, M. 1958. On general minimax theorems. Pacific Journal of Mathematics 8, 1, 171–176. DOI: 10.2140/pjm.1958.8.171.\n\n\nStengel, B. von. 2024. Zero-sum games and linear programming duality. Mathematics of Operations Research 49, 2, 1091–1108. DOI: 10.1287/moor.2022.0149.\n\n\nvon Neumann, J. 1928. Zur theorie der gesellschaftsspiele. Mathematische Annalen 100, 1, 295–320. DOI: 10.1007/bf01448847.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Zero-sum games</span>"
    ]
  },
  {
    "objectID": "static-games/jamming-games.html",
    "href": "static-games/jamming-games.html",
    "title": "3  Jamming Games",
    "section": "",
    "text": "3.1 System Model\n:Radio frequency (RF) jamming is the act of blocking or causing interference to radio or wireless communication by transmitting noise to decrease the signal to noise ratio. Jamming of radio transmission orginated in World War II and is still used today in military and civilian conflicts.\nWe present a simple model of jamming and obtain a solution using tools from ZSG from the previous section.\nConsider a line of sight communication link between a transmitter (Tx) and a receiver (Rx). There is a jammer (Jm) who wants to disrupt this communication link.\nThe communication link consists of \\(m\\) orthogonal frequency bands, each of bandwidth \\(B\\). Let \\(M \\coloneqq \\{1, \\dots, m\\}\\) denote the set of bands. For each band \\(i \\in \\ALPHABET M\\), let\nIt is assumed that both the Tx and Jm have constraints on the total power that they can use. In particular, \\[\n  \\sum_{i \\in \\ALPHABET M} x_i \\le P_T\n  \\quad\\text{and}\\quad\n  \\sum_{i \\in \\ALPHABET M} y_i \\le P_J.\n\\]\nLet \\[\\begin{align*}\n  \\ALPHABET X &= \\biggl\\{ x \\in \\reals^m_{\\ge 0} :\n                        \\sum_{i \\in \\ALPHABET M} x_i \\le P_T \\biggr\\},\n  \\\\\n  \\ALPHABET Y &= \\biggl\\{ y \\in \\reals^m_{\\ge 0} :\n                        \\sum_{i \\in \\ALPHABET M} y_i \\le P_J \\biggr\\}.\n\\end{align*}\\]\nThen the above constraints may be written as \\[\n  x = (x_1, \\dots, x_m) \\in \\ALPHABET X\n  \\quad\\text{and}\\quad\n  y = (y_1, \\dots, y_m) \\in \\ALPHABET Y.\n\\]\nA basic result from communication theory is that the capacity of a narrowband channel with bandwidth \\(B\\) and signal to noise ratio \\(\\text{SNR}\\) is \\[\n  C = B \\log(1 + \\text{SNR}).\n\\]\nUsing this expression, the capacity of the Tx when the Tx uses power \\(x \\in \\ALPHABET X\\) and $Jm uses power \\(y \\in \\ALPHABET Y\\) is \\[\n  u(x,y) = \\sum_{i \\in M} B \\log\\left( 1 + \\frac{x_i}{y_i + σ_i}\\right).\n\\] Thus, we can consider a zero-sum jamming game between the Tx and the Jm, where the Tx wants to maximize \\(u(x,y)\\) and the Jm wants to minimize it.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jamming Games</span>"
    ]
  },
  {
    "objectID": "static-games/jamming-games.html#system-model",
    "href": "static-games/jamming-games.html#system-model",
    "title": "3  Jamming Games",
    "section": "",
    "text": "\\(σ_i \\in \\reals_{\\ge 0}\\) denote the background noise power\n\\(x_i \\in \\reals_{\\ge 0}\\) denote the signal power of Tx\n\\(y_i \\in \\reals_{\\ge 0}\\) denote the interference power of Jm.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jamming Games</span>"
    ]
  },
  {
    "objectID": "static-games/jamming-games.html#existence-of-value-in-pure-strategies",
    "href": "static-games/jamming-games.html#existence-of-value-in-pure-strategies",
    "title": "3  Jamming Games",
    "section": "3.2 Existence of value in pure strategies",
    "text": "3.2 Existence of value in pure strategies\n\nProposition 3.1 The two player ZSG formulated above has a value in pure strategies and the optimal strategy is unique.\n\n\n\n\n\n\n\nProof\n\n\n\nWe will prove the result by showing that the model satisfies the sufficient conditions of Theorem 2.7.\n\nCompactness of \\(\\ALPHABET X\\) and \\(\\ALPHABET Y\\)\nBy construction, we can see that \\(\\ALPHABET X\\) and \\(\\ALPHABET Y\\) are closed and bounded subsets of Eucledian space. Hence, both sets are compact.\n\n\nStrict concavity of \\(u(x,y)\\) in \\(x\\)\nFix \\(y\\) and consider the function \\(f(x) = u(x,y)\\). Then, \\[\n\\frac{∂ f}{∂x_i} = \\frac{∂}{∂ x_i}\n  \\sum_{j \\in \\ALPHABET M} B \\log\\left(1 + \\frac{x_j}{y_j + σ_j} \\right)\n  = \\frac{B}{1 + \\dfrac{x_i}{y_i + σ_i}} \\cdot \\frac{1}{y_i + σ_i}\n  = \\frac{B}{x_i + y_i + σ_i}.\n\\] Moreover \\[\n\\frac{∂^2 f}{∂x_i ∂x_j}\n= \\begin{cases}\n    \\frac{-B}{(x_i + y_i + σ_i)^2}, & \\hbox{if $i = j$} \\\\\n    0, & \\hbox{otherwise}\n  \\end{cases}\n\\] Thus, the Hessian of \\(f\\) is \\[\n  \\GRAD_x f = \\def\\1#1{\\frac{-B}{(x_{#1} + y_{#1} + σ_{#1})^2}}\n  \\diag\\left( \\1{1}, \\dots, \\1{m} \\right)\n\\] which is a diagonal matrix with a negative diagonal. Recall that the eigenvalues of a diagonal matrix is the diagonal elements. Thus, all eigenvalues are negative and hence the Hessian is negative definite. Thus, \\(f\\) is strictly concave.\n\n\nStrict convexity of \\(u(x,y)\\) in \\(y\\)\nFix \\(x\\) and consider the function \\(g(y) = u(x,y)\\). Then, \\[\\begin{align*}\n\\frac{∂ g}{∂y_i} &= \\frac{∂}{∂ y_i}\n  \\sum_{j \\in \\ALPHABET M} B \\log\\left(1 + \\frac{x_j}{y_j + σ_j} \\right)\n  \\\\\n  &= \\frac{∂}{∂ y_i} B \\log\\left(1 + \\frac{x_i}{y_i + σ_i} \\right)\n  = \\frac{∂}{∂ y_i} B \\log\\left(\\frac{x_i + y_i + σ_i}{y_i + σ_i} \\right)\n  \\\\\n  & = \\frac{B}{x_i + y_i + σ_i} - \\frac{B}{y_i + σ_i}.\n\\end{align*}\\] Moreover \\[\n\\frac{∂^2 g}{∂x_i ∂x_j}\n= \\begin{cases}\n    \\frac{-B}{(x_i + y_i + σ_i)^2} + \\frac{B}{(y_i + σ_i)^2}, & \\hbox{if $i = j$} \\\\\n    0, & \\hbox{otherwise}\n  \\end{cases}\n\\] Observe that \\[\n\\frac{-B}{(x_i + y_i + σ_i)^2} + \\frac{B}{(y_i + σ_i)^2}\n= \\frac{B (x_i^2 + 2x_i y_i + 2x_i σ_i)}{(x_i + y_i + σ_i)^2 (y_i + σ_i)^2}\n\\] which is always positive for \\(x_i, y_i \\in \\reals_{\\ge 0}\\).\nThus, the Hessian of \\(f\\) is \\[\n  \\GRAD_x f = \\def\\1#1{\\frac{-B}{(x_{#1} + y_{#1} + σ_{#1})^2} + \\frac{B}{(y_{#1} + σ_{#1})^2}}\n  \\diag\\left( \\1{1}, \\dots, \\1{m} \\right)\n\\] which is a diagonal matrix with a positive diagonal. Recall that the eigenvalues of a diagonal matrix is the diagonal elements. Thus, all eigenvalues are positive and hence the Hessian is positive definite. Thus, \\(g\\) is strictly convex.\n\n\nExistence of a value in pure strategies\nSince all the conditions of Theorem 2.7 are satisfied. Thus, the game has a value in pure strategies and the optimal strategy is unique.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jamming Games</span>"
    ]
  },
  {
    "objectID": "static-games/jamming-games.html#structre-of-optimal-strategies",
    "href": "static-games/jamming-games.html#structre-of-optimal-strategies",
    "title": "3  Jamming Games",
    "section": "3.3 Structre of optimal strategies",
    "text": "3.3 Structre of optimal strategies\nNow that we know that the game has a value in pure strategies and the optimal strategy is unique, we search for an optimal solution.\n\n3.3.1 Optimal strategy of the transmitter\nWe know that \\[\nv = \\min_{y \\in \\ALPHABET Y} \\max_{x \\in \\ALPHABET X} u(x,y).\n\\] Consider the inner optimization problem: for a given \\(y\\) compute \\[\n  \\max_{x \\in \\ALPHABET X} u(x,y)\n\\] or \\[\n  \\max_{x \\in \\reals^m} \\sum_{i \\in \\ALPHABET M} B \\log\\left(1 + \\frac{x_i}{y_i + σ_i} \\right)\n\\] such that \\[\\begin{align*}\n  \\sum_{i \\in \\ALPHABET M} x_i &\\le P_T, \\\\\n  x_i &\\ge 0, \\quad \\forall i \\in \\ALPHABET M\n\\end{align*}\\] This is a constrained optimization problem with a concave objective and convex constraint set. So, we can find the optimal solution using :KKT conditions.\nLet us consider Lagrange multiplier \\(λ\\) for the first constraint and multiplies \\(α_i\\) for the non-negativity constraints. The Lagrange relaxation is \\[\n\\sum_{i \\in \\ALPHABET M} B \\log\\left(1 + \\frac{x_i}{y_i + σ_i}\\right)\n- λ \\biggl( \\sum_{i \\in \\ALPHABET M} x_i - P_T \\biggr)\n+ \\sum_{i \\in M} α_i x_i.\n\\] The KKT conditions are:\n\nOptimality equation \\[\n\\frac{∂ (\\hbox{Lagrange relaxation})}{∂x_i}\n= \\frac{B}{x_i + y_i + σ_i} - λ + α_i = 0.\n\\]\nComplementary slackness conditions \\[\\begin{align*}\nα_i &\\ge 0 & \\text{and} && α_i x_i &= 0 \\\\\nλ & \\ge 0  & \\text{and} && λ\\biggl( \\sum_{i \\in \\ALPHABET M} x_i - P_T \\biggr) &= 0.\n\\end{align*}\\]\n\nThe optimality equation implies that \\[\n  α_i = λ - \\frac{B}{x_i + y_i + σ_i}.\n\\] The first complementary slackness condition requires that \\(α_i \\ge 0\\). Thus, \\[\\begin{equation}\\label{eq:cond-1}\n  λ \\ge \\frac{B}{x_i + y_i + σ_i}.\n\\end{equation}\\] Moreover, since \\(α_i x_i = 0\\), we have \\[\\begin{equation}\\label{eq:cond-2}\n  \\biggl(λ - \\frac{B}{x_i + y_i + σ_i}\\biggr) x_i = 0.\n\\end{equation}\\]\nWe now consider two cases:\n\nCase 1: \\(λ &lt; B/(y_i + σ_i)\\).\nThen \\(x_i = 0\\) is impossible, which we prove by contraction. If \\(x_i = 0\\), then by \\(\\eqref{eq:cond-1}\\) \\[λ \\ge \\frac{B}{(x_i + y_i + σ_i)} = \\frac{B}{(y_i + σ_i)}\\] which violates the assumption that \\(λ &lt; B/(y_i + σ_i)\\).\nHence, \\(x_i &gt; 0\\). Then, \\(\\eqref{eq:cond-2}\\) implies that \\[\n    λ = \\frac{B}{x_i + y_i + σ_i}\n    \\implies\n    x_i = λ^{-1} B - y_i - σ_i.\n\\]\nCase 2: \\(λ \\ge B/(y_i + σ_i)\\)\nThen \\(x_i &gt; 0\\) is impossible, which we prove by contradiction. If \\(x_i &gt; 0\\), then \\[\n  λ \\ge \\frac{B}{y_i + σ_i} &gt; \\frac{B}{x_i + y_i + σ_i},\n\\] which violates \\(\\eqref{eq:cond-1}\\). Then \\(x_i = 0\\).\n\nThus, we have \\[\\begin{align*}\n  x_i^* &= \\begin{cases}\n    λ^{-1}B - y_i - σ_i, & \\hbox{if } λ &lt; \\frac{B}{y_i + σ_i} \\\\\n    0, & \\hbox{if } λ \\ge \\frac{B}{y_i + σ_i}\n  \\end{cases}\n  \\\\\n  &= [ λ^{-1}B - y_i - σ_i]^{+}\n\\end{align*}\\] where the notation \\([z]^{+}\\) means \\(\\max\\{z, 0\\}\\).\nIf \\(λ &gt; 0\\), then the complementary slackness condition implies that \\[\\begin{equation}\\label{eq:lambda}\n  \\sum_{i \\in \\ALPHABET M} \\big[ λ^{-1} B - y_i - σ_i \\bigr]^{+} = P_T.\n\\end{equation}\\] The LHS is a piecewise linear and increasing in \\(λ^{-1}\\). So, we can always find a solution.\nAs an illustration, suppose the power levels \\(y_i + σ_i\\) are given as in Figure 3.1 (a). Let \\(λ\\) denote the solution of \\(\\eqref{eq:lambda}\\) which is shown geometrically in Figure 3.1 (b). This solution is called a water filling solution for the following reason: Suppose a region is divided into \\(m\\) cells (of unit width) and the ground level above cell \\(i\\) is \\(y_i + σ_i\\) and then flood the region with region with a \\(P_T\\) amount of water. Then, the new water level at cell \\(i\\) will be \\(x_i^*\\).\n\n\n\n\n\n\nFigure 3.1: Water-filling solution for the transmitter. Plot (a) shows the “interference” levels \\(y_i + σ_i\\). In plot (b), the blue shaded region shows the power levels \\(x_i^*\\).\n\n\n\n\n\n3.3.2 Optimal strategy of the jammer\nTo find the optimal strategy of the jammer, we reconsider consider the minmax problem \\[\n  v = \\min_{y \\in \\ALPHABET Y} \\max_{x \\in \\ALPHABET X} u(x,y)\n  = \\min_{y \\in Y} u(x^*, y)\n\\] where \\(x^*_i = [λ^{-1}B - y_i - σ_i]^{+}\\), as determined in the previous step. This optimization problem can be written as \\[\n  \\min_{y \\in \\reals^m}\n    \\sum_{i \\in \\ALPHABET M} B \\log\\left( 1 + \\frac{x_i^*}{y_i + σ_i} \\right)\n\\] such that \\[\\begin{align*}\n  \\sum_{i \\in \\ALPHABET M} y_i &\\le P_J \\\\\n  y_i & \\ge 0, \\quad \\forall i \\in \\ALPHABET M.\n\\end{align*}\\] Consider the Lagrange relaxation, were \\(μ\\) is the Lagrange multiplier for the first constraint and \\(β_i\\) is the Lagrange multiplier for the non-negativity constraints. The Lagrange relaxation is \\[\n   \\sum_{i \\in \\ALPHABET M} B \\log(x_i^* + y_i + σ_i)\n- \\sum_{i \\in \\ALPHABET M} B \\log(        y_i + σ_i)\n+ μ \\biggl( \\sum_{i \\in \\ALPHABET M} y_i - P_J \\biggr)\n- \\sum_{i \\in \\ALPHABET M} β_i y_i\n\\] The KKT conditions are:\n\nOptimality equation: \\[\n\\frac{∂ (\\hbox{Lagrange relaxation})}{∂y_i}\n= \\frac{B}{x_i^* + y_i + σ_i} - \\frac{B}{y_i + σ_i} +  μ - β_i = 0.\n\\]\nComplementary slackness: \\[\\begin{align*}\n  β_i &\\ge 0 & \\text{and} && β_i y_i &= 0 \\\\\n  μ & \\ge 0  & \\text{and} && μ\\biggl( \\sum_{i \\in \\ALPHABET M} y_i - P_J \\biggr) &= 0.\n\\end{align*}\\]\n\nConsider a channel where \\(x_i^* = 0\\). The optimality euqations imply that for such a channel \\(β_i = μ &gt; 0\\). Thus, complementary slackness condition implies that \\(y_i^* = 0\\).\nConsider a channel where \\(x_i^* &gt; 0\\). From the solution of the previous section, we know that for this case \\(x^*_i = λ^{-1}B - y_i - σ_i\\). Thus, the optimality equation simplifies to \\[\n    λ + μ = β_i + \\frac{B}{y_i + σ_i}\n\\] The first complementary slackness condition requires that \\(β_i \\ge 0\\). Thus, \\[\\begin{equation}\\label{eq:cond-3}\n  λ + μ \\ge \\frac{B}{y_i + σ_i}.\n\\end{equation}\\]. Moreover, since \\(β_i y_i = 0\\), we have \\begin{equation} ( λ + μ - ) y_i = 0. $$\nWe now consider two cases:\n\nCase 1: \\(λ + μ &lt; B/σ_i\\).\nThen \\(y_i = 0\\) is impossible, which we prove by contraction. If \\(y_i = 0\\), then by \\(\\eqref{eq:cond-3}\\) we have \\[\n  λ + μ \\ge \\frac{B}{y_i + σ_i} = \\frac{B}{σ_i}\n\\] which violates the assumption that \\(λ + μ &lt; B/σ_i\\).\nHence, \\(y_i &gt; 0\\). Then, \\(\\eqref{eq:cond-4}\\) implies that \\[\n  λ + μ = \\frac{B}{y_i + σ_i} \\implies y_i = (λ + μ)^{-1}B - σ_i.\n\\]\nCase 2: \\(λ + μ \\ge B/σ_i\\).\nThen \\(y_i &gt; 0\\) is impossible, which we prove by contraction. If \\(y_i &gt; 0\\), then \\[\n  β_i = λ + μ - \\frac{B}{y_i + σ_i} &gt; λ + μ - \\frac{B}{σ_i} &gt; 0.\n\\] Then, from complementary slackness conditions, we get that \\(y_i = 0\\), which contradicts the assumption that \\(y_i &gt; 0\\). Thus, \\(y_i\\) must be \\(0\\).\n\nThus, we have \\[\\begin{align*}\n  y_i^* &= \\begin{cases}\n    (λ + μ)^{-1}B - σ_i, & \\hbox{if } λ + μ &lt; \\frac{B}{σ_i} \\\\\n    0, & \\hbox{if } λ + μ \\ge \\frac{B}{σ_i}.\n  \\end{cases}\n  \\\\\n  &= \\bigl[ (λ+μ)^{-1}B - σ_i \\bigr]^{+}.\n\\end{align*}\\]\nIf \\(μ &gt; 0\\), then complementary slackness implies that \\[\n  \\sum_{i \\in \\ALPHABET M} \\bigl[ (λ + μ)^{-1} B - σ_i \\bigr]^{+} = P_J.\n\\] As before, we can argue that this equation has a solution, which we can find using water-filling.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jamming Games</span>"
    ]
  },
  {
    "objectID": "static-games/jamming-games.html#summary-of-the-solution",
    "href": "static-games/jamming-games.html#summary-of-the-solution",
    "title": "3  Jamming Games",
    "section": "3.4 Summary of the solution",
    "text": "3.4 Summary of the solution\nThe main steps of finding the optimal \\(x\\) and \\(y\\) are as follows:\n\nDetermine \\((λ + μ)\\) which sastisfies \\[\n  \\sum_{i \\in \\ALPHABET M} \\bigl[ (λ + μ)^{-1} B - σ_i \\bigr]^{+} = P_J.\n\\]\nIf \\((λ + μ) &gt; 0\\), set \\(y_i^* = \\big[ (λ+μ)^{-1} B - σ_i \\bigr]^{+}\\). Otherwise, set \\(y_i^* = 0\\) for all \\(i\\).\nDetermine \\(λ\\) which satisfies \\[\n  \\sum_{i \\in \\ALPHABET M} \\bigl[ λ^{-1} B - y_i^* - σ_i \\bigr]^{+} = P_T.\n\\]\nIf \\(λ &gt; 0\\), set \\(x_i^* = \\bigl[ λ^{-1}B - y_i^* - σ_i \\bigr]^{+}\\). Otherwise, set \\(x_i^* = 0\\) for all \\(i\\).",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jamming Games</span>"
    ]
  },
  {
    "objectID": "static-games/jamming-games.html#salient-features-of-the-solution",
    "href": "static-games/jamming-games.html#salient-features-of-the-solution",
    "title": "3  Jamming Games",
    "section": "3.5 Salient features of the solution",
    "text": "3.5 Salient features of the solution\nThe power allocated to a channel depends on the noise power \\(σ_i\\). We have three cases:\n\n\\(σ_i &gt; λ^{-1} B\\) in which case \\(x_i^* = y_i^* = 0\\).\nThis can be interpreted as follows. The channel is very bad. THerefore the transmitter does not transmit on it and consequently the jammer doesn’t jam it.\n\\((λ + μ)^{-1}B &lt; σ_i &lt; λ^{-1}B\\) in which case \\(x_i^* &gt; 0\\) but \\(y_i^* = 0\\).\nThis can be interpreted as follows. The channel is bad enough that the jammer doesn’t need to deteriorate it further (because it is better for the jammer to use its resources elsewhere).\n\\(σ_i &lt; (λ + μ)^{-1}B\\) in wihch case both \\(x_i^* &gt; 0\\) and \\(y_i^* &gt; 0\\).\nThis can be interpreted as follows. The jammer deteriorates the channel but the transmitter still transmits on it.\n\nThis situation is illustrated in Figure 3.2.\n\n\n\n\n\n\nFigure 3.2: Water-filling solution for the transmitter. Plot (a) shows the noise levels \\(σ_i\\). In plot (b), the red shaded region shows the power levels \\(y_i^*\\) and the blue area shows the power levels \\(x_i^*\\).",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jamming Games</span>"
    ]
  },
  {
    "objectID": "static-games/jamming-games.html#notes",
    "href": "static-games/jamming-games.html#notes",
    "title": "3  Jamming Games",
    "section": "Notes",
    "text": "Notes\nThe material in this section is adapted from Fasoulakis et al. (2019). For generalizations of this model, see Altman et al. (2007) and Altman et al. (2009). For a computationally efficient algorithm to solve the waterfilling problem, see He et al. (2013).\n\n\n\n\nAltman, E., Avrachenkov, K., and Garnaev, A. 2007. A jamming game in wireless networks with transmission cost. Network control and optimization: First EuroFGI international conference, Springer Berlin Heidelberg, 1–12. DOI: 10.1007/978-3-540-72709-5_1.\n\n\nAltman, E., Avrachenkov, K., and Garnaev, A. 2009. Jamming in wireless networks: The case of several jammers. International conference on game theory for networks, IEEE, 585–592.\n\n\nFasoulakis, M., Traganitis, A., and Ephremides, A. 2019. Jamming in multiple independent gaussian channels as a game. In: Game theory for networks. Springer International Publishing, 3–8. DOI: 10.1007/978-3-030-16989-3_1.\n\n\nHe, P., Zhao, L., Zhou, S., and Niu, Z. 2013. Water-filling: A geometric approach and its application to solve generalized radio resource allocation problems. IEEE Transactions on Wireless Communications 12, 7, 3637–3647. DOI: 10.1109/twc.2013.061713.130278.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jamming Games</span>"
    ]
  },
  {
    "objectID": "static-games/fictitious-play.html",
    "href": "static-games/fictitious-play.html",
    "title": "4  Fictitious Play",
    "section": "",
    "text": "4.1 Fictitious play for two player zero sum games\nFictitious play is an iterative algorithm to compute the solution of static game. It was first proposed by Brown (1951) who conjectured that it could be used to find the optimal solution of two player zero sum games; a result that was soon proved by Robinson (1951). Later it was shown to converge for \\(2 × 2\\) games (Miyasawa 1961), potential games (Monderer and Shapley 1996), and games with an interior evolutionary stable equilibrium (Hofbauer 1995). For a detailed discussion, see Fudenberg and Levine (1998).\nIn this section, we will describe fictitious play for two player games and present the high level idea of why it converges in the zero sum setting.\nConsider a two player ZSG that is played iteratively over an infinite horizon. Let \\(\\{t_{i,n}\\}_{n \\ge 0}\\) denote the (pure) strategies played by player \\(i\\) at time \\(n\\). Define sequence of mixed strategy profiles \\(\\{ σ_{i,n} \\}_{n \\ge 1}\\) as follows: \\[\n  σ_{i,n}(s_i) = \\frac{1}{n} \\sum_{k=1}^n \\IND\\{ t_{i,k} = s_i \\}.\n\\] These mixed strategies are called beliefs. Think of \\(σ_{i,n}\\) as the belief of player \\(-i\\) (in general, all players other than \\(i\\)) on how player \\(i\\) will play.\nThen, at time \\(n+1\\), each player \\(i\\) chooses a pure strategy \\(t_{i,n+1}\\) that is a best response of its belief on how others will play: \\[\n  t_{i,n+1} \\in \\text{BR}_i(σ_{-i,n})\n\\] where \\(\\text{BR}_i(σ_{-i})\\) denotes the best response of player \\(i\\) to the other players playing mixed strategy \\(σ_{-i}\\).\nThis is a simple algorithm, which can be implemented as follows.\nfunction FP_ZSG(u, N; initial = [1 1])\n  # Define best response maps\n  BR_row(u, σ) = argmax(u*σ)[1]\n  BR_col(u, σ) = argmin(σ'*u)[2]\n\n  # Size of straegy spaces\n  (m₁, m₂) = size(u)\n\n  # Strategies over time\n  t = zeros(Int,2,N)\n\n  # Beliefs over time\n  σ₁ = zeros(m₁, N)\n  σ₂ = zeros(m₂, N)\n\n  # Initial strategies\n  t[:,1] = initial\n\n  # Initial beliefs\n  σ₁[ t[1,1], 1] = 1\n  σ₂[ t[2,1], 1] = 1\n\n  for n = 1:N-1\n    t[1,n+1] = BR_row(u, σ₂[:,n])\n    t[2,n+1] = BR_col(u, σ₁[:,n])\n\n    σ₁[:,n+1] = n*σ₁[:, n]/(n+1)\n    σ₁[ t[1, n+1], n+1 ] += 1/(n+1)\n\n    σ₂[:,n+1] = n*σ₂[:, n]/(n+1)\n    σ₂[ t[2, n+1], n+1 ] += 1/(n+1)\n  end\n\n  return (t, σ₁, σ₂)\nend\nWe now test it over Rock-Paper-Scissors.\nUtility for Rock Paper Scissors$\\mathsf{R}$$\\mathsf{P}$$\\mathsf{S}$$\\mathsf{R}$$0$$-1$$1$$\\mathsf{P}$$1$$0$$-1$$\\mathsf{S}$$-1$$1$$0$\nTo fix ideas, we run FP algorithm for \\(N=4\\) starting with the intial strategy of P1 playing rock and P2 playing scissors.\nTime\n$n=1$\n$n=2$\n$n=3$\n$n=4$\n\n\n\n\nAction P1\nR\nR\nS\nS\n\n\nAction P2\nS\nP\nP\nR\n\n\nBelief P1\n$\\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$\n$\\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$\n$\\begin{bmatrix} 0.67 \\\\ 0.0 \\\\ 0.33 \\end{bmatrix}$\n$\\begin{bmatrix} 0.5 \\\\ 0.0 \\\\ 0.5 \\end{bmatrix}$\n\n\nBelief P2\n$\\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 1.0 \\end{bmatrix}$\n$\\begin{bmatrix} 0.0 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$\n$\\begin{bmatrix} 0.0 \\\\ 0.67 \\\\ 0.33 \\end{bmatrix}$\n$\\begin{bmatrix} 0.25 \\\\ 0.5 \\\\ 0.25 \\end{bmatrix}$\nIf we run the algorithm for a longer time, say \\(N=1000\\), we observe that the beliefs are slowly converging to \\([1/3, 1/3, 1/3]\\), which is the optimal mixed strategy for both players in this example.\nTime\n$n=996$\n$n=997$\n$n=998$\n$n=999$\n$n=1000$\n\n\n\n\nBelief P1\n$\\begin{bmatrix} 0.35 \\\\ 0.32 \\\\ 0.33 \\end{bmatrix}$\n$\\begin{bmatrix} 0.35 \\\\ 0.32 \\\\ 0.33 \\end{bmatrix}$\n$\\begin{bmatrix} 0.35 \\\\ 0.32 \\\\ 0.33 \\end{bmatrix}$\n$\\begin{bmatrix} 0.35 \\\\ 0.32 \\\\ 0.33 \\end{bmatrix}$\n$\\begin{bmatrix} 0.35 \\\\ 0.32 \\\\ 0.33 \\end{bmatrix}$\n\n\nBelief P2\n$\\begin{bmatrix} 0.31 \\\\ 0.35 \\\\ 0.34 \\end{bmatrix}$\n$\\begin{bmatrix} 0.31 \\\\ 0.35 \\\\ 0.34 \\end{bmatrix}$\n$\\begin{bmatrix} 0.31 \\\\ 0.35 \\\\ 0.34 \\end{bmatrix}$\n$\\begin{bmatrix} 0.31 \\\\ 0.35 \\\\ 0.34 \\end{bmatrix}$\n$\\begin{bmatrix} 0.31 \\\\ 0.35 \\\\ 0.34 \\end{bmatrix}$\nHowever, the rate of convergence is pretty slow as can be seen from Figure 4.1.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fictitious Play</span>"
    ]
  },
  {
    "objectID": "static-games/fictitious-play.html#fictitious-play-for-two-player-zero-sum-games",
    "href": "static-games/fictitious-play.html#fictitious-play-for-two-player-zero-sum-games",
    "title": "4  Fictitious Play",
    "section": "",
    "text": "(a) Belief of player 1\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Belief of player 2\n\n\n\n\n\n\n\nFigure 4.1: Rate of convergence of fictitious play for rock paper scissors",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fictitious Play</span>"
    ]
  },
  {
    "objectID": "static-games/fictitious-play.html#convergence-guarantee",
    "href": "static-games/fictitious-play.html#convergence-guarantee",
    "title": "4  Fictitious Play",
    "section": "4.2 Convergence Guarantee",
    "text": "4.2 Convergence Guarantee\n\nTheorem 4.1 For a two player ZSG, the belief in fictitious play converge to the optimal strategy, i.e., \\[\n  \\lim_{n \\to ∞} σ_{i,n} = σ_i^*, \\quad \\forall i \\in \\{1,2\\}\n\\] where \\((σ_1^*, σ_2^*)\\) is an optimal strategy.\n\nThis result was proved in Robinson (1951). The main idea of the proof is as follows: Define \\[\\begin{align*}\n  \\underline V_n(s_2) &= \\frac{1}{n} \\sum_{k=1}^n u(t_{1,k}, s_2),\n  & \\forall s_2 & \\in \\ALPHABET S_2 \\\\\n  \\bar V_n(s_1) &= \\frac{1}{n} \\sum_{k=1}^n u(s_1, t_{2,k}),\n  & \\forall s_1 & \\in \\ALPHABET S_1\n\\end{align*}\\] Recall that the expected utility \\(U(σ_1, σ_2)\\) is bilinear. Therefore, \\[\\begin{align*}\n  \\underline V_n(s_2) &= U(σ_{1,n}, s_2) \\\\\n  \\bar V_n(s_1) &= U(s_1, σ_{2,n}).\n\\end{align*}\\] Moreover, we have that \\[\\begin{align*}\n  \\min_{s_2 \\in \\ALPHABET S_2} \\underline V_n(s_2)\n  &= \\min_{s_2 \\in \\ALPHABET S_2} U(σ_{1,n}, s_2)\n  \\\\\n  &\\le \\max_{σ_1 \\in Δ(\\ALPHABET S_1)} \\min_{s_2 \\in \\ALPHABET S_2} U(σ_1, s_2) \\\\\n  &\\stackrel{(a)}=   \\max_{σ_1 \\in Δ(\\ALPHABET S_1)} \\min_{σ_2 \\in \\ALPHABET S_2} U(σ_1, σ_2)\n  \\\\\n  &= v\n\\end{align*}\\] where \\((a)\\) uses the fact that for a fixed \\(σ_1\\), the term \\(U(σ_1, σ_2)\\) is linear in \\(σ_2\\), therefore \\(\\min_{σ_2 \\in Δ(\\ALPHABET S_2)} U(σ_1, σ_2) = \\min_{s_2 \\in \\ALPHABET S_2} U(σ_1, s_2)\\). [This is the same idea we used when coming up with an algorithm to compute the value of a ZSG.]\nBy a symmetric argument, we have \\[\n  \\max_{s_1 \\in \\ALPHABET S_1} \\ge v.\n\\] Thus, we have established that \\[\n  \\min_{s_2 \\in \\ALPHABET S_2} \\underline V_n(s_2)\n  \\le  v \\le\n  \\max_{s_1 \\in \\ALPHABET S_1} \\bar V_n(s_1).\n\\]\nThe proof of Theorem 4.1 works by establishing that \\[\n  \\lim_{n \\to ∞} \\min_{s_2 \\in \\ALPHABET S_2} \\underline V_n(s_2) = v\n  \\quad\\text{and}\\quad\n  \\lim_{n \\to ∞} \\max_{s_1 \\in \\ALPHABET S_1} \\bar V_n(s_1) = v.\n\\] See Robinson (1951) for details.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fictitious Play</span>"
    ]
  },
  {
    "objectID": "static-games/fictitious-play.html#exercises",
    "href": "static-games/fictitious-play.html#exercises",
    "title": "4  Fictitious Play",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 4.1 Run fictitious play for \\(N=1000\\) steps for each of the games given in Exercise 2.6. Plot the beliefs of each player as a function of time. (To simplify grading, please start with \\((1,1)\\) as the inital pure strategy profile.)\nPart of the objective of this exercise is to completely understand fictitious play by coding the algorithm. So, instead of blindly copy pasting the code provided above, implement the algorithm from the description provided above. You may use any programming language to do this exercise.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe belifs as a function of time are shown below.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Belief of player 1\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Belief of player 2\n\n\n\n\n\n\n\nFigure 4.2: Rate of convergence of fictitious play for Game 1 of Exercise 2.6.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Belief of player 1\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Belief of player 2\n\n\n\n\n\n\n\nFigure 4.3: Rate of convergence of fictitious play for Game 2 of Exercise 2.6.\n\n\n\n\n\n\n\nExercise 4.2 In this exercise, we describe a variation of fictitious play called stochastic fictitious play. This is version is useful when we consider general sum games, but for the purpose of this exercise, we will continue to work with zero sum games but will use the notation the extended notation \\(u_1, u_2 \\colon \\ALPHABET S \\to \\reals\\) as the utility functions of the two players (instead of the short notation \\(u \\colon \\ALPHABET S \\to \\reals\\) as the utility of the row player).\nOne of the technical challenges in analyzing fictitious play is that the best response is a set valued object. To circiumven this issue, one can replace the best response by the best response of the regularized utility function \\[\n  \\tilde U_i(σ_1, σ_2) = U_i(σ_1, σ_2) + θ \\sum_{s_i \\in \\ALPHABET S_i} σ_i(s_i) \\log σ_i(s_i)\n\\] where \\(θ\\) is a design parameter (often called temperature). Observe that the regularized utility function is strictly concave in \\(σ_i\\), therefore the best response to the regularized utility function is unique.\nStochastic ficiticious play is similar to regular fictitious play except that at each time, player \\(i\\) chooses a pure strategy \\(t_{i,n}\\) which is the best response to the regularized utlity function \\(\\tilde U_i\\).\nSuch a best response can be computed efficiently from the following result: Let \\(μ_i = \\arg \\max_{σ_i \\in Δ(\\ALPHABET S_i)} \\tilde U_i(σ_i, σ_{-i})\\). Then \\(μ_i\\) can be computed in closed form as \\[\n  μ_{i}(s_i) = \\frac{ \\exp\\bigl(\\frac{1}{θ} U_i(s_i, σ_{-i}) \\bigr) }\n                    { \\sum_{\\tilde s_i \\in \\ALPHABET S_i} \\exp\\bigl(\\frac{1}{θ} U_i(\\tilde s_i, σ_{-i}) \\bigr) }.\n\\] This means that the players pick a pure strategy according to the probability \\(μ_i\\).\nRepeat Exercise 4.1 for stochastic fictitious play and \\(θ \\in \\{1, 0.1, 0.01\\}\\). Compare with the solution that you obtained in Exercise 4.1.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe belifs as a function of time for \\(θ = 0.1\\) are shown below.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Belief of player 1\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Belief of player 2\n\n\n\n\n\n\n\nFigure 4.4: Rate of convergence of fictitious play for Game 1 of Exercise 2.6.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Belief of player 1\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Belief of player 2\n\n\n\n\n\n\n\nFigure 4.5: Rate of convergence of fictitious play for Game 2 of Exercise 2.6.\n\n\n\n\n\n\n\n\n\n\nBrown, G.W. 1951. Iterative solutions of games by fictitious play. Activity analysis of production and allocation., Wiley.\n\n\nFudenberg, D. and Levine, D.K. 1998. The theory of learning in games. MIT press.\n\n\nHofbauer, J. 1995. Stability for the best response dynamics. Universität Wien.\n\n\nMiyasawa, K. 1961. On the convergence of learning processes in a 2x2 non-zero-person game. Econometric Research Program. Available at: https://econpapers.repec.org/paper/clalevarc/419.htm.\n\n\nMonderer, D. and Shapley, L.S. 1996. Fictitious play property for games with identical interests. Journal of economic theory 68, 1, 258–265.\n\n\nRobinson, J. 1951. An iterative method of solving a game. The Annals of Mathematics 54, 2, 296. DOI: 10.2307/1969530.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fictitious Play</span>"
    ]
  },
  {
    "objectID": "static-games/nash-equilibrium.html",
    "href": "static-games/nash-equilibrium.html",
    "title": "5  Nash equilibrium",
    "section": "",
    "text": "5.1 What is Nash equilibrium\nSo far we have looked at two solution concepts:\nIn this section, we look at the most popular solution concept known as Nash equilibrium (which we will abbreviate to NE).\nThe above definition can be written formally as follows.\nAnother way to define NE is use Best response correspondece \\(\\BR_i \\colon \\ALPHABET S_{-i} \\rightrightarrows A_i\\) defined as follows \\[\n  \\BR_i(s_{-i}) = \\bigl\\{ s_i \\in \\ALPHABET S_i : u_i(s_i, s_{-i}) \\ge u_i(τ_i, s_{-i}), \\forall τ_i \\in \\ALPHABET S_i \\bigr\\}.\n\\]",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nash equilibrium</span>"
    ]
  },
  {
    "objectID": "static-games/nash-equilibrium.html#what-is-nash-equilibrium",
    "href": "static-games/nash-equilibrium.html#what-is-nash-equilibrium",
    "title": "5  Nash equilibrium",
    "section": "",
    "text": "Informal definition of Nash equilibrium\n\n\n\nA strategy profile is a NE if no player can improve its performance by unilateral deviations.\n\n\n\n\nDefinition 5.1 A Nash equilibrium in pure strategies for a strategic game \\(\\mathscr{G} = \\langle N, (\\ALPHABET S_i)_{i \\in N}, (u_i)_{i \\in N} \\rangle\\) is a strategy profile \\(s^* = (s^*_1, \\dots, s^*_n) \\in \\ALPHABET S\\) such that for each \\(i \\in N\\) the following is satisfied: \\[\n  u_i(s_i^*, s_{-i}^*) \\ge u_i(s_i, s_{-i}^*),\n  \\quad \\forall s_i \\in \\ALPHABET S_i.\n\\] The payoff vector \\(u(s^*)\\) is called the equilibrium payoff corresponding to NE \\(s^*\\).\n\n\n\nDefinition 5.2 A strategy profile \\(s^* \\in \\ALPHABET S\\) is a Nash equilibrium if each player is playing a best response to the strategy profile of other players, i.e., \\[\n  s_i^* \\in \\BR_i(s^*_{-i}), \\quad \\forall i \\in N.\n\\]",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nash equilibrium</span>"
    ]
  },
  {
    "objectID": "static-games/nash-equilibrium.html#relationship-with-previous-solution-concepts",
    "href": "static-games/nash-equilibrium.html#relationship-with-previous-solution-concepts",
    "title": "5  Nash equilibrium",
    "section": "5.2 Relationship with previous solution concepts",
    "text": "5.2 Relationship with previous solution concepts\n\n5.2.1 Relationship between NE and dominant strategy equilibrium\nConsider prisoner’s dilemma game.\n\n\nPrisoner's Dilemma$\\mathsf{A}$$\\mathsf{R}$$\\mathsf{A}$$-2$$-2$$0$$-3$$\\mathsf{R}$$-3$$0$$-1$$-1$\n\n\nRecall that \\((\\mathsf{A}, \\mathsf{A})\\) was the equilibrium in strongly dominant strategies. It can be easily verified that is also a Nash equillbrium. In fact, we have the following general result:\n\nProposition 5.1 Any equilibrium in strongly or weakly dominant strategy is also a Nash equilibrium.\n\n\n\n\n\n\n\nProof\n\n\n\nSuppose \\(s^* \\in \\ALPHABET S\\) is an equilibrium in strongly or weakly dominant strategies. Then, by definition, for all players \\(i \\in N\\), we have \\[\nu_i(s_i^*, s_{-i}) \\ge u_i(s_i, s_{-i}),\n  \\quad \\forall s_i \\in \\ALPHABET S_i, s_{-i} \\in \\ALPHABET S_{-i}.\n\\] In particular, setting \\(s_{-i} = s_{-i}^*\\) gives \\[\nu_i(s_i^*, s_{-i}^*) \\ge u_i(s_i, s_{-i}^*),\n  \\quad \\forall s_i \\in \\ALPHABET S_i.\n\\] Thus, \\(s^*\\) is a Nash equilibrium.\n\n\n\n\n5.2.2 Relationship between NE and rationalizable strategies\nRecall the following example discussed earlier:\n\n\n$\\mathsf{L}$$\\mathsf{C}$$\\mathsf{R}$$\\mathsf{T}$$1$$0$$1$$2$$0$$1$$\\mathsf{B}$$0$$3$$0$$1$$2$$0$\n\n\nWe had shown earlier that \\((\\mathsf{T}, \\mathsf{C})\\) is a rationalizble strategy obtained by IEDS. It can be easily verified that it is also a Nash equilibrium. In fact, we have the following general result.\n\nProposition 5.2 Any rationalizable output come IEDS or IEWDS is a Nash equilibrium.\n\n\n\n\n\n\n\nProof Sketch\n\n\n\nFor IEDS, the proof hinges on the fact that at each step of IEDS, we remove strategies that are dominated and therefore cannot be part of the best response.\nFor IEWDS, we may remove some best response strategies, but anything that survives has to be a best response pair.\n\n\n\n\n5.2.3 Relationship between NE and optimal strategies in zero sum games\nRecall the following example discussed earlier in Example 2.2\n\n\n$\\mathsf{L}$$\\mathsf{C}$$\\mathsf{R}$$\\mathsf{T}$$2$$-1$$-2$$\\mathsf{M}$$1$$0$$1$$\\mathsf{B}$$-2$$-1$$2$\n\n\nWe had shown that \\((\\mathsf{M}, \\mathsf{C})\\) is an optimal strategy of this game. It can be easily verified that it is also a Nash equilibrium. In fact, we have the following general result:\n\nProposition 5.3 Any optimal strategy of a ZSG is a NE and vice versa.\n\n\n\n\n\n\n\nProof\n\n\n\nThis follows immediately from the saddle point property of optimal strategies.\n\n\nThe above discussion shows that the concept of Nash equilibrium generalizes all the solution concepts that we have discussed so far. The main advantage of Nash equilibrium is that it exists even when the previously discussed solution concepts do not. For instance, consider the following games.\n\nExample 5.1 Find all NE (in pure strategies) for Battle of Sexes game.\n\n\nBattle of Sexes game$\\mathsf{F}$$\\mathsf{O}$$\\mathsf{F}$$2$$1$$0$$0$$\\mathsf{O}$$0$$0$$1$$2$\n\n\n\n\nExample 5.2 Find all NE (in pure strategies) for the game of chicken\n\n\nThe game of chicken$\\mathsf{C}$$\\mathsf{H}$$\\mathsf{C}$$3$$3$$1$$10$$\\mathsf{H}$$10$$1$$0$$0$\n\n\n\nNote that a game may not have an equilibrium in pure strategyes. For example, the game of matching pennies has no equilibrium in pure strategies.\n\n\nMatching pennies game$\\mathsf{H}$$\\mathsf{T}$$\\mathsf{H}$$1$$-1$$-1$$1$$\\mathsf{T}$$-1$$1$$1$$-1$\n\n\nIt is natural to ask if we can seek a NE in mixed strategies in such instances. And, in general, when does a game have a NE?\nThis question was answered by (Nash 1950; Nash 1951) who showed that any finte game has a NE in mixed strategy. We will come to that proof later.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nash equilibrium</span>"
    ]
  },
  {
    "objectID": "static-games/nash-equilibrium.html#computing-ne-in-2-2-games",
    "href": "static-games/nash-equilibrium.html#computing-ne-in-2-2-games",
    "title": "5  Nash equilibrium",
    "section": "5.3 Computing NE in \\(2 × 2\\) games",
    "text": "5.3 Computing NE in \\(2 × 2\\) games\nWe start by a brute-force method to compute NE in \\(2 × 2\\) games. This method is just for illustration. Based on the intuition from this method, we will later present an efficient method to compute NE.\nConsider the game shown below. We can check that the game has no NE in pure strategies.\n\n\nGame with no NE in pure strategies$\\mathsf{L}$$\\mathsf{R}$$\\mathsf{T}$$1$$-1$$0$$2$$\\mathsf{B}$$0$$1$$2$$0$\n\n\nLet \\(σ_1 = (p, 1-p)\\) and \\(σ_2 = (q, 1-q)\\) be a NE in mixed strategies. Then, the probability distribution of the different actions is shown below:\n\n\nProbability of choosing different actions$\\mathsf{L}$$\\mathsf{R}$$\\mathsf{T}$$pq$$p\\left( 1 - q \\right)$$\\mathsf{B}$$\\left( 1 - p \\right) \\cdot q$$\\mathrm{1 - p}\\left( 1 - q \\right)$\n\n\nThus, we have \\[\\begin{align*}\n  U_1(p, q) \\coloneqq U_1(σ_1, σ_2) &= 1 ⋅ pq + 0 ⋅ p (1-q) + 0 ⋅ (1-p) q + 2 ⋅ (1-p)(1-q) \\\\\n  &= 3pq - 2p -2q + 2,\n\\end{align*}\\] and \\[\\begin{align*}\n  U_2(p, q) \\coloneqq U_2(σ_1, σ_2) &= -1 ⋅ pq + 2 ⋅ p (1-q) + 1 ⋅ (1-p) q + 0 ⋅ (1-p)(1-q) \\\\\n  &= -4pq + 2p + q .\n\\end{align*}\\]\nThus, we can think of this as a two player game on the unit square where \\(P_1\\) chooses \\(p \\in [0,1]\\) and \\(P_2\\) chooses \\(q \\in [0,1]\\) and the pageoff functions are \\(U_1(p, q)\\) and \\(U_2(p,q)\\), respectively.\nIn this notation, NE is a point \\((p,q)\\) such that \\[\n  p \\in \\BR_1(q)\n  \\quad\\text{and}\\quad\n  q \\in \\BR_2(p).\n\\]\nWe will now identify the best response maps \\(\\BR_1\\) and \\(\\BR_2\\) and then compute the NE.\n\n5.3.1 Best response map of player 1\nRecall that \\[\n  U_1(p,q) = 3 pq - 2p - 2q + 2 = (3q - 2) p - 2q + 2\n\\] which is linear in \\(p\\) (for a fixed \\(q\\)). Thus, \\[\n  \\max_{p \\in [0,1]} U_1(p,q)\n  = \\begin{cases}\n    U_1(1, q) = q, & \\hbox{if } 3q - 2 \\ge 0 \\\\\n    U_1(0, q) = -2q + 2, & \\hbox{if } 3q - 2 &lt; 0\n  \\end{cases}\n\\] and \\[\n  \\arg\\max_{p \\in [0,1]} U_1(p,q)\n  = \\begin{cases}\n    1, & \\hbox{if } 3q - 2 &gt; 0 \\\\\n    [0,1], & \\hbox{ if } 3q - 2 = 0 \\\\\n    0, & \\hbox{if } 3q - 2 &lt; 0\n  \\end{cases}\n\\]\nWe can visual these results graphically, as shown in Figure 5.1.\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(\\max_{p \\in [0,1]} U_1(p,q)\\) as a function of \\(q\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(\\arg\\max_{p \\in [0,1]} U_1(p,q)\\) as a function of \\(q\\)\n\n\n\n\n\n\n\nFigure 5.1: Best response map for player 1\n\n\n\n\n\n5.3.2 Best response map of player 2\nWe repeat the above process for player 2. Recall that \\[\n  U_2(p,q) = -4 pq + 2p + q = (1 - 4p)q + 2p\n\\] which is linear in \\(q\\) (for a fixed \\(p\\)). Thus, \\[\n  \\max_{q \\in [0,1]} U_2(p,q)\n  = \\begin{cases}\n    U_2(p, 0) =  2p, & \\hbox{if } 1 - 4p \\le 0 \\\\\n    U_2(p, 1) = -2p + 1, & \\hbox{if } 1 - 4p &gt; 0\n  \\end{cases}\n\\] and \\[\n  \\arg\\max_{q \\in [0,1]} U_2(p,q)\n  = \\begin{cases}\n    0, & \\hbox{if } 1 - 4p &lt; 0 \\\\\n    [0,1], & \\hbox{ if } 1 - 4p = 0 \\\\\n    1, & \\hbox{if } 1 - 4p &gt; 0\n  \\end{cases}\n\\]\nWe can visual these results graphically, as shown in Figure 5.2.\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(\\max_{q \\in [0,1]} U_2(p,q)\\) as a function of \\(p\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(\\arg\\max_{q \\in [0,1]} U_2(p,q)\\) as a function of \\(p\\)\n\n\n\n\n\n\n\nFigure 5.2: Best response map for player 2\n\n\n\n\n\n5.3.3 Identifying the NE\nTo compute the NE, we look at the two BR curves and find their point of intersection, as shown in Figure 5.3.\n\n\n\n\n\n\nFigure 5.3: Intersection of the best response curves of the two players. Note that the axes of the best response curve of player 2 has been flipped\n\n\n\nThus, the mixed strategy NE is \\[σ_1 = (\\tfrac 14, \\tfrac 34)\n\\quad\\text{and}\\quad\nσ_2 = (\\tfrac 23, \\tfrac 13).\\] Since the curves have no other intersection, the game has a unique Nash equilibrium.\nNotice that each BR curve have a “staircase” appearance. If we are looking for an equilibrium in strictly mixed strategies (i.e., \\(p \\in (0,1)\\) and \\(q \\in (0,1)\\)), then the intersection point of the two BR curves will be the vertically straight line part of the BR curves.\nRecall that we have argued that \\[\n  \\max_{p \\in [0,1]} U_1(p,q) =\n  \\begin{cases}\n    U_1(\\mathsf{T}, q), & q \\ge q^* \\\\\n    U_1(\\mathsf{B}, q), & q \\le q^*\n  \\end{cases}\n\\] where \\(q^*\\) is the point where \\(U_1(\\mathsf{T}, q) = U_1(\\mathsf{B}, q)\\).\nBy a symmetric argument, we have that \\(p^*\\) point in the BR curve for player 2 is the point where \\(U_2(p, \\mathsf{L}) = U_2(p, \\mathsf{R})\\).\nThus, instead of drawing the BR curves, we could have directly found the mixed strategy NE \\((p^*, q^*)\\) as \\[\n  \\bbox[5pt,border: 1px solid]{\n    \\begin{aligned}\n      q^* &: q \\hbox{ such that } U_1(\\mathsf{T}, q) = U_1(\\mathsf{B}, q) \\\\\n      p^* &: p \\hbox{ such that } U_2(p, \\mathsf{R}) = U_2(p, \\mathsf{B})\n    \\end{aligned}\n  }\n\\] This provides a general method of finding the NE for \\(2 × 2\\) games, which we use in general.\n\nExample 5.3 Find all the NE of battle of sexes game.\n\n\nBattle of Sexes game$\\mathsf{F}$$\\mathsf{O}$$\\mathsf{F}$$2$$1$$0$$0$$\\mathsf{O}$$0$$0$$1$$2$\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe have already identified the pure strategy NE in Example 5.1. We now consider NE in strictly mixed strategies.\nLet \\(σ_1 = (p, 1-p)\\) and \\(σ_2 = (q, 1-q)\\) be a NE in mixed strategies. Then, the equilibrium value of \\(p\\) is such that \\[\nU_2(p, \\mathsf{F}) = U_2(p, \\mathsf{O})\n\\] Hence, \\[\n  p = 2 (1 - p) \\implies p = \\tfrac 23.\n\\]\nSimilarly, the equilibrium value of \\(q\\) is such that \\[\n  U_1(\\mathsf{F}, q) = U_1(\\mathsf{O}, q)\n\\] Hence, \\[ 2 q = 1 - q \\implies q = \\tfrac 13.\\] Thus, in addition to the two pure strategy NE, we have \\(σ_1 = (\\tfrac 23, \\tfrac 13)\\) and \\(σ_2 = (\\tfrac 13, \\tfrac 23)\\) is a mixed strategy NE.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nash equilibrium</span>"
    ]
  },
  {
    "objectID": "static-games/nash-equilibrium.html#the-indifference-principle",
    "href": "static-games/nash-equilibrium.html#the-indifference-principle",
    "title": "5  Nash equilibrium",
    "section": "5.4 The indifference principle",
    "text": "5.4 The indifference principle\nWe now present a general method to compute NE for finite games. The method relies on the following characterization of NE.\n\nTheorem 5.1 Let \\(\\mathscr{G} = \\langle N, (\\ALPHABET S_i)_{i \\in N}, (u_i)_{i \\in N} \\rangle\\) be a finite strategic game. Then \\(σ^* = (σ^*_1, \\dots, σ^*_n)\\), where \\(σ^*_i \\in Δ(\\ALPHABET S_i)\\), \\(i \\in N\\)&lt; is a mixed strategy NE of \\(\\mathscr{G}\\) if and only if for every player \\(i \\in N\\) and every pure strategy \\(s_i\\) in the support of \\(σ^*_i\\) (i.e., every \\(s_i\\) such that \\(σ^*_i(s_i) &gt; 0\\)), we have \\[\n    s_i \\in \\BR_i(σ^*_{-i}).\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\\((\\implies)\\) Suppose there is a \\(s_i\\) in the support of \\(σ^*_i\\) (i.e., \\(σ^*_i(s_i) &gt; 0\\)) but \\(s_i \\not\\in \\BR_{i}(σ^*_{-i})\\). Then, by linearity of \\(U_i(σ_i, σ^*_{-i})\\) in \\(σ_i\\), player \\(i\\) can increase its payoff by transferring the probability mass on \\(s_i\\) to an action that is a BR. This will imply that \\(σ^*_i\\) is not a Br, which is a contradiction. Thus, our initial supposition must be incorrect and \\(s_i \\in \\BR_i(σ^*_{-i})\\).\n\\((\\impliedby)\\) Suppose for every player \\(i \\in N\\) and every action \\(s_i\\) in the support of \\(σ^*_i\\) is a BR of \\(σ^*_{-i}\\) but \\(σ^*\\) is not a NE. Then, there must be a player \\(i\\) such that \\(σ^*_i\\) is not a BR to \\(σ^*_{-i}\\).\nSince \\(σ^*_i\\) is not a BR, there must be a mixed strategy \\(τ_i\\) such that \\[\\begin{equation}\\label{eq:ineq}\n    U_i(σ^*_i, σ^*_{-i}) &lt; U_i(τ_i, σ^*_{-i}).\n\\end{equation}\\] Now consider a \\(t_i\\) in the support of \\(τ_i\\) and \\(s_i\\) in the support of \\(σ^*_i\\). By linearity of \\(U_i\\), we have \\[\n  U_i(t_i, σ^*_{-i}) = U_i(τ_i, σ^*_{-i})\n\\] and \\[\n  U_i(s_i, σ^*_{-i}) = U_i(σ^*_i, σ^*_{-i}).\n\\] Eq. \\(\\eqref{eq:ineq}\\) implies that \\(U_i(t_i, σ^*_{-i}) &gt; U_i(s_i, σ^*_{-i})\\). This means that \\(s_i\\) is not a BR of \\(σ^*_{-i}\\), which is a contraction. Hence, our supposition that \\(σ^*_i\\) is not a NE must be incorrect.\n\n\n\nAn immediate consequence of the previous result is the following.\n\nTheorem 5.2 (The indifference principle) Let \\(σ^* = (σ^*_1, \\dots, σ^*_n)\\) be a NE in mixed strategy. Then, we have the following:\n\nFor any \\(s_i\\) such that \\(σ^*_i(s_i) &gt; 0\\), we have \\[\n    U_i(s_i, σ^*_{-i}) = U_i(σ^*_i, σ^*_{-i}).\n\\]\nFor any \\(s_i\\) such that \\(σ^*_i(s_i) = 0\\), we have \\[\n    U_i(s_i, σ^*_{-i}) \\le U_i(σ^*_i, σ^*_{-i}).\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\nFrom Theorem 5.1, we know that every pure strategy in the support of \\(σ^*_i\\) must be a BR to \\(σ^*_{-i}\\) and hence all such strategies must have the same payoff. Moreover, if all the pure strategies in the support of \\(σ^*_i\\) have the same payoff, their payoff must be the same as the payoff of \\(σ^*_i\\). This establishes the first property.\nThe second property follows from the definition of NE.\n\n\nTheorem 5.2 can be used to compute the NE of general two-player finite game. Let \\((σ^*_1, σ^*_2)\\) be a mixed strategy NE. Let \\[\n  \\text{supp}(σ^*_i) = \\{ s_i \\in \\ALPHABET S_i : σ^*_i(s_i) &gt; 0 \\}\n\\] denote the support of \\(σ^*_i\\). From Theorem 5.2, we have that for any \\(s^∘_i \\in \\text{supp}(σ^*_i)\\), \\(i \\in \\{1, 2\\}\\), we must have \\[\\begin{align*}\n  U_1(s^∘_1, σ^*_2) &= U_1(s_1, σ^*_2), & s_1 &\\in \\text{supp}(σ^*_1) \\\\\n  U_1(s^∘_1, σ^*_2) &\\ge U_1(t_1, σ^*_2), & t_1 &\\not\\in \\text{supp}(σ^*_1)\n\\end{align*}\\] and \\[\\begin{align*}\n  U_2(σ^*_1, s^∘_2) &= U_2(σ^*_1, s_2), & s_2 &\\in \\text{supp}(σ^*_2) \\\\\n  U_2(σ^*_1, s^∘_2) &\\ge U_2(σ^*_1, t_2), & t_2 &\\not\\in \\text{supp}(σ^*_2)\n\\end{align*}\\]\nNote that for any given choice of \\(\\text{supp}(σ^*_1)\\) and \\(\\text{supp}(σ^*_2)\\), we can check if the above equations have a consistent solution. If they do, that solutin is a NE.\nThe difficulty is that we do not know the support of \\(σ^*_1\\) and \\(σ^*_2\\). So, we need to repeat the above process for each choice of supports. Computing NE by such a brute-force search has an exponential complexity in \\(\\ABS{\\ALPHABET S_1}\\) and \\(\\ABS{\\ALPHABET S_2}\\).\n\nExample 5.4 Compute all NE of the following game.\n\n\n$\\mathsf{L}$$\\mathsf{C}$$\\mathsf{R}$$\\mathsf{T}$$7$$2$$2$$7$$3$$6$$\\mathsf{B}$$2$$7$$7$$2$$4$$5$\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe first check for pure strategy NE by inspection and see that the game has no pure strategy NE.\nTo search for mixed strategy NE, we will consider all possible choices for the support for mixed strategyes for both players. Player 1 has two pure strategies and any mixed strategy must randomize between them. For player 2, there are three pure strategies, so there are \\(\\binom{3}{2} + \\binom{3}{3} = 4\\) possible choices for the support of mixed strategies. We must search for NE in each of these cases.\nFor all possibilities, we will assume that \\[\n  σ^*_1 = (p[\\mathsf{T}], 1-p[\\mathsf{B}])\n  \\quad\\text{and}\\quad\n  σ^*_2 = (q_1[\\mathsf{L}], q_2[\\mathsf{C}], q_3[\\mathsf{R}])\n\\] where \\(q_1, q_2, q_3 \\ge 0\\) and \\(q_1 + q_2 + q_3 = 1\\). Thus, \\[\\begin{align*}\n  U_1(\\mathsf{T}, σ^*_2) &= 7q_1  + 2 q_2 + 3 q_3 \\\\\n  U_1(\\mathsf{B}, σ^*_2) &= 2q_1  + 7 q_2 + 4 q_3\n\\end{align*}\\] and \\[\\begin{align*}\n  U_2(σ^*_1, \\mathsf{L}) &= 2 p + 7(1-p) = -5p + 7 \\\\\n  U_2(σ^*_1, \\mathsf{C}) &= 7p + 2(1-p) = 5p + 2 \\\\\n  U_2(σ^*_1, \\mathsf{R}) &= 6p + 5(1-p) = p + 5\n\\end{align*}\\]\n\nCase 1: Assume that \\(\\text{supp}(σ^*_2) = \\{\\mathsf{L}, \\mathsf{C}, \\mathsf{R}\\}\\) so \\(q_1, q_2, q_3 &gt; 0\\). By the irrelevance principle, we must have \\[\n    U_1(\\mathsf{T}, σ^*_2) = U_1(\\mathsf{B}, σ^*_2)\n\\] and \\[\n  U_2(σ^*_1, \\mathsf{L}) = U_2(σ^*_1, \\mathsf{C}) = U_2(σ^*_1, \\mathsf{R}).\n\\] The second set of equations has no consistent solution for \\(p\\). Thus, there is no NE in this case.\nCase 2: Assume that \\(\\text{supp}(σ^*_2) = \\{\\mathsf{L}, \\mathsf{C}\\}\\), i.e., \\(q_1, q_2 &gt; 0\\) and \\(q_3 = 0\\). By the irrelevance principle, we must have \\[\n    U_1(\\mathsf{T}, σ^*_2) = U_1(\\mathsf{B}, σ^*_2)\n\\] and \\[\n  U_2(σ^*_1, \\mathsf{L}) = U_2(σ^*_1, \\mathsf{C}) \\ge U_2(σ^*_1, \\mathsf{R}).\n\\] Solving \\(U_2(σ^*_1, \\mathsf{L}) = U_2(σ^*_1, \\mathsf{C})\\), we get \\(p = \\tfrac 12\\) and \\(U_2(σ^*_1, \\mathsf{L}) = U_2(σ^*_1, \\mathsf{C}) = 4.5\\).\nWe now check \\(U_2(σ^*_1, \\mathsf{R}) = p + 5 = 5.5 &gt; 4.5\\). Thus, the second system of equations has no consistent solution for \\(p\\). Thus, there is no NE in this case.\nCase 3: Assume that \\(\\text{supp}(σ^*_2) = \\{\\mathsf{L}, \\mathsf{R}\\}\\), i.e., \\(q_1, q_3 &gt; 0\\) and \\(q_2 = 0\\). By the irrelevance principle, we must have \\[\n    U_1(\\mathsf{T}, σ^*_2) = U_1(\\mathsf{B}, σ^*_2)\n\\] and \\[\n  U_2(σ^*_1, \\mathsf{L}) = U_2(σ^*_1, \\mathsf{R}) \\ge U_2(σ^*_1, \\mathsf{C}).\n\\] Solving \\(U_2(σ^*_1, \\mathsf{L}) = U_2(σ^*_1, \\mathsf{R})\\), we get \\(p = \\tfrac 13\\) and \\(U_2(σ^*_1, \\mathsf{L}) = U_2(σ^*_1, \\mathsf{C}) = \\tfrac{16}{3}\\).\nWe now check \\(U_2(σ^*_1, \\mathsf{C}) = 5p + 2 = \\tfrac{11}{3} \\le \\tfrac{16}{3}\\). Thus, the second system of equations has a consistent solution for \\(p\\), \\(p = \\tfrac{1}{3}\\).\nSo, we check the first system of equations \\(U_1(\\mathsf{T}, σ^*_2) = U_1(\\mathsf{B}, σ^*_2)\\) (for \\(q_2 = 0\\)), which gives \\(q_1 = \\tfrac{1}{6}\\) and \\(q_3 = \\tfrac{5}{6}\\) and the corresponding payoff is \\(U_1(\\mathsf{T}, σ^*_2) = U_1(\\mathsf{B}, σ^*_2) = \\tfrac{11}{3}\\).\nThus, \\(σ^*_1 = (\\tfrac 13, \\tfrac 23)\\) and \\(σ^*_2 = (\\tfrac 16, 0, \\tfrac 56)\\) is a NE with payoff \\((\\tfrac{11}{3}, \\tfrac{16}{3})\\).\nCase 4: Assume that \\(\\text{supp}(σ^*_2) = \\{\\mathsf{C}, \\mathsf{R}\\}\\), i.e., \\(q_2, q_3 &gt; 0\\) and \\(q_1 = 0\\). By the irrelevance principle, we must have \\[\n    U_1(\\mathsf{T}, σ^*_2) = U_1(\\mathsf{B}, σ^*_2)\n\\] and \\[\n  U_2(σ^*_1, \\mathsf{C}) = U_2(σ^*_1, \\mathsf{R}) \\ge U_2(σ^*_1, \\mathsf{L}).\n\\] Solving \\(U_2(σ^*_1, \\mathsf{C}) = U_2(σ^*_1, \\mathsf{R})\\), we get \\(p = \\tfrac 34\\) and \\(U_2(σ^*_1, \\mathsf{C}) = U_2(σ^*_1, \\mathsf{R}) = \\tfrac{23}{4}\\).\nWe now check \\(U_2(σ^*_1, \\mathsf{L}) = -5p + 7 = \\tfrac{13}{4} \\le \\tfrac{23}{4}\\). Thus, the second system of equations has a consistent solution for \\(p\\), \\(p = \\tfrac{3}{4}\\).\nSo, we check the first system of equations \\(U_1(\\mathsf{T}, σ^*_2) = U_1(\\mathsf{B}, σ^*_2)\\) (for \\(q_1 = 0\\)), which gives \\(q_2 = -\\tfrac{1}{4}\\) and \\(q_3 = \\tfrac{5}{4}\\), which is not a valid probability mass function. Thus, the first system of equations does not have a consistent solution.\n\nTherefore, the game has a unique NE that is identified in Case 3.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nash equilibrium</span>"
    ]
  },
  {
    "objectID": "static-games/nash-equilibrium.html#proof-of-the-existence-of-nash-equilibrium",
    "href": "static-games/nash-equilibrium.html#proof-of-the-existence-of-nash-equilibrium",
    "title": "5  Nash equilibrium",
    "section": "5.5 Proof of the existence of Nash equilibrium",
    "text": "5.5 Proof of the existence of Nash equilibrium\nWe now present a proof that every finite game has a NE in mixed strategies. The proof is based on Nash (1950) and relies on :Kakutani’s fixed point theorem, which we state below.\n\nTheorem 5.3 (Kakutani’s fixed point theorem) Let \\(\\ALPHABET X\\) be a compact and convex subset of \\(\\reals^n\\) and \\(f \\colon \\ALPHABET X \\rightrightarrows \\ALPHABET X\\) be a set valued correspondence which satisfies:\n\nfor all \\(x \\in \\ALPHABET X\\), the set \\(f(x)\\) is non-empty and convex.\nThe graph of \\(f(x)\\) is closed, which means that if we consider two converging sequences \\(\\{x_n\\}_{n \\ge 1}\\) and \\(\\{y_n\\}_{n \\ge 1}\\) such that \\(x_n \\to x^*\\) and \\(y_n \\to y^*\\) such that \\(y_n \\in f(x_n)\\) for all \\(n\\), then \\(y^* \\in f(x^*)\\).\n\nThen \\(f\\) has a fixed point \\(x^*\\), i.e., there exists an \\(x^* \\in \\ALPHABET X\\) such that \\(x^* \\in f(x^*)\\).\n\nTo use this theorem, we need the notion of quasi-concave function, which can be defined in two equivalent ways.\n\nSuper-level set definition Let \\(\\ALPHABET C\\) be a convex subset of \\(\\reals^n\\). A function \\(g \\colon \\ALPHABET C \\to \\reals\\) is said to be quasi-concave if for every \\(α \\in \\reals\\), the super-level set \\[\n\\ALPHABET S^{α} \\coloneqq \\{ x \\in \\ALPHABET C : g(x) \\ge α \\}\n\\] is convex.\nInterpolation definition For the same setup as above, the function \\(g \\colon \\ALPHABET C \\to \\reals\\) is said to be quasi-concave if for every \\(x, y \\in \\ALPHABET C\\) and \\(λ \\in [0, 1]\\) \\[\n   f(λ x + (1-λ)y) \\ge \\min\\{ f(x), f(y) \\}.\n\\] Thus, if we draw a line between two points in \\(\\ALPHABET C\\), then the the value of the function is along this line is never less than the smaller of the two end point values.\n\nQuasi-concavity is a milder assumption that concavity. In particular, a concave function is quasi-concave but the reverse in not true. For example, consider the density of a Gassian random variable: \\[\n    g(x) = e^{-\\NORM{x}^2}\n\\] which is quasi-concave but not concave.\nQuasi-concave functions are useful in optimization because for a quasi-concave function, a local minimum is also a global minimum.\nNow we state a theorem which guarantees the existence of Nash equilibrium in pure strategies.\n\nTheorem 5.4 A strategic game \\(\\mathscr{G} = \\langle N, (\\ALPHABET S_i)_{i \\in N}, (u_i)_{i \\in N} \\rangle\\) has a NE in pure strategies if for all \\(i \\in N\\):\n\n\\(\\ALPHABET S_i\\) is non-empty, compact, and a convex subset of a Eucledian space.\nThe utlitiy function \\(u_i \\colon \\ALPHABET S \\to \\reals\\) is continuous in \\(\\ALPHABET S\\) and quasi-concave in \\(\\ALPHABET S_i\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\nConsider the set valued map \\(\\BR \\colon \\ALPHABET S \\rightrightarrows \\ALPHABET S\\) define by \\[\n  \\BR(s) = \\MATRIX{ \\BR_1(s_{-1} \\\\ \\vdots \\\\ B_n(s_{-n}) }.\n\\] We verify that \\(\\BR(s)\\) satisfies the conditions of Kakutani’s fixed point theorem (Theorem 5.3).\n\nFor any \\(i \\in N\\) and \\(s_{-i} \\in \\ALPHABET S_{-i}\\), the set \\(\\BR_i(s_{-i})\\) is non-empty because \\(u_i\\) is continuous and \\(\\ALPHABET S_i\\) is compact.\n\\(\\BR_i(s_{-i})\\) is convex because \\(u_i\\) is quasi-concave in \\(\\ALPHABET S_i\\)\n\\(\\BR(s)\\) has a closed graph because each \\(B_i(s_{-i})\\) is close because \\(u_i\\) is continuous.\n\nThus, by Kakutani’s fixed point theorem, \\(\\BR(s)\\) has a fixed point \\(s^*\\) such that \\(s^* = \\BR(s^*)\\). By definition, such a \\(s^*\\) is a NE.\n\n\n\nTheorem 5.5 (Nash) Every finite strategic game has a NE in mixed strategies.\n\n\n\n\n\n\n\nProof\n\n\n\nFor any game \\(\\mathscr{G}\\), consider the mixed extension \\(\\mathscr{G}'\\). If \\(\\mathscr{G}\\) is finite, then\n\n\\(Δ(\\ALPHABET S_i)\\) is non-empty, compact and convex subset of a Eucledian space.\n\\(U_i\\) is continuous on \\(Δ(\\ALPHABET S_1) × \\cdots × Δ(\\ALPHABET S_n)\\) and linear (and therefore also quasi-concave) on \\(Δ(\\ALPHABET S_i)\\).\n\nHence, by Theorem 5.4, the mixed extension \\(\\mathscr{G}'\\) has a NE in pure strategies. Thus, the original game \\(\\mathscr{G}\\) has a NE in mixed strategies.\n\n\nWe now state a generalization of Theorem 5.5 without a proof.\n\nTheorem 5.6 (Glicksberg) A strategic game \\(\\mathscr{G} = \\langle N, (\\ALPHABET S_i)_{i \\in N}, (u_i)_{i \\in N} \\rangle\\) has a mixed strategy NE if for all \\(i \\in N\\),\n\n\\(\\ALPHABET S_i\\) is a non-empty, convex, and compact subset of a Eucledian space.\nThe utility function \\(u_i\\) is continuous in \\(\\ALPHABET S\\).\n\n\nNote that if the utility function \\(u_i\\) is also quasi-concave in \\(\\ALPHABET S_i\\), then by Theorem 5.4, the game has a pure strategy NE.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nash equilibrium</span>"
    ]
  },
  {
    "objectID": "static-games/nash-equilibrium.html#some-examples",
    "href": "static-games/nash-equilibrium.html#some-examples",
    "title": "5  Nash equilibrium",
    "section": "5.6 Some examples",
    "text": "5.6 Some examples\n\nExample 5.5 (Taxation and Auditing) Consider the following game between a population and a tax auditor.\n\n\n$\\mathsf{H}$$\\mathsf{C}$$\\mathsf{A}$$2$$0$$4$$-10$$\\mathsf{N}$$4$$0$$0$$4$\n\n\nWhat happens to the equilibrium if the penalty for cheating is increased.\n\n\n$\\mathsf{H}$$\\mathsf{C}$$\\mathsf{A}$$2$$0$$4$$-20$$\\mathsf{N}$$4$$0$$0$$4$\n\n\nWhat happens if we increase the benefit of not getting caught\n\n\n$\\mathsf{H}$$\\mathsf{C}$$\\mathsf{A}$$2$$0$$4$$-10$$\\mathsf{N}$$4$$0$$0$$8$",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nash equilibrium</span>"
    ]
  },
  {
    "objectID": "static-games/nash-equilibrium.html#games-with-more-than-two-players",
    "href": "static-games/nash-equilibrium.html#games-with-more-than-two-players",
    "title": "5  Nash equilibrium",
    "section": "5.7 Games with more than two players",
    "text": "5.7 Games with more than two players\nSo far, we have presented examples of games with only two players. The same ideas work for games with more than two players, as is illustrated by the following examples.\n\nExample 5.6  \n\nConsider the following three player game where player 1 chooses the row, player 2 chooses the column and player 3 chooses whether game (a) or game (b) is played.\n\n\n\n\n\n\n\n(a)$\\mathsf{L}$$\\mathsf{R}$$\\mathsf{T}$$2$$2$$2$$0$$1$$0$$\\mathsf{B}$$1$$0$$0$$1$$1$$0$\n\n\n\n\n(b)$\\mathsf{L}$$\\mathsf{R}$$\\mathsf{T}$$0$$0$$1$$0$$1$$0$$\\mathsf{B}$$1$$0$$1$$1$$1$$1$\n\n\n\n\nFind all the NE in pure strategies.\n\n\n\n\n\n\n\nSolution\n\n\n\nWe could exhastively check whether each outcome is a NE or not. An alternative method is to check the BR of each player for every choice of strategy of other players. For example, for player 1,\n\n\\(\\BR_1(\\mathsf{L}, a) = \\mathsf{T}\\)\n\\(\\BR_1(\\mathsf{R}, a) = \\mathsf{B}\\)\n\\(\\BR_1(\\mathsf{L}, b) = \\mathsf{B}\\)\n\\(\\BR_1(\\mathsf{R}, b) = \\mathsf{B}\\)\n\nSimilarly, for player 2\n\n\\(\\BR_2(\\mathsf{T}, a) = \\mathsf{L}\\)\n\\(\\BR_2(\\mathsf{B}, a) = \\mathsf{R}\\)\n\\(\\BR_2(\\mathsf{T}, b) = \\mathsf{R}\\)\n\\(\\BR_2(\\mathsf{B}, b) = \\mathsf{R}\\)\n\nand for player 3\n\n\\(\\BR_3(\\mathsf{T}, \\mathsf{L}) = a\\)\n\\(\\BR_3(\\mathsf{T}, \\mathsf{R}) = \\{a, b\\}\\)\n\\(\\BR_3(\\mathsf{B}, \\mathsf{L}) = b\\)\n\\(\\BR_3(\\mathsf{B}, \\mathsf{R}) = b\\)\n\nNE are the fixed point of the BR, and is given by \\[\n  (\\mathsf{T}, \\mathsf{L}, a)\n  \\quad\\text{and}\\quad\n  (\\mathsf{B}, \\mathsf{R}, b)\n\\] An easy way to find this intersection is to visually draw a circle around the payoffs corresponding to each of the BRs. Then, the NE are the cells where all payoffs are encircled.\n\n\n\nExample 5.7  \n\nConsider the following three player game where player 1 chooses the row, player 2 chooses the column and player 3 chooses whether game (a) or game (b) is played.\n\n\n\n\n\n\n\n(a)$\\mathsf{L}$$\\mathsf{R}$$\\mathsf{T}$$0$$0$$0$$-4$$1$$2$$\\mathsf{B}$$1$$-4$$2$$2$$2$$-2$\n\n\n\n\n(b)$\\mathsf{L}$$\\mathsf{R}$$\\mathsf{T}$$3$$3$$-2$$1$$-4$$2$$\\mathsf{B}$$-4$$1$$2$$0$$0$$0$\n\n\n\n\nFind all the NE in mixed strategies.\n\n\n\n\n\n\n\nSolution\n\n\n\nWe can verify that there are no NE in pure strategies. The idea for finding the NE in mixed strategies remains the same, just the algebra is more tedious. In particular, suppose the mixed strategy NE is \\[\n  σ_1 = (p, 1-p),\n  \\quad\n  σ_2 = (q, 1-q),\n  \\quad\\text{and}\\quad\n  σ_3 = (r, 1-r).\n\\]\nThen, by the irrelevance principle, we have that for player 1, \\[\n  U_1(\\mathsf{T}, σ_2, σ_3) = U_1(\\mathsf{B}, σ_2, σ_3)\n\\] which simplifies to \\[\n\\bbox[5pt,border: 1px solid]{\n  7q - 6r + qr = 1.\n}\n\\]\nBy the same argument, we have that for player 2 \\[\n  U_2(σ_1, \\mathsf{L}, σ_3) = U_2(σ_1, \\mathsf{R}, σ_3)\n\\] which simplifies to \\[\n\\bbox[5pt,border: 1px solid]{\n  7p - 6r + pr = 1\n}\n\\]\nFinally, we have that for player 3, \\[\n  U_3(σ_1, σ_2, a) = U_3(σ_1, σ_2, b)\n\\] which simplifies to \\[\n\\bbox[5pt,border: 1px solid]{\n  p + q = 1.\n}\n\\] Thus, we get 3 (non-linear) equations in 3 variables. Simplifying the first two equations give us that \\(p = q\\), which combined with the third equation gives that \\(p = q = \\tfrac 12\\). Substituting back in either the first or the second equatin gives us that \\(r = \\tfrac 5{11}\\). Thus, the mixed strategy NE is \\[\n  σ_1 = (\\tfrac 12, \\tfrac 12),\n  \\quad\n  σ_2 = (\\tfrac 12, \\tfrac 12),\n  \\quad\\text{and}\\quad\n  σ_3 = (\\tfrac 5{11}, \\tfrac 6{11}).\n\\]",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nash equilibrium</span>"
    ]
  },
  {
    "objectID": "static-games/nash-equilibrium.html#exercises",
    "href": "static-games/nash-equilibrium.html#exercises",
    "title": "5  Nash equilibrium",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 5.1 For each of the following two-player games, find all Nash equilibria.\n\n\nGame 1$\\mathsf{L}$$\\mathsf{R}$$\\mathsf{T}$$2$$1$$1$$2$$\\mathsf{B}$$1$$5$$2$$1$\n\n\nGame 2$\\mathsf{L}$$\\mathsf{R}$$\\mathsf{T}$$3$$7$$6$$6$$\\mathsf{B}$$2$$2$$7$$3$\n\n\nGame 3$\\mathsf{L}$$\\mathsf{R}$$\\mathsf{T}$$7$$3$$6$$6$$\\mathsf{B}$$2$$2$$3$$7$\n\n\nHint: Each game has an odd number of equilibria.\n\n\nExercise 5.2 For each of the following two-player games, find all Nash equilibra.\n\n\nGame 1$\\mathsf{L}$$\\mathsf{C}$$\\mathsf{R}$$\\mathsf{T}$$0$$4$$5$$6$$8$$7$$\\mathsf{B}$$2$$9$$6$$5$$5$$1$\n\n\nGame 2$\\mathsf{L}$$\\mathsf{C}$$\\mathsf{R}$$\\mathsf{T}$$0$$0$$5$$4$$4$$5$$\\mathsf{M}$$4$$5$$0$$0$$5$$4$$\\mathsf{B}$$5$$4$$4$$5$$0$$0$\n\n\nHint: For Game 2, you don’t need to consider all 16 cases! Start by considering the case where both mixed strategies have full support. Based on the calculations for that case, you should be able to argue that you do not need to consider strategies of a particular form.\n\n\nExercise 5.3  \n\nFind all the NE of the following three player game where player 1 chooses the row, player 2 chooses the column and player 3 chooses whether game (a) or game (b) is played.\n\n\n\n\n\n\n\n(a)$\\mathsf{L}$$\\mathsf{R}$$\\mathsf{T}$$7$$1$$5$$1$$2$$4$$\\mathsf{B}$$2$$2$$1$$5$$3$$2$\n\n\n\n\n(b)$\\mathsf{L}$$\\mathsf{R}$$\\mathsf{T}$$5$$0$$5$$3$$4$$1$$\\mathsf{B}$$3$$3$$3$$0$$5$$4$\n\n\n\n\n\n\n\n\n\nNash, J. 1951. Non-cooperative games. The Annals of Mathematics 54, 2, 286. DOI: 10.2307/1969529.\n\n\nNash, J.F. 1950. Equilibrium points in n -person games. Proceedings of the National Academy of Sciences 36, 1, 48–49. DOI: 10.1073/pnas.36.1.48.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nash equilibrium</span>"
    ]
  },
  {
    "objectID": "static-games/ESS.html",
    "href": "static-games/ESS.html",
    "title": "6  Evolutionarily stable strategies",
    "section": "",
    "text": "6.1 What is ESS\nAs we saw earlier, one of the interpretations of mixed strategy NE is that it determines the fraction of population playing a certain pure strategy. Maynard Smith and Price (1973) used this interpretation to propose an evolutionary mechanism via which organisms can change their behavior across generations. For a facinating (non-technical) introduction to the area, see Maynard Smith (1982).\nThe key idea in what is now called evolutionary game theory is that of evolutionarily stable strategies, which we will discuss here. For detailed treatment of the subject, see Sandholm (2010).\nTo understand evolutionary stability, consider the variation of Hawk-Dove game shown below.\nA variation of the Hawk-Dove game$\\mathsf{D}$$\\mathsf{H}$$\\mathsf{D}$$4$$4$$2$$8$$\\mathsf{H}$$8$$2$$1$$1$\nNote that this is a symmetric game. Now suppose that there is a large population comprising of 80% doves and 20% hawks. THus, we are considering the population state \\(σ = (0.8, 0.2)\\).\nWe assume that there are repeated interactions between the organisms (or agents) in the population. At state \\(σ\\) described above, a generic agent will encounter a dove with probability \\(0.8\\) and a hawk with probability \\(0.2\\). Thus, if this organism is a dove, his payoff is \\[\n  U(\\mathsf{D}, σ) = 0.8 × 4 + 0.2 × 2 = 3.6.\n\\] On the other hand, if this organism is a hawk, then his payoff is \\[\n  U(\\mathsf{H}, σ) = 0.8 × 8 + 0.2 × 1 = 6.6.\n\\] The argument of Maynard Smith (1982) is that in this situation, the population of hawks have an evolutionary advantage. So, over time, the fraction of hawks will increase. The population state \\(σ^*\\) where no trait has an evolutionary advantage is given by \\[\n  U(\\mathsf{H}, σ^*) = U(\\mathsf{D}, σ^*)\n\\] which is precisely the NE of the game.\nNow the key difference in this interpretation from NE is how we determine if an equilibrium is sustainable. To define sustainability, we introduce some notation. Consider a symmetric two player game \\(\\mathscr{G} = \\langle 2, (\\ALPHABET S_1, \\ALPHABET S_2), (u_1, u_2) \\rangle\\) where \\(\\ALPHABET S_1 = \\ALPHABET S_2\\) and \\(u_1(s,t) = u_2(t,s)\\) for all \\(t, s \\in \\ALPHABET S_1\\). For the ease of notation, we will denote \\(\\ALPHABET S_1\\) and \\(\\ALPHABET S_2\\) simply as \\(\\ALPHABET S\\) (note that this is inconsistent with the notation that we use in general!) and denote the payoff function by \\(u \\colon \\ALPHABET S × \\ALPHABET S \\to \\reals\\) where \\[\n  u(s,t) = u_1(s,t) = u_2(t,s).\n\\]\nConsider a symmetric mixed strategy \\(σ^*\\), i.e., we have a large population where \\(σ^*(s)\\) fraction of the population is of type \\(s\\), \\(s \\in \\ALPHABET S\\).\nNow suppose a fraction \\(ε\\) of the population mutates and starts playing another strategy \\(σ\\). Thus, the new state of the population is \\[\n  \\bar σ_{ε} = (1-ε) σ^* + ε σ\n\\]\nThen, the average payoff of a normal player (i.e., the player belonging to the unmutated population) is \\[\\def\\1{\\textcolor{red}{σ^*}}\n  U(\\1, \\bar σ_{ε}) = (1-ε) U(\\1, σ^*) + ε U(\\1, σ).\n\\] Similarly, the average payoff of a mutated player is \\[\\def\\1{\\textcolor{red}{σ}}\n  U(\\1, \\bar σ_{ε}) = (1-ε) U(\\1, σ^*) + ε U(\\1, σ).\n\\]",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evolutionarily stable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/ESS.html#what-is-ess",
    "href": "static-games/ESS.html#what-is-ess",
    "title": "6  Evolutionarily stable strategies",
    "section": "",
    "text": "Definition 6.1 A population state \\(σ^*\\) is said to be evolutionarily stable strategy (ESS) if for all mutations \\(σ \\in Δ(\\ALPHABET S)\\), \\(σ \\neq σ^*\\), there exists a \\(ε_\\circ = ε_\\circ(σ) &gt; 0\\) such that for all \\(ε \\in (0, ε_\\circ)\\) and \\(\\bar σ_{ε} = (1 -ε) σ^* + ε σ\\), we have \\[\n  U(σ^*, \\bar σ_{ε}) &gt; U (σ, \\bar σ_{ε}).\n\\] i.e., \\[\\begin{equation}\\label{eq:ESS}\n  (1-ε)U(σ^*, σ^*) + ε U(σ^*, σ) &gt;\n  (1-ε)U(σ, σ^*) + ε U(σ, σ).\n\\end{equation}\\]",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evolutionarily stable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/ESS.html#relationship-between-ne-and-ess",
    "href": "static-games/ESS.html#relationship-between-ne-and-ess",
    "title": "6  Evolutionarily stable strategies",
    "section": "6.2 Relationship between NE and ESS",
    "text": "6.2 Relationship between NE and ESS\nIf we take the limit as \\(ε \\to 0\\) in the definition of ESS, we get that an ESS has be a symmetric NE. Thus, all ESS are NE. We now present an example to show that not all NE are ESS.\nConsider the variation of Hawk Dove shown below.\n\n\nA variation of the Hawk-Dove game$\\mathsf{D}$$\\mathsf{H}$$\\mathsf{D}$$4$$4$$2$$2$$\\mathsf{H}$$2$$2$$2$$2$\n\n\nThis game has two pure strategy NE: \\((\\mathsf{D}, \\mathsf{D})\\) and \\((\\mathsf{H}, \\mathsf{H})\\). We now check if each of these is ESS.\n\nConsider the NE \\((\\mathsf{D}, \\mathsf{D})\\), i.e., \\(σ^* = (1,0)\\). Consider a mutation \\(σ = (p, 1-p)\\), where \\(p &lt; 1\\). Then, \\[\\begin{align*}\n  U(σ^*, σ^*) &= 4  & U(σ^*, σ) &= 4p + 2(1-p) = 2p + 2 \\\\\n  U(σ, σ^*) &= 2p + 2  & U(σ, σ) &= 4p^2 + 2(1-p^2) = 2p^2 + 2\n\\end{align*}\\] Therefore, \\[\n  U(σ^*, σ_{ε}) = (1-ε)4 + ε(2p + 2) = 4 - 2 ε(1-p)\n\\] and \\[\n  U(σ, σ_{ε}) = (1-ε)(2p+2) + ε(2p^2 + 2) = (2p+2) - 2p ε(1-p).\n\\] Consider \\[\\begin{align*}\n  U(σ^*, σ_{ε}) - U(σ, σ_{ε})\n  &=\n  4 - 2 ε(1-p) - (2p+2) + 2p ε(1-p) \\\\\n  &=\n  2(1 - p - ε(1-p)^2) \\\\\n  &= 2(1-p)(1 - ε(1-p))\n\\end{align*}\\] Thus, for any \\(p \\in [0, 1)\\) (note that \\(p=1\\) is ruled out because the mutation cannot be the same as the normal population), and any \\(ε \\in (0, 1)\\), we have that \\[\n  U(σ^*, σ_{ε}) - U(σ, σ_{ε}) &gt; 0.\n\\] So, \\((\\mathsf{D}, \\mathsf{D})\\) is an ESS.\nConsider the NE \\((\\mathsf{H}, \\mathsf{H})\\), i.e., \\(σ^* = (0, 1)\\). Consider a mutation \\((p, 1-p)\\), where \\(p &gt; 0\\). Then, \\[\\begin{align*}\n  U(σ^*, σ^*) &= 2  & U(σ^*, σ) &= 2 \\\\\n  U(σ, σ^*) &= 2  & U(σ, σ) &= 4p^2 + 2(1-p^2) = 2p^2 + 2\n\\end{align*}\\] Therefore, \\[\n  U(σ^*, σ_{ε}) = (1-ε)2 + ε2 = 2\n\\] and \\[\n  U(σ, σ_{ε}) = (1-ε)2 + ε(2p^2 + 2) = 2 - 2 ε p^2\n\\] Consider \\[\n  U(σ^*, σ_{ε}) - U(σ, σ_{ε})\n  = 2 - 2 - 2 ε p^2 = -2 ε p^2\n\\] which is strictly positive for any \\(p \\in (0, 1]\\) (note that \\(p = 0\\) is ruled out because the mutation cannot be the same as the normal population) and \\(ε &gt; 0\\), we have \\[\n  U(σ^*, σ_{ε}) - U(σ, σ_{ε}) &lt; 0.\n\\] So, \\((\\mathsf{H}, \\mathsf{H})\\) is not an ESS.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evolutionarily stable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/ESS.html#necessary-and-sufficient-conditions-for-ess",
    "href": "static-games/ESS.html#necessary-and-sufficient-conditions-for-ess",
    "title": "6  Evolutionarily stable strategies",
    "section": "6.3 Necessary and sufficient conditions for ESS",
    "text": "6.3 Necessary and sufficient conditions for ESS\n\nTheorem 6.1 A strategy \\(σ^*\\) is ESS if and only if for every \\(σ \\neq σ^*\\) one of the following conditions is true:\n\n\\(U(σ^*, σ^*) &gt; U(σ, σ^*)\\)\nThat is, if a mutation deviates from \\(σ^*\\) it will lose in its encounters with the normal population\n\\(U(σ^*, σ^*) = U(σ, σ^*)\\) and \\(U(σ^*, σ) &gt; U(σ,σ)\\).\nThat is, if the payoff that a mutation receives from encountering the normal population is equal to the payoff that a normal indvidual receives from encountering the normal population, that mutation will receive a smaller payer if it encounters the same mutation than a normal individual will receive in the encounter\n\n\n\n\n\n\n\n\nProof\n\n\n\n\nESS implies the two conditions: Let \\(σ^*\\) be an ESS. As argued earlier, this implies that \\(σ^*\\) is also a NE. Thus, \\[\n  U(σ^*, σ^*) \\ge U(σ, σ^*), \\quad \\forall σ \\in Δ(\\ALPHABET S).\n\\] We prove the result by contradiction. Suppose that neither of the two conditions hold. Then, there must exist an \\(σ \\neq σ^*\\) such that \\[\n  U(σ^*, σ^*) = U(σ, σ^*)\n  \\quad\\text{and}\\quad\n  U(σ^*, σ) \\le U(σ, σ).\n\\] This implies that for any \\(ε \\in (0,1)\\) \\[\n  (1-ε)U(σ^*, σ^*) + ε U(σ^*, σ) \\le\n  (1-ε)U(σ, σ^*) + ε U(σ, σ)\n\\] which contradicts the fact that \\(σ^*\\) is ESS.\nThe two conditions imply ESS: Suppose a mutation \\(σ\\) satisfies condition 1. Let \\(D = U(σ^*, σ^*) - U(σ, σ^*)\\) and \\[\n  M = \\max_{σ,τ} U(σ,τ) - \\min_{σ,τ} u(σ,τ).\n\\] Moreover, for any \\(ε \\in (0,1)\\), define \\(\\bar σ_{ε} = (1-ε) σ^* + ε σ\\). Then, \\[\\begin{align*}\n  U(σ^*, \\bar σ_{ε}) - U(σ, \\bar σ_{ε}) &=\n  (1-ε) \\bigl[ U(σ^*, σ^*) - U(σ, σ^*) \\bigr]\n  +\n  ε \\bigl[ U(σ^*, σ) - U(σ, σ) \\bigr]\n  \\\\\n  & \\ge\n  (1-ε) D - ε M\n\\end{align*}\\] which is positive for \\(ε &lt; D/(D+M)\\).\nNow suppose a mutation statisfied condition 2. Then, \\[\\begin{align*}\n  U(σ^*, \\bar σ_{ε}) - U(σ, \\bar σ_{ε}) &=\n  (1-ε) \\underbrace{\\bigl[ U(σ^*, σ^*) - U(σ, σ^*) \\bigr]}_{=0}\n  +\n  ε \\underbrace{\\bigl[ U(σ^*, σ) - U(σ, σ) \\bigr]}_{&gt; 0}\n  \\\\\n  &&gt; 0, \\quad \\forall ε \\in (0,1).\n\\end{align*}\\]\n\nSince every mutation must satisfy either condition 1 or 2, we have that the strategy \\(σ^*\\) is ESS.\n\n\n\nExample 6.1 Consider the modified Hawk Dove game shown below.\n\n\nA variation of the Hawk-Dove game$\\mathsf{D}$$\\mathsf{H}$$\\mathsf{D}$$4$$4$$2$$8$$\\mathsf{H}$$8$$2$$1$$1$\n\n\nWe can verify that the unique NE is \\(σ^* = (\\tfrac 15, \\tfrac 45)\\). Is this an ESS?\n\n\n\n\n\n\n\nSolution\n\n\n\nConsider a mutation \\(σ = (p, 1-p)\\). We verify the two sufficient and necessary conditions of Theorem 6.1 separately.\n\nCheck condition 1: Since \\(σ^*\\) is a NE with support \\(\\{\\mathsf{H}, \\mathsf{D}\\}\\), from irrelevance principle we must have \\[\n  U(\\mathsf{H}, σ^*) = U(\\mathsf{D}, σ^*).\n\\] This means that for any mixed strategy \\(σ\\), we will have \\[\n  U(σ, σ^*) = U(σ^*, σ^*).\n\\] So, condition 1 is never satisfied.\nCheck condition 2: Since \\(U(σ, σ^*) = U(σ^*, σ^*)\\), condition 2 is satisfied if \\(U(σ^*, σ) &gt; U(σ, σ)\\). Observe that \\[\n  U(σ^*, σ) = \\frac{4p}{5} + \\frac{2(1-p)}{5} + \\frac{32p}{5} + \\frac{4(1-p)}{5}\n  = \\frac{6}{5} + 6p\n\\] and \\[\n  U(σ, σ) = 4p^2 + 2p(1-p) + 8(1-p)p + (1-p)^2\n  = 1 + 8p - 5p^2\n  .\n\\] Thus, \\[\n  U(σ^*, σ) - U(σ,σ) = \\frac{1}{5} - 2p + 5p^2\n  = \\frac{(1-5p)^2}{5}\n\\] which is non-negative for all \\(p \\neq \\tfrac 15\\) (i.e., all \\(σ \\neq σ^*\\)). Thus, condition 2 holds and therefore \\(σ^*\\) is ESS.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evolutionarily stable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/ESS.html#ess-for-arbitrary-two-player-2-2-symmeric-games",
    "href": "static-games/ESS.html#ess-for-arbitrary-two-player-2-2-symmeric-games",
    "title": "6  Evolutionarily stable strategies",
    "section": "6.4 ESS for arbitrary two player \\(2 × 2\\) symmeric games",
    "text": "6.4 ESS for arbitrary two player \\(2 × 2\\) symmeric games\nConsider a symmetric two player \\(2 × 2\\) game.\n\n\n$\\mathsf{1}$$\\mathsf{2}$$\\mathsf{1}$$a$$a$$b$$c$$\\mathsf{2}$$c$$b$$d$$d$\n\n\nA symmetric mixed strategy \\((σ^*, σ^*)\\) with \\(σ^* = (p, 1-p)\\) is a NE if and only if \\[\n  a p + b(1-p) = c p + d (1-p)\n  \\implies p = \\frac{b - d}{(b+c) - (a+d)}.\n\\] This is a valid probability distribution if either\n\nBoth numerator and denominator are positive and numerator is less than the denominator\nBoth numerator and denominator are negative and numerator is more than the denominator\n\nNote that there wil also be cases where \\(b = d\\) or \\(a = c\\) where there will be multiple equilibria. We do not consider these cases here.\n\nCase 1: In this case, we have\n\nnumerator is positive, i.e., \\(b &gt; d\\)\ndenominator is positive, i.e., \\(b+c &gt; a + d\\)\nnumerator is less than denominator, i.e., \\(c &gt; a\\).\n\nThus, \\[\n\\bbox[5pt,border: 1px solid]{\n  b &gt; d \\quad\\text{and}\\quad c &gt; a\n}\n\\]\nCase 2: In this case, we have\n\nnumerator is negative, i.e., \\(b &lt; d\\)\ndenominator is negative, i.e., \\(b+c &lt; a + d\\)\nnumerator is more than denominator, i.e., \\(c &lt; a\\).\n\nThus, \\[\n\\bbox[5pt,border: 1px solid]{\n  b &lt; d \\quad\\text{and}\\quad c &lt; a\n}\n\\]\n\nWhen is this mixed strategy NE an ESS?\nSince \\(p \\in (0,1)\\), for any other \\(σ = (q, 1-q)\\), we will have \\(U(σ, σ^*) = U(σ^*, σ^*)\\). So, we only need to check condition 2 of Theorem 6.1. For that mater, consider \\[\\begin{align*}\n  U(σ, σ^*) - U(σ, σ) &=\n  (p - q) \\big[ qa + (1-q) b \\bigr] - (p - q) \\bigl[ qc + (1-q) d \\bigr]\n  \\\\\n  &= (p -q) \\bigl[ b - d - q( (b+c) - (a+d) ) \\bigr] \\\\\n  &= (p - q)^2 ( (b+c) - (a+d) )\n\\end{align*}\\] where the last equality uses the fact that \\(b-d = p( (b+c) - (a+d) )\\).\nThus, in case 1 we have \\(U(σ, σ^*) - U(σ, σ)\\) and therefore \\(σ^*\\) is an ESS while in case 2 we have \\(U(σ, σ^*) - U(σ, σ) &lt; 0\\) and therefore \\(σ^*\\) is not an ESS.\n\n6.4.0.1 General Hawk Dove game\nConsider a general Hawk Dove game\n\n\nA general Hawk-Dove game$\\mathsf{D}$$\\mathsf{H}$$\\mathsf{D}$$\\frac{v - c}{2}$$\\frac{v - c}{2}$$0$$v$$\\mathsf{H}$$v$$0$$\\frac{v}{2}$$\\frac{v}{2}$\n\n\nwhere\n\n\\(v\\) denotes the value of the resource\n\\(c\\) denotes the value lost due to fighting.\n\nCharacterize all NE of this game and verify if the symmetric NE are ESS.\n\n\n\n\n\n\n\nSolution\n\n\n\nWe have the following cases.\n\nIf \\(\\tfrac 12 (v-c) &gt; 0\\), then \\((\\mathsf{H}, \\mathsf{H})\\) is a NE in strictly dominated strategies (and also an ESS)\nIf \\(\\tfrac 12 (v-c) = 0\\), then \\((\\mathsf{H}, \\mathsf{H})\\) is a NE in weakly dominated strategies (and not an ESS)\nIf \\(\\tfrac 12 (v-c) &lt; 0\\), then \\((\\mathsf{H}, \\mathsf{D})\\) and \\((\\mathsf{D}, \\mathsf{H})\\) are NE (but they are not symmetric). The symmetric NE in mixed strategies is \\(σ^* = (p, 1-p)\\) where \\[\n  p = \\frac{v - \\tfrac 12 v}{v - (v - \\frac{1}{2}c)} = \\frac{v}{c}.\n\\] Since in this case \\(v &gt; \\frac 12 v\\) and \\(0 &lt; \\frac 12(v-c)\\), we are in case 1 of the general setting and therefore this NE is ESS.\n\n\n\n\n6.4.0.2 Medium Access Control\nConsider a Medium Access Control game\n\n\nMedium access control game$\\mathsf{T}$$\\mathsf{D}$$\\mathsf{T}$$ - c$$ - c$$v - c$$0$$\\mathsf{D}$$0$$v - c$$0$$0$\n\n\nwhere\n\n\\(v\\) denotes the value of communication\n\\(c\\) denotes the cost of communication\n\nAssuming \\(v &gt; c\\), characterize all NE of this game and verify if the symmetric NE are ESS.\n\n\n\n\n\n\n\nSolution\n\n\n\nIn this case \\((\\mathsf{T}, \\mathsf{D})\\) and \\((\\mathsf{D}, \\mathsf{T})\\) are pure strategy NE of the game, but they are not symmetric. A symmetric mixed strategy of the form \\((σ^*, σ^*)\\) with \\(σ^* = (p, 1-p)\\) is mixed strategy NE where \\[\n  -pc + (v-c)(1-p) = 0 \\implies p = 1 - \\frac{c}{v}.\n\\] Since, \\(v - c &gt; 0\\) and \\(0 &gt; -c\\), we are in case 1 of the general setting and therefore \\(σ^*\\) is an ESS.\n\n\n\nExample 6.2 (Medium Access Control with lost opportiunity cost) Consider a Medium Access Control game\n\n\nMedium access control game with lost opportunity cost$\\mathsf{T}$$\\mathsf{D}$$\\mathsf{T}$$ - c$$ - c$$v - c$$0$$\\mathsf{D}$$0$$v - c$$ - d$$ - d$\n\n\nwhere\n\n\\(v\\) denotes the value of communication\n\\(c\\) denotes the cost of communication\n\\(d\\) denotes the cost of wasted channel\n\nAssuming \\(v &gt; c\\), characterize all NE of this game and verify if the symmetric NE are ESS.\n\n\n\n\n\n\n\nSolution\n\n\n\nIn this case \\((\\mathsf{T}, \\mathsf{D})\\) and \\((\\mathsf{D}, \\mathsf{T})\\) are pure strategy NE of the game, but they are not symmetric. A symmetric mixed strategy of the form \\((σ^*, σ^*)\\) with \\(σ^* = (p, 1-p)\\) is mixed strategy NE where \\[\n  -pc + (v-c)(1-p) = -d(1-p) \\implies p = 1 - \\frac{c}{v+d}.\n\\] Note that this shows that adding a lost opportunity cost is equivalent to increasing the value of communication\nSince, \\(v - c &gt; 0 &gt; -d\\) and \\(0 &gt; -c\\), we are in case 1 of the general setting and therefore \\(σ^*\\) is an ESS.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evolutionarily stable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/ESS.html#a-brief-overview-of-evolutionary-game-theory",
    "href": "static-games/ESS.html#a-brief-overview-of-evolutionary-game-theory",
    "title": "6  Evolutionarily stable strategies",
    "section": "6.5 A brief overview of evolutionary game theory",
    "text": "6.5 A brief overview of evolutionary game theory\nSo far we have discussed evolutionarily stable equilibrium. In this section we discuss evolutionary game theory which rovides revision protocols that explain how a population state evolves over time to reach an ESS.\nConsider a two player symmetric game with finite strategy space. let \\(\\ALPHABET S_1 = \\ALPHABET S_2 = \\{1, \\dots, m\\}\\) and define \\[\n  Δ^m = \\biggl\\{ (p_1, \\dots, p_m) \\in \\reals^{m}_{\\ge 0} : \\sum_{i=1}^m p_i = 1 \\biggr\\}.\n\\] For the ease of notation, we will use \\(p = (p_1, \\dots, p_m)\\) (instead of \\(σ\\) used earlier) to denote a mixed strategy.\nThe simplest form of revision protocol is called replicator dynamics. Suppose there is a large population of \\(N\\) agents playing strategy \\(p\\). Let \\(n_i = p_i N\\) denote the number of agents playing pure strategy \\(i\\). Suppose \\(r_i\\) denotes the rate of change of \\(n_i\\), i.e., \\[\n  \\dot n_i = r_i n_i.\n\\] This implies that \\[\n  \\dot N = \\sum_{i=1}^m \\dot n_i\n  = \\sum_{i=1}^m r_i n_i = \\bar r N\n\\] where \\(\\bar r = (r_1 p_1 + \\dots + r_m p_m)\\).\nRecall that \\(p_i = n_i/N\\). Therefore, \\[\n\\dot p_i = \\frac{\\dot n_i N - n_i \\dot N}{N^2}\n= \\frac{r_i n_i N - n_i \\bar r N}{N^2} = p_i(r_i - \\bar r).\n\\] Thus, the differential equation \\[\n  \\bbox[5pt,border: 1px solid]{\n    \\dot p_i = p_i (r_i - \\bar r)\n  }\n\\] tells us how the population state changes over time.\nIn general the rate \\(r_i\\) of change of population \\(i\\) is some function of the population state. Motivated by the theory of evolution, the simplest assumption is that the rate of change equals the fitness function, i.e., \\[\n  r_i = U(i, p).\n\\] This means that \\[\n  \\bar r = \\sum_{i=1}^m r_i p_i = \\sum_{i=1}^m U(i,p) p_i = U(p, p).\n\\] Substituting that back in the above differential equation we get \\[\n  \\bbox[5pt,border: 1px solid]{\n    \\dot p_i = p_i \\bigl[ U(i,p)  - U(p,p) \\bigr],\n    \\quad i \\in \\{1, \\dots, m\\}\n  }\n\\] This differential equation is called replicator dynamics.\nAn equilibrium point of this dynamics is one where \\(\\dot p_i = 0\\) for all \\(i\\). This means that for all \\(i\\) either \\(p_i = 0\\) or \\(U(i,p) = U(p,p)\\). Thus, any NE of the game is an equilibrium point of the replicator dynamics. The converse is not true. For example, every pure strategy \\(p\\) satisfies the replicator dynamics.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evolutionarily stable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/ESS.html#stability-of-equilibrium-points-of-replicator-dynamics",
    "href": "static-games/ESS.html#stability-of-equilibrium-points-of-replicator-dynamics",
    "title": "6  Evolutionarily stable strategies",
    "section": "6.6 Stability of equilibrium points of replicator dynamics",
    "text": "6.6 Stability of equilibrium points of replicator dynamics\nThe next question is whether the equilibrium points of replicator dynamics are stable. Recall that there are three common notions of stability of differential equations. Consider the differential equation \\[\n  \\dot x = f(x)\n\\] and let \\(x_e\\) be an equilibrium point (i.e., \\(f(x_e) = 0\\)).\n\nLyapunov stability: if the intiail state lies in a small neighborhood of \\(x_e\\), then the trajectory of the differential equation remains in the neighborhood.\nLocal asymptotic stability: If the initial state lies in a neighborhood of \\(x_e\\), then the trajectory of the trajectory of the differential equation converges to \\(x_e\\).\nGlobal asymptotic stability: For any initial state, the trajectory of the differential equation converges to \\(x_e\\).\n\nThe main result of replicator dynamics is the following.\n\nTheorem 6.2  \n\nAn ESS \\(p^*\\) is locally asymptotically stable equilibrium point of the replicator dyanmics.\nAn ESS \\(p^*\\) in the interior of \\(Δ^m\\) is a globally asymptotically stable equilibrium point of the replicator dynamics.\n\n\n\n\n\n\n\n\nProof\n\n\n\nThe proof uses \\(V(p) = \\prod_{i=1}^m p_i^{p_i^*}\\) as a Lyapunov function. See Hofbauer et al. (1979) for details.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evolutionarily stable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/ESS.html#other-evolutionary-dynamics",
    "href": "static-games/ESS.html#other-evolutionary-dynamics",
    "title": "6  Evolutionarily stable strategies",
    "section": "6.7 Other evolutionary dynamics",
    "text": "6.7 Other evolutionary dynamics\nReplicator dynamics are not ideal because they have the following property: if \\(p_i = 0\\) at some point then \\(p_i = 0\\) for all times in the future. To avoid this limitation, various other forms of evolutionary dynamics have been proposed in the literature. We list a few of them here:\n\nBest response dynamics \\[ \\dot p_i = \\BR_i(p) - p_i, \\quad i \\in \\{1, \\dots, m\\}. \\] Since best response is not unique, this gives us a differential inclusion rather than a differential equation.\nSmoothed best response dynamics \\[ \\dot p_i = \\sigma_i(p) - p_i, \\quad i \\in \\{1, \\dots, m\\} \\] where \\[\n  σ_i(p) = \\frac{\\exp(η U(i,p))}{\\sum_{j=1}^m \\exp(η U(j,p))}\n\\]\nBrown-von Neumann-Nash (BNN) dynamics: \\[\n  \\dot p_i = \\hat U_i(p) - p_i \\sum_{j=1}^m \\hat U_j(p),\n  \\quad i \\in \\{1, \\dots, m\\}\n\\] where \\[\n  \\hat U_i(p) = \\max\\{ U(i,p) - U(p,p), 0 \\}\n\\]\n\nThere are various stability results for these dynamics. These results rely on the notion of negative definite games and negative semi-definite games.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evolutionarily stable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/ESS.html#negative-definite-and-semidefinite-games",
    "href": "static-games/ESS.html#negative-definite-and-semidefinite-games",
    "title": "6  Evolutionarily stable strategies",
    "section": "6.8 Negative definite and semidefinite games",
    "text": "6.8 Negative definite and semidefinite games\nFor the ease of notation, define a \\(m × m\\) matrix \\(Q\\) such that \\(Q_{ij} = u(i,j)\\). Note that \\[\n  U(i,p) = \\sum_{j = 1}^m u(i,j) p_j = [Qp]_i\n\\] and \\[\n  U(q,p) = \\sum_{i=1}^m q_i U(i,p) = q^\\TRANS Q p.\n\\] With this notation, we have the following:\n\nNE A strategy \\(p\\) is a NE if for all \\(q \\in Δ^m\\), \\[\n  p^\\TRANS Q p \\ge q^\\TRANS Q p.\n\\]\nESS: A strategy \\(p\\) is ESS if for all \\(q \\in Δ^m\\), \\(q \\neq p\\), either of the following hold:\n\n\\(p^\\TRANS Q p &gt; q^\\TRANS Q p\\)\n\\(p^\\TRANS Q p = q^\\TRANS Q p\\) and \\(p^\\TRANS Q q &gt; q^\\TRANS Q q\\).\n\n\nDefine the set \\(\\reals^m_0\\) as \\(\\{ q \\in \\reals^m : \\sum_{i=1}^m q_i = 0 \\}\\). We say that a (symmetric) game is negative semidefinite if \\[\n  q^\\TRANS Q q \\le 0, \\quad \\forall q \\in \\reals^m_0.\n\\] We say that the game is negative definite if the inequality is strict for all \\(q \\neq 0\\).\nAnalogously the game is said to be positive semidefinite if \\[\n  q^\\TRANS Q q \\ge 0, \\quad \\forall q \\in \\reals^m_0.\n\\] We say that the game is postiive definite if the inequality is strict for all \\(q \\neq 0\\).\nNegative definite/semidefinite games have the following properties:\n\nA negative definite game has a unique NE (possibly at the boundary) which is also an ESS.\nThe set of NE of a negative semdefinite game is a convex subset of \\(Δ^m\\). These NE need not be ESS.\n\n\nExample 6.3 (A modefied version of rock paper scissors) Consider a variation of rock paper scissors where \\[\n  Q = \\MATRIX{0 & -b & a \\\\ a & 0  & -b \\\\ -b & a & 0 },\n  \\quad a, b &gt; 0.\n\\] The standard RPS is when \\(a = b\\). In all cases, it is easy to verify that \\(p^* = (\\tfrac 13, \\tfrac 13, \\tfrac 13)\\) is the unique NE.\nShow the following:\n\nIf \\(0 &lt; b &lt; a\\), then the game is negative definite.\nIf \\(0 &lt; a &lt; b\\), then the game is positive definite.\n\n\n\n\n\n\n\n\nProof\n\n\n\nTake a \\(q \\in \\reals^3_0\\), i.e., $q = (q_1, q_2, q_3) such that \\(q_1 + q_2 + q_3 = 0\\). Then, \\[\n  q^\\TRANS Q q = (a-b)[q_1q_2 + q_2 q_3 + q_3 q_1).\n\\] Since \\((q_1 + q_2)^2 = q_3^2\\), we have that \\(q_1q_2 = (q_3^2 - q_1^2 - q_2^2)/2\\). Symmetric expressions hold for \\(q_2q_3\\) and \\(q_3q_1\\). Substituting these in the above equation, we have \\[\n  q^\\TRANS Q q = \\frac{(b-a)}{2}(q_1^2 + q_2^2 + q_3^2).\n\\] Then the result follows from the definition of positive and negative definite games.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evolutionarily stable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/ESS.html#evolutionary-dynamics-and-negative-definitesemi-definite-games",
    "href": "static-games/ESS.html#evolutionary-dynamics-and-negative-definitesemi-definite-games",
    "title": "6  Evolutionarily stable strategies",
    "section": "6.9 Evolutionary dynamics and negative definite/semi-definite games",
    "text": "6.9 Evolutionary dynamics and negative definite/semi-definite games\nThe main results for replicator dynamics is that for negative definite games (which have a unique NE that is also an ESS), the unique NE is globally asymptotically stable equilibrium point of replicator dynamics.\nThe big advantage of the modified dynamics is that for negative semi-definite games, the convex set of NE is globally asymptotically stable for BR and BNN dynamics.\nThe smoothed BR dynamics have the advantage that they are always globally asymptotically stable but their equilibrium points are not NE but something called perturbed NE.\nSee the book Sandholm (2010) for more details on evolutionary dynamics.\nThe above results imply that for the modified version of rock paper scissors considered in Example 6.3, replicator dynamics will converge for games where \\(b &lt; a\\) but not when \\(b = a\\) (in which case the NE is not an ESS). However, BR and BNN dynamics will converge even when \\(b = a\\). The smoothed BR dynamics converge, but to a perturbed equilibrium point. We verify this numerically.\n\nusing DifferentialEquations, Plots\n\nfunction replicator_dynamics!(dp, p, Q, t)\n    dp .= p .* ( Q*p .- p'*Q*p)\nend\n\nfunction BNN_dynamics!(dp, p, Q, t)\n    Û = max.(Q*p .- p'Q*p, 0.0)\n    dp .= Û - sum(Û) .* p\nend\n\n## Modified RPS game\nQ(a,b) = [0 -b a; a 0 -b; -b a 0]\n\n## Initial mixed strategy\np0 = [0.3, 0.2, 0.5]\n\n\nComparing replicator and BNN dynamics on a negative definite game\nThis corresponds to the case when \\(a &gt; b\\). In this case, we expect both dynamics to converge. The numerical solution is as expected.\n\nreplicator_trajectory = ODEProblem(replicator_dynamics!, p0, (0.0, 35), Q(2,1)) |&gt; solve\nBNN_trajectory        = ODEProblem(BNN_dynamics!,        p0, (0.0, 35), Q(2,1)) |&gt; solve\n\nplot(replicator_trajectory, title=\"Replicator Dynamics\") |&gt; display\nplot(BNN_trajectory,        title=\"BNN Dynamics\")        |&gt; display\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparing replicator and BNN dynamics on a negative semidefinite game\nThis corresponds to the case when \\(a = b\\). In this case, we expect replicator dynamics not to converge but BNN to converge.\nThe numerical solution is as expected: replicator dynamics oscillates around the equilibrium point but BNN dynamics converge. However, the convergence is slow, so we have to solve the ODE for a longer time. I also used a more accurate ODE solver because the default solver was showing some numerical artifacts.\n\nreplicator_trajectory = ODEProblem(replicator_dynamics!, p0, (0.0, 75), Q(1,1)) |&gt; solve\nBNN_trajectory        = ODEProblem(BNN_dynamics!,        p0, (0.0, 75), Q(1,1)) |&gt; prob -&gt; solve(prob, Vern9())\n\nplot(replicator_trajectory, title=\"Replicator Dynamics\") |&gt; display\nplot(BNN_trajectory,        title=\"BNN Dynamics\")        |&gt; display\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparing replicator and BNN dynamics on a positive definite game\nThis corresponds to the case when \\(a &lt; b\\). In this case, we don’t expect either replicator dynamics or BNN to converge. The numerical solution shows that both the dynamics oscillate, but the behavior of replicator dynamics is a bit unexpected.\n\nreplicator_trajectory = ODEProblem(replicator_dynamics!, p0, (0.0, 35), Q(1,2)) |&gt; solve\nBNN_trajectory        = ODEProblem(BNN_dynamics!,        p0, (0.0, 35), Q(1,2)) |&gt; solve\n\nplot(replicator_trajectory, title=\"Replicator Dynamics\") |&gt; display\nplot(BNN_trajectory,        title=\"BNN Dynamics\")        |&gt; display\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that in the replicator dynamics, the trajectory is oscillating very close to the boundary (because each complenent cyclically gets very close to 1, which corresponds to a corner of the simplex). We can observe this behavior more easily if we solve the ODE for a longer horizon.\n\nreplicator_trajectory = ODEProblem(replicator_dynamics!, p0, (0.0, 250), Q(1,2)) |&gt; solve\nplot(replicator_trajectory, title=\"Replicator Dynamics\") |&gt; display\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHad we zoomed into the plot, say around time \\(t=125\\), it would appear that the solution has converged because all the components of the state are changing very slowing. But such a conclusion will be mistaken because the state switches after a while. This shows that it is dangerous to infer the convergence of a sequence by looking at the rate ofchange of that sequence!",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evolutionarily stable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/ESS.html#exercises",
    "href": "static-games/ESS.html#exercises",
    "title": "6  Evolutionarily stable strategies",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 6.1 Consider the following two-player symmetric game where \\(x\\) can be either \\(0\\), \\(1\\), or \\(2\\).\n\n\n$\\mathsf{A}$$\\mathsf{B}$$\\mathsf{A}$$1$$1$$2$$x$$\\mathsf{B}$$x$$2$$3$$3$\n\n\n\nFor each possible value of \\(x\\), find all NE of the game and check if they are ESS or not.\n\n\n\nExercise 6.2 The purpose of this exercise is to develop a method to check if a game is negative definite/semidefinte. This is not the most efficient method numerically, but is the simplest to understand.\nRecall the definition of \\(\\reals^m_0\\): \\[\n  \\reals^m_0 = \\biggl\\{ q \\in Δ^m : \\sum_{i=1}^m q_i = 0 \\biggr\\}.\n\\] This is a linear subspace of dimension \\(m-1\\) because \\(q_m = -(q_1 + \\cdots + q_{m-1})\\). A basis of this linear subspace is \\[\n  T = \\MATRIX{ I_{(m-1) × (m-1)} \\\\ -\\ONES_{1 \\times (m-1) } }_{m \\times (m-1)}\n\\] Then any vector \\(q \\in \\reals^m_0\\) must be of the form \\(T x\\), where \\(x \\in \\reals^m\\).\n\nBased on the above, argue that a game with matrix \\(Q\\) is negative definite if and only if \\(T^\\TRANS Q T\\) is a negative definite matrix (i.e., all the eigenvalues are real and negative).\nConsider the symmetric game in Exercise 5.2, which is repeated below for convenience.\n\n$\\mathsf{L}$$\\mathsf{C}$$\\mathsf{R}$$\\mathsf{T}$$0$$0$$5$$4$$4$$5$$\\mathsf{M}$$4$$5$$0$$0$$5$$4$$\\mathsf{B}$$5$$4$$4$$5$$0$$0$\nCheck if this case is negative definite? (You may use any numerical software to compute the eigenvalues of \\(T^\\TRANS Q T\\), but you should report the eigenvalues in your solution).\n\n\nExercise 6.3 Consider the symmetric game of Exercise 6.2. Run replicator dynamics and BNN dynamics on this game and describe the behavior (converges and if so to what value, does not converge and oscillates, etc.).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can verify that this game is negative semi-definite. So, we expect that Replicator dynamics will not converge but BNN dynamics will. We test that numerically.\n\nQex = [0 5 4; 4 0 5; 5 4 0]\n\nreplicator_trajectory = ODEProblem(replicator_dynamics!, p0, (0.0, 15), Qex) |&gt; solve\nBNN_trajectory        = ODEProblem(BNN_dynamics!,        p0, (0.0, 15), Qex) |&gt; solve\n\nplot(replicator_trajectory, title=\"Replicator Dynamics\") |&gt; display\nplot(BNN_trajectory,        title=\"BNN Dynamics\")        |&gt; display\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHofbauer, J., Schuster, P., and Sigmund, K. 1979. A note on evolutionary stable strategies and game dynamics. Journal of Theoretical Biology 81, 3, 609–612. DOI: 10.1016/0022-5193(79)90058-4.\n\n\nMaynard Smith, J. 1982. Evolution and the theory of games. Cambridge University Press. DOI: 10.1017/cbo9780511806292.\n\n\nMaynard Smith, J. and Price, G.R. 1973. The logic of animal conflict. Nature 246, 5427, 15–18. DOI: 10.1038/246015a0.\n\n\nSandholm, W.H. 2010. Population games and evolutionary dynamics. MIT Press.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evolutionarily stable strategies</span>"
    ]
  },
  {
    "objectID": "static-games/correlated-equilibrium.html",
    "href": "static-games/correlated-equilibrium.html",
    "title": "7  Correlated equilibrium",
    "section": "",
    "text": "7.1 Correlated equilibrium\nCosider the following “traffic stop” game:\nThere are two pure strategy Nash equilibria: \\((\\mathsf{Stop}, \\mathsf{Go})\\) and \\((\\mathsf{Go}, \\mathsf{Stop})\\). In both equilibria, one player gets a payoff of \\(1\\) and the other gets a payoff of \\(0\\).\nIn addition, there is a mixed strategy Nash equilibrium \\((\\tfrac {100}{101}, \\tfrac{1}{101})\\). The mixed strategy equilibrium seems to be worse off for both players: on average both of them get a payoff of \\(0\\) but risk a huge negative penalty of \\(-100\\).\nThe mixed strategy Nash equilibrium induces the following probability distribution on the action profiles\nA better solution is to do a randomization between the pure Nash equilibrium strategies, i.e., use the following probability distribution on the action profiles:\nThis is consistent with how we resolve the conflict in real-life: by using a traffic light which tells which user should go and which should stop. Once a traffic light makes a joint recommendation to both players, it is in the best interest of the players to follow that recommendation.\nIt is not always possible to achieve such a “coordination” via mixed strategies. The reason is that, in mixed strategies, the players are randomizing independently: so the joint distribution of the form above cannot be achieved.\nOne way to interpret correlated equilibrium is as follows:\nNow consider a strategic game \\(\\mathscr{G} = \\langle \\ALPHABET N, (\\ALPHABET A_i)_{i \\in \\ALPHABET N}, (u_i)_{i \\in \\ALPHABET N} \\rangle\\). Let \\(\\ALPHABET A = \\prod_{i \\in \\ALPHABET N} \\ALPHABET A_i\\) be the strategy space of all players. Consider a probability distribution \\(π\\) over \\(\\ALPHABET A\\). Define an extensive form game \\(Γ(π)\\) with imperfect information as follows:\nA correlated strategy \\(π\\) is a joint probability distribution on all the pure strategies of the game. For example, consider any \\(2×2\\) two-player game. Then, a correlated strategy is of the form \\[\n  π = \\begin{bmatrix} π_{11} & π_{12} \\\\ π_{21} & π_{22} \\end{bmatrix},\n  \\quad π_{ij} \\ge 0,\n  \\quad \\sum_{i,j} π_{ij} = 1.\n\\]\nTo understand this definition, we restrict attetion to two player games. Consider a correlated strategy \\(π\\). Then the conditional distribution of \\(A_2\\) given \\(A_1 = a_1\\) is \\[\n  \\PR(A_2 = a_2 \\mid A_1 = a_1) =\n  \\frac{π(a_1, a_2)}{\\sum_{b_2 \\in \\ALPHABET A_2} π(a_1, b_2)}.\n\\] Note that the denominator is the same for both sides of the expectation in \\eqref{eq:corr}. So, we can rewrite \\eqref{eq:corr} as \\[\\begin{equation}\\label{eq:corr2}\n  \\sum_{a_j \\in \\ALPHABET A_j} π(a_i, a_j)\n  \\bigl[ u_i(a_i, a_j) - u_i(b_i, a_j) \\bigr] \\ge 0,\n  \\quad \\forall a_i, b_i \\in \\ALPHABET A_i,\n  \\quad \\forall i \\in \\{1,2\\}.\n\\end{equation}\\]\nNote that \\eqref{eq:corr2} are a set of linear inequalities. Therefore, the set of all correlated equilibria is convex and can be obtained by identifying the feasibility region of a linear program.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Correlated equilibrium</span>"
    ]
  },
  {
    "objectID": "static-games/correlated-equilibrium.html#correlated-equilibrium",
    "href": "static-games/correlated-equilibrium.html#correlated-equilibrium",
    "title": "7  Correlated equilibrium",
    "section": "",
    "text": "There is an omniscient observer (who may be viewed as an information designer) who recommends strategies to the players.\nThe observer randomizes to choose his recommendations based on a probability distribution that is known to the players.\nThe recommendations are private, with each player knows only the recommendation addressed to him.\nThe mechanism is common knowledge among the players: each plaoer knows that this mechanism is being used; each player knows that the other players know that this mechanism is being used, and so on.\n\n\n\nAn outside observer (i.e., the information designer) probabilistically chooses an action profile \\(a \\in \\ALPHABET A\\) according to \\(π\\).\nThe observer reveals the coordinate \\(a_i\\) (but not \\(a_{-i}\\)) to each player \\(i \\in \\ALPHABET N\\). We may interpret this as the observer recommending strategy \\(a_i\\) to player \\(i\\).\nThe recommendation of the observer is non-binding. Each player chooses an action \\(b_i \\in \\ALPHABET A_i\\), where \\(b_i\\) may be different from \\(a_i\\).\nThe payoff of each player \\(i\\) is \\(u_i(b_1, \\dots, b_n)\\).\n\n\nDefinition 7.1 A (pure) strategy of player \\(i\\) in game \\(Γ(π)\\) is a function \\(s_i \\colon \\ALPHABET A_i \\to \\ALPHABET A_i\\), mapping every recommendation \\(a_i\\) of the observer to an action \\(s_i(a_i)\\).\n\n\n\nDefinition 7.2 Given a strategic game \\(\\mathscr{G} = \\langle \\ALPHABET N, (\\ALPHABET A_i)_{i\n\\in \\ALPHABET N}, (u_i)_{i \\in \\ALPHABET N} \\rangle\\), a correlated equilibrium is a is a correlated strategy \\(π\\) such that \\[\\begin{equation}\\label{eq:corr}\n   \\EXP^{π}[ u(a_i, A_{-i}) \\mid A_i = a_i ]\n   \\ge\n   \\EXP^{π}[ u(a_i, A_{-i}) \\mid A_i = b_i ]\n   , \\quad\n   \\forall b_i \\in \\ALPHABET A_i,\n   \\forall i \\in \\ALPHABET N.\n\\end{equation}\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nNote that Nash equilibria are special case of correlated equilibria in which the mediator recommends actions via independent randomizations. So, correlated equilibria always exist.\nIt can also be shown that any convex combination of Nash equilibira is a correlated equilibirum.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Correlated equilibrium</span>"
    ]
  },
  {
    "objectID": "static-games/correlated-equilibrium.html#examples",
    "href": "static-games/correlated-equilibrium.html#examples",
    "title": "7  Correlated equilibrium",
    "section": "7.2 Examples",
    "text": "7.2 Examples\n\n7.2.1 A Hawk-Dove game\nConsider the following variation of “hawk-dove” game:\n\n\n\n\n\n\\(\\mathsf{D}\\)\n\n\n\\(\\mathsf{H}\\)\n\n\n\n\n\\(\\mathsf{D}\\)\n\n\n\\(2\\)\n\n\n\\(2\\)\n\n\n\\(0\\)\n\n\n\\(3\\)\n\n\n\n\n\\(\\mathsf{H}\\)\n\n\n\\(3\\)\n\n\n\\(0\\)\n\n\n\\(-10\\)\n\n\n\\(-10\\)\n\n\n\nWe can verify that this game has two pure strategies Nash equilibria \\((\\mathsf{D}, \\mathsf{H})\\) and \\((\\mathsf{H}, \\mathsf{D})\\). In addition, there is a symmetric mixed strategy Nash equilibrium \\((\\tfrac {10}{11},\n\\tfrac{1}{11})\\) which has a payoff of \\((\\tfrac{20}{11}, \\tfrac{20}{11})\\).\n\\[\n\\def\\D{\\mathsf{D}}\n\\def\\H{\\mathsf{H}}\n\\]\nWe claim that the following is a correlated equilibrium for the game: \\[\n  π = \\begin{bmatrix}\n    \\frac {10}{12} & \\frac{1}{12} \\\\ \\frac{1}{12} & 0\n  \\end{bmatrix}\n\\]\nWe first verify the conditions in \\eqref{eq:corr} and then verify the conditions in \\eqref{eq:corr2}.\n\n7.2.1.1 Verification of \\eqref{eq:corr}\nWe consider the analysis from the point of view of player 1. By symmetry, the argument is the same for player 1.\n\nSuppose the mediator recommends strategy \\(\\D\\) to player~\\(1\\). The conditional payoff if player 1 follows the recommendation is \\[\n  \\frac{2 π(\\D, \\D) + 0 π(\\D, \\H)}{ π(\\D, \\D) + π(\\D, \\H) }\n  = \\frac{2 \\frac{10}{12}}{\\frac{11}{12}}\n  = \\frac{20}{11}.\n\\] The player’s payoff if they deviate is \\[\n  \\frac{3 π(\\D, \\D) - 10 π(\\D, \\H)}{ π(\\D, \\D) + π(\\D, \\H) }\n  = \\frac{3 \\frac{10}{12} - 10\\frac{1}{12}}{\\frac{11}{12}}\n  = \\frac{20}{11}.\n\\] Thus, the player has no incentive to deviate.\nNow suppose the mediator recommends strategy \\(\\H\\) to player \\(1\\). Then the player knows that player \\(2\\) has received a recommendation of \\(\\D\\). Since \\((\\H, \\D)\\) is a Nash equilibrium, there is no incentive to deviate.\n\n\n\n7.2.1.2 Verification of \\eqref{eq:corr2}\nWe consider each case separately:\n\n\\(i = 1\\), \\(a_1 = \\D\\), \\(b_1 = \\H\\): \\[\\begin{align*}\n    \\hskip 2em & \\hskip -2em\n    π(\\D,\\D)[ u_1(\\D,\\D) - u_1(\\H,\\D) ] +\n    π(\\D,\\H)[ u_1(\\D,\\H) - u_1(\\H,\\H) ]\n    \\\\\n    &=\n    \\frac{10}{12}[ 2 - 3 ] + \\frac{1}{12}[ 0 + 10]\n    \\\\\n    &= 0 \\ge 0.\n\\end{align*}\\]\n\\(i = 1\\), \\(a_1 = \\H\\), \\(b_1 = \\D\\): \\[\\begin{align*}\n    \\hskip 2em & \\hskip -2em\n    π(\\H,\\D)[ u_1(\\H,\\D) - u_1(\\D,\\D) ] +\n    π(\\H,\\H)[ u_1(\\H,\\H) - u_1(\\D,\\H) ]\n    \\\\\n    &=\n    \\frac{1}{12}[ 3 - 2 ] + 0 [ -10 + 0]\n    \\\\\n    &= \\frac{1}{12} \\ge 0.\n\\end{align*}\\]\n\\(i = 2\\), \\(a_2 = \\D\\), \\(b_2 = \\H\\): \\[\\begin{align*}\n    \\hskip 2em & \\hskip -2em\n    π(\\D,\\D)[ u_2(\\D,\\D) - u_2(\\D,\\H) ] +\n    π(\\H,\\D)[ u_2(\\H,\\D) - u_2(\\H,\\H) ]\n    \\\\\n    &=\n    \\frac{10}{12}[ 2 - 3 ] + \\frac{1}{12}[ 0 + 10]\n    \\\\\n    &= 0 \\ge 0.\n\\end{align*}\\]\n\\(i = 2\\), \\(a_2 = \\H\\), \\(b_2 = \\D\\): \\[\\begin{align*}\n    \\hskip 2em & \\hskip -2em\n    π(\\D,\\H)[ u_2(\\D,\\H) - u_2(\\D,\\D) ] +\n    π(\\H,\\H)[ u_2(\\H,\\H) - u_2(\\H,\\D) ]\n    \\\\\n    &=\n    \\frac{1}{12}[ 3 - 2 ] + 0 [ -10 + 0]\n    \\\\\n    &= \\frac{1}{12} \\ge 0.\n\\end{align*}\\]\n\nThus in all cases, neither player has an incentive to deviate.\nNote that the calculation for verifying \\eqref{eq:corr2} are the same as the calculations in the numerator of verifying \\eqref{eq:corr}.\n\n\n\n\n\n\nRemark\n\n\n\nThe social payoff of the correlated equilibrium strategy is \\[\n    4 \\frac{10}{12} + 3 \\frac{1}{12} + 3 \\frac{1}{12}\n    = \\frac{46}{12}\n\\] which is higher than the social welfare of \\(40/11\\) for the mixed strategy Nash equilibrium but is worse than the team optimal payoff of \\(4\\).",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Correlated equilibrium</span>"
    ]
  },
  {
    "objectID": "static-games/correlated-equilibrium.html#notes",
    "href": "static-games/correlated-equilibrium.html#notes",
    "title": "7  Correlated equilibrium",
    "section": "Notes",
    "text": "Notes\nThe notion of correlated equilibrium is due to Aumann (1974). Also see Aumann (1987). See Amir et al. (2017) for discussion of correlated equilibrium from \\(2×2\\) games. Some of the discussion in this section is adapted from Maschler et al. (2020).\nSee Papadimitriou and Roughgarden (2008) for algorithmic aspects of computing correlated equilibrium.\n\n\n\n\nAmir, R., Belkov, S., and Evstigneev, I.V. 2017. Correlated equilibrium in a nutshell. Theory and Decision 83, 4, 457–468. DOI: 10.1007/s11238-017-9609-9.\n\n\nAumann, R.J. 1974. Subjectivity and correlation in randomized strategies. Journal of Mathematical Economics 1, 1, 67–96. DOI: 10.1016/0304-4068(74)90037-8.\n\n\nAumann, R.J. 1987. Correlated equilibrium as an expression of bayesian rationality. Econometrica 55, 1, 1. DOI: 10.2307/1911154.\n\n\nMaschler, M., Zamir, S., and Solan, E. 2020. Game theory. Cambridge University Press.\n\n\nPapadimitriou, C.H. and Roughgarden, T. 2008. Computing correlated equilibria in multi-player games. Journal of the ACM 55, 3, 1–29. DOI: 10.1145/1379759.1379762.",
    "crumbs": [
      "Multi-Agent Systems",
      "Static Games",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Correlated equilibrium</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Acemoglu, D., Kakhbod, A., and Ozdaglar,\nA. 2017. Competition in electricity markets with renewable energy\nsources. The Energy Journal 38, 1_suppl, 137–156.\n\n\nAltman, E., Avrachenkov, K., and Garnaev,\nA. 2007. A jamming game in wireless networks with transmission\ncost. Network control and optimization: First EuroFGI international\nconference, Springer Berlin Heidelberg, 1–12. DOI: 10.1007/978-3-540-72709-5_1.\n\n\nAltman, E., Avrachenkov, K., and Garnaev,\nA. 2009. Jamming in wireless networks: The case of several\njammers. International conference on game theory for networks,\nIEEE, 585–592.\n\n\nAmir, R., Belkov, S., and Evstigneev,\nI.V. 2017. Correlated equilibrium in a nutshell. Theory and\nDecision 83, 4, 457–468. DOI: 10.1007/s11238-017-9609-9.\n\n\nAumann, R.J. 1974. Subjectivity and\ncorrelation in randomized strategies. Journal of Mathematical\nEconomics 1, 1, 67–96. DOI: 10.1016/0304-4068(74)90037-8.\n\n\nAumann, R.J. 1987. Correlated equilibrium\nas an expression of bayesian rationality. Econometrica\n55, 1, 1. DOI: 10.2307/1911154.\n\n\nBernheim, B.D. 1984. Rationalizable\nstrategic behavior. Econometrica: Journal of the Econometric\nSociety, 1007–1028.\n\n\nBrown, G.W. 1951. Iterative solutions of\ngames by fictitious play. Activity analysis of production and\nallocation., Wiley.\n\n\nDantzig, G.B. 1951. Activity analysis\nofproduetion and allocation. In: T.C. Koopmans, ed., John Wiley, New\nYork, 333–335.\n\n\nFasoulakis, M., Traganitis, A., and Ephremides,\nA. 2019. Jamming in multiple independent gaussian channels as a\ngame. In: Game theory for networks. Springer International\nPublishing, 3–8. DOI: 10.1007/978-3-030-16989-3_1.\n\n\nFudenberg, D. and Levine, D.K. 1998.\nThe theory of learning in games. MIT press.\n\n\nHe, P., Zhao, L., Zhou, S., and Niu, Z.\n2013. Water-filling: A geometric approach and its application to solve\ngeneralized radio resource allocation problems. IEEE Transactions on\nWireless Communications 12, 7, 3637–3647. DOI: 10.1109/twc.2013.061713.130278.\n\n\nHespanha, J.P. 2017. Noncooperative\ngame theory: An introduction for engineers and computer scientists.\nPrinceton University Press. DOI: 10.1515/9781400885442.\n\n\nHofbauer, J. 1995. Stability for the\nbest response dynamics. Universität Wien.\n\n\nHofbauer, J., Schuster, P., and Sigmund,\nK. 1979. A note on evolutionary stable strategies and game\ndynamics. Journal of Theoretical Biology 81, 3,\n609–612. DOI: 10.1016/0022-5193(79)90058-4.\n\n\nKeynes, J.M. 1936. The general theory\nof employment, interest and money. Harcourt Brace; Company, New\nYork.\n\n\nKjeldsen, T.H. 2001. John von neumann’s\nconception of the minimax theorem: A journey through different\nmathematical contexts. Archive for History of Exact Sciences\n56, 1, 39–68. Available at: http://www.jstor.org/stable/41134130\n(Accessed: January 5, 2025).\n\n\nLundin, E. and Tangerås, T.P. 2020.\nCournot competition in wholesale electricity markets: The nordic power\nexchange, nord pool. International Journal of Industrial\nOrganization 68, 102536.\n\n\nMaschler, M., Zamir, S., and Solan, E.\n2020. Game theory. Cambridge University Press.\n\n\nMaynard Smith, J. 1982. Evolution and\nthe theory of games. Cambridge University Press. DOI: 10.1017/cbo9780511806292.\n\n\nMaynard Smith, J. and Price, G.R. 1973.\nThe logic of animal conflict. Nature 246, 5427, 15–18.\nDOI: 10.1038/246015a0.\n\n\nMiyasawa, K. 1961. On the convergence\nof learning processes in a 2x2 non-zero-person game. Econometric\nResearch Program. Available at: https://econpapers.repec.org/paper/clalevarc/419.htm.\n\n\nMonderer, D. and Shapley, L.S. 1996.\nFictitious play property for games with identical interests. Journal\nof economic theory 68, 1, 258–265.\n\n\nMoulin, H. 1986. Game theory for\nsocial sciences. NYU Press, New York.\n\n\nNagel, R. 1995. Unraveling in guessing\ngames: An experimental study. The American Economic Review\n85, 5, 1313–1326.\n\n\nNash, J. 1951. Non-cooperative games.\nThe Annals of Mathematics 54, 2, 286. DOI: 10.2307/1969529.\n\n\nNash, J.F. 1950. Equilibrium points in n\n-person games. Proceedings of the National Academy of Sciences\n36, 1, 48–49. DOI: 10.1073/pnas.36.1.48.\n\n\nNeumann, J. von and Morgenstern, O. 1944.\nTheory of games and economic behaviour. Princeton University\nPress.\n\n\nOsborne, M.J. and Rubinstein, A. 1994.\nA course in game theory. MIT Press.\n\n\nPapadimitriou, C.H. and Roughgarden, T.\n2008. Computing correlated equilibria in multi-player games. Journal\nof the ACM 55, 3, 1–29. DOI: 10.1145/1379759.1379762.\n\n\nRaghavan, T.E.S. 1994. Chapter 20\nzero-sum two-person games. In: Handbook of game theory with economic\napplications. Elsevier, 735–768. DOI: 10.1016/s1574-0005(05)80052-9.\n\n\nRobinson, J. 1951. An iterative method of\nsolving a game. The Annals of Mathematics 54, 2, 296.\nDOI: 10.2307/1969530.\n\n\nSandholm, W.H. 2010. Population games\nand evolutionary dynamics. MIT Press.\n\n\nSelten, R. 1975. Reexamination of the\nperfectness concept for equilibrium points in extensive games.\nInternational Journal of Game Theory 4, 1, 25–55. DOI:\n10.1007/bf01766400.\n\n\nSion, M. 1958. On general minimax\ntheorems. Pacific Journal of Mathematics 8, 1,\n171–176. DOI: 10.2140/pjm.1958.8.171.\n\n\nStengel, B. von. 2024. Zero-sum games and\nlinear programming duality. Mathematics of Operations Research\n49, 2, 1091–1108. DOI: 10.1287/moor.2022.0149.\n\n\nvon Neumann, J. 1928. Zur theorie der\ngesellschaftsspiele. Mathematische Annalen 100, 1,\n295–320. DOI: 10.1007/bf01448847.\n\n\nWillems, B. 2002. Modeling cournot\ncompetition in an electricity market with transmission constraints.\nThe Energy Journal 23, 3, 95–125.",
    "crumbs": [
      "Multi-Agent Systems",
      "References"
    ]
  }
]